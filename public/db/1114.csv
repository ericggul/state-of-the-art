idx,name,version,category,assignedTo,paperName,year,authors,institution,citations_APA,relatedPapers_1,relatedPapers_2,explanation,highlights-1,highlights-2,highlights-3,limitations-1,limitations-2,limitations-3,citationsNumber,metric,yAxisLabel,labels,data,format,model_image
1.0,McCulloch-Pitts Neuron,v1.0,basic_nn,정윤,A logical calculus of the ideas immanent in nervous activity,1943.0,"WS McCulloch, W Pitts ",,"McCulloch, W. S., & Pitts, W. (1943). A logical calculus of the ideas immanent in nervous activity. The bulletin of mathematical biophysics, 5, 115-133.","Hansen, B. B., Spittle, S., Chen, B., Poe, D., Zhang, Y., Klein, J. M., ... & Sangoro, J. R. (2020). Deep eutectic solvents: A review of fundamentals and applications. Chemical reviews, 121(3), 1232-1285.","Zuo, C., Qian, J., Feng, S., Yin, W., Li, Y., Fan, P., ... & Chen, Q. (2022). Deep learning in optical metrology: a review. Light: Science & Applications, 11(1), 1-54.","Propose a logical framework to model neural activity using propositional logic, highlighting neural networks' computational properties",Logical Framework: Neural activity modeled using propositional logic.,Computational Equivalence: Neural networks simulate logical expressions.,Neural Dynamics: Circuits affect temporal behavior.,"Simplistic Assumptions: Oversimplifies neuron behavior, ignoring biological complexities.",Static Structure: Assumes unchanging neural networks.,Time Indeterminacy: Struggles with precise timing in feedback loops.,31925.0,,,,,,1.png
8.0,Variational Autoencoder (VAE),v2.1.2,vae,정윤,Auto-encoding variational bayes,2013.0,"Diederik P Kingma, Max Welling",Universiteit van Amsterdam,"Kingma, D. P. (2013). Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114.","Yang, L., Zhang, Z., Song, Y., Hong, S., Xu, R., Zhao, Y., ... & Yang, M. H. (2023). Diffusion models: A comprehensive survey of methods and applications. ACM Computing Surveys, 56(4), 1-39.","Zhang, C., Zhang, C., Zheng, S., Qiao, Y., Li, C., Zhang, M., ... & Hong, C. S. (2023). A complete survey on generative ai (aigc): Is chatgpt from gpt-4 to gpt-5 all you need?. arXiv preprint arXiv:2303.11717.","The paper introduces Variational Autoencoders (VAEs), which use the reparameterization trick for efficient optimization and generative modeling of complex data with continuous latent variables.",Reparameterization Trick: Enables efficient gradient optimization.,Generative Power: Combines deep learning with variational inference,Scalability: Works well with large datasets.,Approximate Posterior: Limited to simple distributions.,High Variance: Unstable gradient estimators.,Model Expressiveness: Dependent on neural network architecture.,39468.0,,,,,,8.png
9.0,Beta-VAE,v2.1.2.1,vae,세준,B-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework,2017.0,"Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, Alexander Lerchner",Google DeepMind,"Higgins, I., Matthey, L., Pal, A., Burgess, C. P., Glorot, X., Botvinick, M. M., ... & Lerchner, A. (2017). beta-vae: Learning basic visual concepts with a constrained variational framework. ICLR (Poster), 3.","Higgins, I., Matthey, L., Glorot, X., Pal, A., Uria, B., Blundell, C., ... & Lerchner, A. (2016). Early visual concept learning with unsupervised deep learning. arXiv preprint arXiv:1606.05579.","Karaletsos, T., Belongie, S., & Rätsch, G. (2015). Bayesian representation learning with oracle constraints. arXiv preprint arXiv:1506.05011.","The β-VAE model extends the traditional variational autoencoder (VAE) by introducing a hyperparameter β, which encourages learning disentangled representations of independent factors in visual data.",Disentangles data without supervision.,Improved interpretability of learned factors.,Achieves stable training across datasets.,May blur reconstructions at high β.,Optimal β varies by dataset.,Struggles with low data continuity.,5412.0,Disentanglement Metric Score,Comparison (%),"['VAE untrained', 'VAE', 'B-VAE']","[44,14, 61.58, 99.23]",%,9.png
10.0,SimCLR,v2.2.1,self_supervised,세준,A Simple Framework for Contrastive Learning of Visual Representations,2020.0,"Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton",Google Research Brain Team,"Chen, T., Kornblith, S., Norouzi, M., & Hinton, G. (2020, November). A simple framework for contrastive learning of visual representations. In International conference on machine learning (pp. 1597-1607). PMLR.","Khosla, P., Teterwak, P., Wang, C., Sarna, A., Tian, Y., Isola, P., ... & Krishnan, D. (2020). Supervised contrastive learning. Advances in neural information processing systems, 33, 18661-18673.","Wu, J., Hobbs, J., & Hovakimyan, N. (2023). Hallucination improves the performance of unsupervised visual representation learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 16132-16143).","SimCLR is a simple contrastive learning framework that improves unsupervised visual representation learning by maximizing agreement between augmented views of images, requiring no specialized architectures or memory bank.",High performance without labels.,Improves with larger batch sizes.,Stronger augmentation benefits learning.,Needs large computational resources.,High memory demand for large batches.,Sensitive to augmentation strategies​. ,20280.0,Top-1,Comparison (%),"['SimCLR using ResNet-50', 'SimCLR using other architectures']","[69.3, 76.5]",%,10.png
11.0,MoCo,v2.2.2,self_supervised,세준,Momentum Contrast for Unsupervised Visual Representation Learning,2020.0,"Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick",Facebook AI Research,"He, K., Fan, H., Wu, Y., Xie, S., & Girshick, R. (2020). Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 9729-9738).","Wei, C., Wang, H., Shen, W., & Yuille, A. (2020). Co2: Consistent contrast for unsupervised visual representation learning. arXiv preprint arXiv:2010.02217.","Pang, B., Zhang, Y., Li, Y., Cai, J., & Lu, C. (2022, October). Unsupervised visual representation learning by synchronous momentum grouping. In European Conference on Computer Vision (pp. 265-282). Cham: Springer Nature Switzerland.","MoCo is an unsupervised learning framework using contrastive learning to train visual representations. It builds a dynamic dictionary with a momentum-updated encoder and queue, enabling consistent, large-scale dictionaries for better representations, particularly in downstream tasks like detection and segmentation.","Builds a large, consistent dynamic dictionary with a queue.",Uses momentum-based updates for the key encoder.,Transfers effectively to various vision tasks.,High memory requirements due to large dictionary size.,Limited by the complexity of instance discrimination tasks.,Performance gain decreases with scale limitations.,13540.0,Accuracy,Comparison (%),"['Exemplar', 'RelativePosition', 'Jigsaw', 'Rotation', 'Colorization', 'DeepCluster', 'BigBiGAN', 'InstDisc', 'LocalAgg', 'CPCv1', 'CPCv2', 'CMC', 'AMDIM', 'MoCo']","[46.0, 51.4, 44.6, 55.4, 39.6, 48.4, 56.6, 54.0, 58.8, 48.7, 65.9, 64.1, 63.5, 60.6]",%,11.png
12.0,DINO,v2.3.1,self_supervised,세준,Emerging Properties in Self-Supervised Vision Transformers,2021.0,"Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, Armand Joulin",Facebook AI Research,"Caron, M., Touvron, H., Misra, I., Jégou, H., Mairal, J., Bojanowski, P., & Joulin, A. (2021). Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 9650-9660).","Chen, X., Xie, S., & He, K. (2021). An empirical study of training self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 9640-9649).","d’Ascoli, S., Touvron, H., Leavitt, M. L., Morcos, A. S., Biroli, G., & Sagun, L. (2021, July). Convit: Improving vision transformers with soft convolutional inductive biases. In International conference on machine learning (pp. 2286-2296). PMLR.","DINO (self-distillation with no labels) trains a Vision Transformer in a self-supervised manner. Using a student-teacher framework, the student network learns to predict the teacher's output through self-attention, enabling effective learning without labeled data.",Learns visual features without labeled data.,Enables object segmentation from attention maps.,Achieves strong performance in classification benchmarks.,Computationally intensive training.,"Requires specific configurations (momentum encoder, multi-crop).",Limited performance gains in smaller-scale applications.,5429.0,k-NN evaluation,Accuracy (%),"[‘Supervised RN50’, ‘SCLR’, ‘MoCov2’, ‘InfoMin’, ‘BarlowT’, ‘OBoW’, ‘BYOL’, ‘DCv2’, ‘SwAV’, ‘DINO RN50’]","[79.3, 69.1, 71.1, 73.0, 73.2, 73.8, 74.4, 75.2, 75.3, 75.3]",%,12.png
13.0,DINOv2,v2.3.2,self_supervised,세준,DINOv2: Learning Robust Visual Features without Supervision,2023.0,"Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Hervé Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, Piotr Bojanowski",Meta AI Research,"Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V., ... & Bojanowski, P. (2023). Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193.","Newell, A., & Deng, J. (2020). How useful is self-supervised pretraining for visual tasks?. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 7345-7354).","Fang, Y., Dong, L., Bao, H., Wang, X., & Wei, F. (2022). Corrupted image modeling for self-supervised visual pre-training. arXiv preprint arXiv:2202.03382.","DINOv2 is a large-scale self-supervised Vision Transformer model for learning robust, general-purpose visual features without supervision. It leverages a curated dataset and a suite of optimized training techniques to produce visual representations that work across various computer vision tasks without fine-tuning.",Uses curated dataset for enhanced feature quality.,Incorporates scalable training optimizations for efficiency.,Matches or surpasses weakly-supervised model benchmarks.,Requires significant computational resources.,"Bias remains toward high-income, Western data sources.",Environmental impact due to large-scale model training.,1648.0,Top-1,Linear Evaluation (%),"[‘MAE ViT-H/14’, ‘DINO ViT-B/8’, ‘iBOT ViT-L/16’, ‘DINOv2 ViT-S/14’, ‘DINOv2 ViT-B/14’, ‘DINOv2 ViT-L/14’, ‘DINOv2 ViT-g/14’]","[76.6, 79.2, 82.3, 81.1, 84.5, 86.3, 86.5]",%,13.png
17.0,AlexNet,v3.2.1,cnn,정윤,Imagenet classification with deep convolutional neural networks,2012.0,"Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton",University of Toronto,"Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25.","Sarker, Iqbal H. ""Machine learning: Algorithms, real-world applications and research directions."" SN computer science 2.3 (2021): 160.","Sarker, Iqbal H. ""Deep learning: a comprehensive overview on techniques, taxonomy, applications and research directions."" SN computer science 2.6 (2021): 420.","AlexNet is a deep convolutional neural network that achieved breakthrough performance on ImageNet using ReLU activation, dropout regularization, GPU training, and data augmentation to reduce overfitting.",Utilized GPUs for faster training.,Introduced ReLU for non-saturating activations.,Effective dropout regularization method.,High computational resource requirements.,Susceptible to overfitting without techniques.,Limited by GPU memory constraints.,135156.0,Top-1,Comparison (%),"['Sparse coding', 'SIFT + FVs', 'CNN']","[47.1, 45.7, 37.5]",%,17.png
18.0,VGGNet,v3.2.2,cnn,유안,Very Deep Convolutional Networks For Large-Scale Image Recognition,2014.0,"Karen Simonyan, Andrew Zisserman",University of Oxford,"Simonyan, K., & Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556.","Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022). High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 10684-10695).","Wang, C. Y., Bochkovskiy, A., & Liao, H. Y. M. (2023). YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 7464-7475).","The paper presents deep convolutional networks (up to 19 layers) using small 3x3 filters for large-scale image recognition, achieving state-of-the-art performance on the ImageNet dataset by focusing on depth rather than complex network structures.",Significant accuracy boost by deepening network.,Uses only 3x3 filters for simplification.,Outperforms previous models on ImageNet.,High computational cost and memory usage.,Longer training time with increased depth.,Dependent on large-scale labeled datasets.,132477.0,Top-1,Comparison (%),"['D_dense', 'D_multi-crop', 'D_multi-crop & dense']","[24.8, 24.6, 24.4]",%,18.png
19.0,GoogLeNet,v3.2.3,cnn,유안,Going Deeper with Convolutions,2014.0,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich",Google Inc.,"Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., ... & Rabinovich, A. (2015). Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1-9).","Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., ... & Guo, B. (2021). Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 10012-10022).","Wang, C. Y., Bochkovskiy, A., & Liao, H. Y. M. (2023). YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 7464-7475).",The GoogLeNet model (Inception architecture) improves deep network efficiency by using modular layers with mixed kernel sizes for multi-scale feature extraction while keeping computational cost low.,Multi-scale feature extraction via Inception modules.,Optimized for efficient computation with reduced parameters.,Superior performance in classification and detection tasks.,High memory demand during training.,Limited applicability to non-vision tasks.,Complexity in designing and tuning Inception modules.,62896.0,Top-5,Error (%),"['SuperVision', 'Clarifai', 'MSRA', 'VGG', 'GoogLeNet']","[16.4, 11.7, 7.35, 7.32, 6.67]",%,19.png
20.0,ResNet,v3.2.4,cnn,유안,Deep Residual Learning for Image Recognition,2015.0,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",Microsoft Research,"He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).","Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., ... & Girshick, R. (2023). Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 4015-4026).","He, K., Chen, X., Xie, S., Li, Y., Dollár, P., & Girshick, R. (2022). Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 16000-16009).","The paper introduces a deep residual learning framework to address difficulties in training very deep neural networks. Instead of learning direct mappings, residual networks (ResNets) learn residual functions with shortcut connections, which improves optimization and enables deeper models. ResNets show significant gains in accuracy and outperform shallower networks on multiple benchmarks.",Enables efficient training of extremely deep networks (up to 152 layers).,Uses residual connections to address degradation in deeper models.,Achieves state-of-the-art performance on ImageNet and COCO datasets.,Residual connections don't fully eliminate overfitting in very large datasets.,Requires substantial computational resources for deep models.,"May face diminishing returns with extreme depth (e.g., over 1000 layers).",242492.0,Top-1,Error (%),"['VGG-16', 'GoogLeNet', 'PReLU-net', 'plain-34', 'ResNet-34 A', 'ResNet-34 B', 'ResNet-34 C', 'ResNet-50', 'ResNet-101', 'ResNet-152']","[28.07, '-', 24.27, 28.54, 25.03, 24.52, 24.19, 22.85, 21.75, 21.43]",%,20.png
21.0,DenseNet,v3.2.5,cnn,유안,Densely Connected Convolutional Networks,2017.0,"Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger",Cornell University,"Huang, G., Liu, Z., Van Der Maaten, L., & Weinberger, K. Q. (2017). Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 4700-4708).","Liu, Z., Mao, H., Wu, C. Y., Feichtenhofer, C., Darrell, T., & Xie, S. (2022). A convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 11976-11986).","Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., ... & Guo, B. (2021). Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 10012-10022).","DenseNet is a deep neural network architecture where each layer is connected to every other layer to improve feature reuse, gradient flow, and efficiency, enabling powerful performance with fewer parameters.",Improved feature reuse through dense connections.,"Mitigates vanishing gradient, making training easier.",Achieves high accuracy with fewer parameters.,Increased memory usage due to dense connections.,May require specific growth rate tuning.,Dense connectivity can slow down computations.,48567.0,Top-1 (single-crop),Error (%),"['DenseNet-121 (k=32)', 'DenseNet-169 (k=32)', 'DenseNet-201 (k=32)', 'DenseNet-161 (k=48)']","[25.02, 23.80, 22.58, 22.33]",%,21.png
22.0,EfficientNet,v3.2.6,cnn,유안,EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,2019.0,"Mingxing Tan, Quoc Le",Google Research,"Tan, M., & Le, Q. (2019, May). Efficientnet: Rethinking model scaling for convolutional neural networks. In International conference on machine learning (pp. 6105-6114). PMLR.","Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., ... & Sutskever, I. (2021, July). Learning transferable visual models from natural language supervision. In International conference on machine learning (pp. 8748-8763). PMLR.","Bochkovskiy, A., Wang, C. Y., & Liao, H. Y. M. (2020). Yolov4: Optimal speed and accuracy of object detection. arXiv preprint arXiv:2004.10934.","EfficientNet is a family of Convolutional Neural Networks that scales width, depth, and resolution using a compound scaling method, achieving high accuracy with fewer parameters. This balanced scaling approach, coupled with a new baseline network, allows EfficientNet to outperform previous models in accuracy and efficiency across various tasks and datasets.","Balances width, depth, and resolution with compound scaling.",Achieves high accuracy with fewer parameters than prior models.,State-of-the-art performance on ImageNet and transfer learning datasets.,Limited to single-scaling baseline network.,Heavy reliance on neural architecture search.,High computational resources needed for model training.,23775.0,Top-1,Accuracy (%),"['EfficientNet-B0', 'ResNet-50', 'DenseNet-169', 'EfficientNet-B1', 'ResNet-152', 'DenseNet-264', 'Inception-v3', 'Xception', 'EfficientNet-B2', 'Inception-v4', 'Inception-resnet-v2', 'EfficientNet-B3', 'ResNeXt-101', 'PolyNet', 'EfficientNet-B4', 'SENet', 'NASNet-A', 'AmoebaNet-A', 'PNASNet', 'EfficientNet-B5', 'AmoebaNet-C', 'EfficientNet-B6', 'EfficientNet-B7', 'GPipe']","[76.3, 76.0, 76.2, 78.8, 77.8, 77.9, 78.8, 79.0, 79.8, 80.0, 80.1, 81.1, 80.9, 81.3, 82.6, 82.7, 82.7, 82.8, 82.9, 83.3, 83.5, 84.0, 84.4, 84.3]",%,22.png
23.0,Vision Transformer (ViT),v3.3.1,transformer,민혁,An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,2020.0,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby",,"Dosovitskiy, A. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.","Bao, H., Dong, L., Piao, S., & Wei, F. (2021). Beit: Bert pre-training of image transformers. arXiv preprint arXiv:2106.08254.","Chen, C. F. R., Fan, Q., & Panda, R. (2021). Crossvit: Cross-attention multi-scale vision transformer for image classification. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 357-366).","The Vision Transformer (ViT) divides images into patches, treats them as tokens like words in NLP, and uses self-attention to capture relationships across the image. Trained on large datasets, it outperforms convolutional networks in classification accuracy with fewer computational resources",Applies Transformers directly to image patches as input tokens.,"Achieves strong accuracy, especially when trained on large datasets.",Reduces dependence on convolutional neural networks (CNNs).,Requires large datasets for optimal performance.,"Lacks CNNs’ inherent inductive biases, like spatial locality.","Not as effective on small datasets without pre-training.




",47287.0,Mean,Accuracy (%),"['ViT-H/14', 'ViT-L/16', 'ViT-L/16', 'ResNet52x4', 'EfficientNet-L2']","[88.55, 87.76, 85.30, 87.54, 88.4]",%,23.png
24.0,Swin Transformer,v3.3.2,transformer,민혁,Swin Transformer: Hierarchical Vision Transformer using Shifted Windows,2021.0,"Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo","Microsoft Research Asia, University of Science and Technology of China, Xian Jiaotong University, Tsinghua University","Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., ... & Guo, B. (2021). Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 10012-10022).","Guo, J., Han, K., Wu, H., Tang, Y., Chen, X., Wang, Y., & Xu, C. (2022). Cmt: Convolutional neural networks meet vision transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 12175-12185).","Chen, Z., Duan, Y., Wang, W., He, J., Lu, T., Dai, J., & Qiao, Y. (2022). Vision transformer adapter for dense predictions. arXiv preprint arXiv:2205.08534.","The Swin Transformer builds hierarchical feature maps with local self-attention in shifted windows, which connects neighboring windows efficiently. This enables strong performance on image classification, object detection, and segmentation tasks by modeling at various scales with linear computational complexity relative to image size.",Uses shifted windows for local self-attention.,Achieves state-of-the-art results on multiple vision tasks.,Scales efficiently with linear computational complexity.,Requires significant computational resources for training.,Limited adaptability for very small datasets.,Challenging optimization for certain dense prediction tasks.,23368.0,Average,Precision (%),"['DeiT-S', 'R50', 'Swin-T']","[48.0, 46.3, 50.5]",%,24.png
25.0,DeiT,v3.3.3,transformer,민혁,Training data-efficient image transformers & distillation through attention,2021.0,"Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Hervé Jegou",,"Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., & Jégou, H. (2021, July). Training data-efficient image transformers & distillation through attention. In International conference on machine learning (pp. 10347-10357). PMLR.","Yuan, L., Chen, Y., Wang, T., Yu, W., Shi, Y., Jiang, Z. H., ... & Yan, S. (2021). Tokens-to-token vit: Training vision transformers from scratch on imagenet. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 558-567).","Chen, C. F. R., Fan, Q., & Panda, R. (2021). Crossvit: Cross-attention multi-scale vision transformer for image classification. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 357-366).","DeiT (Data-efficient Image Transformers) leverages a teacher-student distillation approach, with a unique ""distillation token"" to improve data efficiency and achieve strong results on ImageNet without large pre-training datasets. It enables Vision Transformers (ViT) to be trained more effectively on smaller datasets, overcoming prior data limitations​.",Introduces a distillation token for efficient learning.,Matches CNNs’ performance on ImageNet using smaller data.,Achieves high accuracy with minimal pre-training requirements.,Limited to classification tasks initially.,Requires a strong teacher model for distillation.,Performance depends on extensive data augmentation.,6997.0,Top-1,Accuracy (%),,,,25.png
26.0,EVA,v3.3.4,transformer,민혁,EVA: Exploring the Limits of Masked Visual Representation Learning at Scale,2023.0,"Zhang Ge, Yuxin Fang, Yuxin Wang, Kunchang Li, Ziyi Lin, Wenguang Zhou, Fengwei Yu, Xiangyu Zhang, Tong Lu, Jianbin Jiao, Yu Qiao, Xiaogang Wang, Jianmin Bao, Han Hu","Huazhong University of Science and Technology, Beijing Academy of Artificial Intelligence, Zhejiang University, Beijing Institute of Technology","Fang, Y., Wang, W., Xie, B., Sun, Q., Wu, L., Wang, X., ... & Cao, Y. (2023). Eva: Exploring the limits of masked visual representation learning at scale. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 19358-19369).","Gao, P., Geng, S., Zhang, R., Ma, T., Fang, R., Zhang, Y., ... & Qiao, Y. (2024). Clip-adapter: Better vision-language models with feature adapters. International Journal of Computer Vision, 132(2), 581-595.","Wang, Z., Yu, J., Yu, A. W., Dai, Z., Tsvetkov, Y., & Cao, Y. (2021). Simvlm: Simple visual language model pretraining with weak supervision. arXiv preprint arXiv:2108.10904.","EVA is a large-scale vision model designed to maximize the power of masked image modeling (MIM) by using image-text aligned features. It efficiently scales up to one billion parameters, achieving state-of-the-art results across various tasks, including object detection, segmentation, and classification, using only publicly accessible data​.",Scales MIM with image-text aligned features.,Achieves top results across multiple vision tasks.,Uses efficient training with only public data.,"Relies heavily on large, unlabeled datasets.",High computational requirements for training.,Less effective on smaller or labeled datasets.,579.0,Top-1,Accuracy (%),"['CoAtNet-4', 'MaxViT-XL', 'MViTv2-H', 'FD-CLIP-L', 'BEiT-3', 'EVA']","[88.6, 88.7, 88.8, 89.0, 89.6, 89.6]",%,26.png
27.0,U-Net,v3.4.1,cnn,민혁,U-Net: Convolutional Networks for Biomedical Image Segmentation,2015.0,"Olaf Ronneberger, Philipp Fischer, and Thomas Brox","University of Freiburg, Germany","Ronneberger, O., Fischer, P., & Brox, T. (2015). U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted intervention–MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18 (pp. 234-241). Springer International Publishing.","Zunair, H., & Hamza, A. B. (2021). Sharp U-Net: Depthwise convolutional network for biomedical image segmentation. Computers in biology and medicine, 136, 104699.","Kaul, C., Manandhar, S., & Pears, N. (2019, April). Focusnet: An attention-based fully convolutional network for medical image segmentation. In 2019 IEEE 16th international symposium on biomedical imaging (ISBI 2019) (pp. 455-458). IEEE.",U-Net is a convolutional network designed for biomedical image segmentation. It has a U-shaped architecture with a contracting path for context capture and an expansive path for precise localization. This structure enables accurate segmentation with limited training data by using data augmentation and efficient feature propagation.,Efficient segmentation with minimal training data.,U-shaped design for context and localization.,Uses extensive data augmentation techniques.,Primarily tested on biomedical images.,Performance sensitive to data augmentation.,Limited generalization beyond specific domains.,95843.0,Average,IOU,"['IMCB-SG', 'KTH-SE', 'HOUS-US', 'second-best', 'u-net']","[0.2669, 0.7953, 0.5323, 0.83, 0.9203]",,27.png
61.0,ELECTRA,v4.2.3,transformer,유안,ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators,2020.0,"Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning",Google Research,"Clark, K. (2020). Electra: Pre-training text encoders as discriminators rather than generators. arXiv preprint arXiv:2003.10555.","He, K., Chen, X., Xie, S., Li, Y., Dollár, P., & Girshick, R. (2022). Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 16000-16009).","Liu, Z., Mao, H., Wu, C. Y., Feichtenhofer, C., Darrell, T., & Xie, S. (2022). A convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 11976-11986).","ELECTRA is a pre-training method for NLP that trains a model to detect if input tokens were replaced by plausible alternatives, rather than generating tokens as in BERT. This approach achieves higher efficiency and performance, especially on smaller models.","Learns from all tokens, improving compute efficiency.",Outperforms BERT on downstream tasks with less compute.,Particularly effective on small and medium-sized models.,Slightly lower masked language modeling accuracy than BERT.,More complex architecture with generator-discriminator setup.,Limited fine-tuning compatibility across some generative tasks.,4349.0,GLUE,GLUE Score,"['ELMo', 'GPT', 'BERT-Small', 'BERT-Base', 'ELECTRA-Small', 'ELECTRA-Base']","[71.2, 78.8, 75.1, 82.2, 79.9, 85.1]",,61.png
62.0,Longformer,v4.2.3.1,transformer,유안,Longformer: The Long-Document Transformer,2020.0,"Iz Beltagy, Matthew E. Peters, Arman Cohan","Allen Institute for Artificial Intelligence, Seattle, WA, USA","Beltagy, I., Peters, M. E., & Cohan, A. (2020). Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150.","Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., ... & Liang, P. (2021). On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258.","Zhu, X., Su, W., Lu, L., Li, B., Wang, X., & Dai, J. (2020). Deformable detr: Deformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159.",Longformer is a transformer model optimized for long documents using an efficient attention mechanism that scales linearly with sequence length.,Linear scaling attention for processing long sequences,Outperforms RoBERTa on document-level tasks,Introduces Longformer-Encoder-Decoder (LED) for long document generation tasks,Limited pretrained resources for longer sequences,Dependence on task-specific attention configurations,Computationally intensive fine-tuning,4282.0,ROUGE-1,ROUGE-1 Score,"['Discourse-aware', 'Extr-Abst-TLM', 'Dancer', 'Pegasus', 'LED-large (seqlen: 4,096)', 'BigBird (seqlen: 4,096)', 'LED-large (seqlen: 16,384)']","[35.80, 41.62, 42.70, 44.21, 44.40, 46.63, 46.63]",,62.png
63.0,Reformer,v4.2.3.2,transformer,유안,Reformer: The Efficient Transformer,2020.0,"Nikita Kitaev, Łukasz Kaiser, Anselm Levskaya",Google Research,"Kitaev, N., Kaiser, Ł., & Levskaya, A. (2020). Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451.","Zhou, H., Zhang, S., Peng, J., Zhang, S., Li, J., Xiong, H., & Zhang, W. (2021, May). Informer: Beyond efficient transformer for long sequence time-series forecasting. In Proceedings of the AAAI conference on artificial intelligence (Vol. 35, No. 12, pp. 11106-11115).","Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., ... & Fiedel, N. (2023). Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240), 1-113.","Reformer is an efficient variant of Transformer that reduces memory and computational costs for long sequences by using locality-sensitive hashing for attention and reversible layers, allowing it to train faster with less memory on large datasets.","Efficient Attention: LSH reduces attention complexity from 
𝑂(𝐿^2) to 𝑂(𝐿log⁡𝐿).",Memory Savings: Reversible layers eliminate layer-wise memory buildup.,"Scalability: Handles long sequences, applicable for text, image, and music generation.
",Approximation in Attention: LSH attention can slightly reduce accuracy.,Limited Hash Control: Hash collisions may miss some context.,Performance: Optimal only with specific hyperparameter tuning.,2825.0,LSH-4,Accuracy (%),"['Full Attention', 'LSH-8', 'LSH-4', 'LSH-2', 'LSH-1']","[0.8, 100, 99.9, 99.4, 91.9]",%,63.png
64.0,BigBird,v4.2.3.3,transformer,유안,Big Bird: Transformers for Longer Sequences,2020.0,"Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed
",Google Research,"Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti, C., Ontanon, S., ... & Ahmed, A. (2020). Big bird: Transformers for longer sequences. Advances in neural information processing systems, 33, 17283-17297.","Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., ... & Fiedel, N. (2023). Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240), 1-113.","Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., ... & Liang, P. (2021). On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258.","BIGBIRD is a transformer model designed to handle longer sequences by using sparse attention patterns, which reduce memory and computation costs from quadratic to linear complexity. This approach maintains the expressivity of full self-attention, enabling improved performance on long-sequence NLP and genomics tasks.",Scales to sequences 8x longer than BERT.,Maintains Turing completeness and universal approximation properties.,"Achieves state-of-the-art in QA, summarization, and genomics.",Requires specialized sparse attention structure.,Performance advantage varies with task and sequence length.,Sparse attention models may need more layers for complex tasks.,2245.0,Arxiv ROUGE-1,ROUGE Scores,"['SumBasic', 'LexRank', 'LSA', 'Attn-Seq2Seq', 'Pntr-Gen-Seq2Seq', 'Long-Doc-Seq2Seq', 'Sent-CLF', 'Sent-PTR', 'Extr-Abst-TLM', 'Dancer']","[29.47, 33.85, 29.91, 29.30, 32.06, 35.80, 34.01, 42.32, 41.62, 42.70]",,64.png
65.0,Switch Transformer,v4.2.4,transformer,유안,Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity,2021.0,"William Fedus, Barret Zoph, Noam Shazeer",Google Research,"Fedus, W., Zoph, B., & Shazeer, N. (2022). Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120), 1-39.","Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., ... & Lowe, R. (2022). Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35, 27730-27744.","Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., ... & Fiedel, N. (2023). Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240), 1-113.","The Switch Transformer is a sparse, scalable neural network model using Mixture of Experts (MoE) to route different parameters per input, significantly increasing model size without raising computational costs. This architecture improves training efficiency, enables multi-language learning, and achieves high performance even at large model scales.",Achieves up to 7x speedup in pre-training compared to dense models.,Scales efficiently up to a trillion parameters.,Maintains high performance with limited computational resources.,"Training can be unstable, especially with large models.",Significant communication costs in distributed setups.,Sparse model fine-tuning requires complex regularization strategies.,1804.0,GLUE,GLUE Score,"['T5-Base (d=0.1)', 'Switch-Base (d=0.1)', 'Switch-Base (d=0.2)', 'Switch-Base (d=0.3)', 'Switch-Base (d=0.1, ed=0.4)']","[82.9, 84.7, 84.4, 83.9, 85.2]",,65.png
66.0,ALBERT,v4.2.5,transformer,유안,ALBERT: A Lite BERT for Self-supervised Learning of Language Representations,2019.0,"Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut",Google Research,"Lan, Z. (2019). Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942.","Bender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021, March). On the dangers of stochastic parrots: Can language models be too big?🦜. In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency (pp. 610-623).","Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., & Neubig, G. (2023). Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9), 1-35.","ALBERT is an optimized, lightweight variant of BERT for language understanding that reduces parameters through factorized embedding and cross-layer parameter sharing, maintaining performance while increasing efficiency. It also replaces BERT's NSP loss with a Sentence Order Prediction (SOP) task for coherence modeling.","Achieves high accuracy on GLUE, SQuAD, and RACE benchmarks.","Significantly fewer parameters than BERT, reducing memory usage.",Sentence Order Prediction (SOP) enhances performance in multi-sentence tasks.,Training is computationally intensive for larger configurations.,Limited by cross-layer sharing in capturing deep-layer interactions.,"Optimal hyperparameter tuning required for peak performance.





",7861.0,GLUE,GLUE Score,"['BERT-base', 'BERT-large', 'ALBERT-base', 'ALBERT-large', 'ALBERT-xlarge', 'ALBERT-xxlarge']","[84.5, 86.6, 81.6, 83.5, 86.4, 88.0]",,66.png
67.0,DistilBERT,v4.2.6,transformer,유안,"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",2019.0,"Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf",Hugging Face,"Sanh, V. (2019). DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108.","Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., ... & Liang, P. (2021). On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258.","Bender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021, March). On the dangers of stochastic parrots: Can language models be too big?🦜. In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency (pp. 610-623).","DistilBERT is a smaller, faster, and more efficient version of BERT, created through knowledge distillation, where a student model learns from a larger ""teacher"" BERT model. It retains 97% of BERT's performance with 40% fewer parameters, making it suitable for low-compute environments like mobile devices.",40% smaller with 60% faster inference than BERT.,Retains 97% of BERT's language understanding capabilities.,Suitable for on-device and edge applications.,"Slightly lower accuracy than BERT in certain NLP tasks.
",Performance limited by reduced model complexity.,May need additional fine-tuning for specific tasks.,7456.0,GLUE,GLUE Score,"['ELMo', 'BERT-base', 'DistilBERT']","[44.1, 56.3, 51.3]",,67.png
68.0,ERNIE,v4.3.1,transformer,유안,ERNIE: Enhanced Representation through Knowledge Integration,2019.0,"Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, Hua Wu",Baidu Inc.,"Sun, Y., Wang, S., Li, Y., Feng, S., Chen, X., Zhang, H., ... & Wu, H. (2019). Ernie: Enhanced representation through knowledge integration. arXiv preprint arXiv:1904.09223.","Bender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021, March). On the dangers of stochastic parrots: Can language models be too big?🦜. In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency (pp. 610-623).","Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., & Neubig, G. (2023). Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9), 1-35.","ERNIE is a language representation model that improves on BERT by incorporating knowledge-based masking strategies, including phrase and entity-level masking. This approach enables ERNIE to capture deeper semantic and syntactic information, enhancing performance on various Chinese NLP tasks.","Knowledge-enhanced masking strategies (phrase and entity levels) improve language understanding.
",Achieves state-of-the-art results on five Chinese NLP tasks.,Demonstrates improved knowledge inference through a novel dialogue language model (DLM).,"Designed specifically for Chinese, limiting generalization across languages.","Heavy reliance on large, heterogeneous datasets.",Limited testing on non-dialogue NLP applications.,1153.0,F1,Accuracy (%),"['BERT (dev)', 'BERT (test)', 'ERNIE (dev)', 'ERNIE (test)']","[78.1, 77.2, 79.9, 78.4]",%,68.png
69.0,XLM,v4.3.2,transformer,유안,Cross-lingual Language Model Pretraining,2019.0,"Alexis CONNEAU, Guillaume Lample",Facebook AI Research,"Conneau, A., & Lample, G. (2019). Cross-lingual language model pretraining. Advances in neural information processing systems, 32.","Xue, L. (2020). mt5: A massively multilingual pre-trained text-to-text transformer. arXiv preprint arXiv:2010.11934.","Qiu, X., Sun, T., Xu, Y., Shao, Y., Dai, N., & Huang, X. (2020). Pre-trained models for natural language processing: A survey. Science China technological sciences, 63(10), 1872-1897.","This paper introduces cross-lingual language models (XLMs) that leverage both monolingual and parallel data for language understanding across multiple languages. The model is trained using Causal Language Modeling (CLM), Masked Language Modeling (MLM), and a Translation Language Modeling (TLM) objective to improve performance on cross-lingual tasks.",State-of-the-art accuracy on XNLI cross-lingual classification and machine translation benchmarks.,Effective in zero-shot cross-lingual transfer for various NLP tasks.,Publicly available code and pretrained models on GitHub.,Requires large computational resources for training.,"Performance decrease on distant language pairs without shared vocabulary.
",Limited effectiveness in very low-resource language settings.,1655.0,BLEU,BLEU Score,"['- -', 'EMB EMB', 'CLM CLM', 'MLM MLM', 'CLM -', 'MLM -', '- CLM', '- MLM', 'CLM MLM', 'MLM CLM']","[13.0, 29.4, 30.4, 33.4, 28.7, 31.6, 25.3, 29.2, 32.3, 33.4]",,69.png
70.0,mT5,v4.3.3,transformer,유안,mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer,2021.0,"Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua,  Colin Raffel",Google Research,"Xue, L. (2020). mt5: A massively multilingual pre-trained text-to-text transformer. arXiv preprint arXiv:2010.11934.","Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., & Neubig, G. (2023). Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9), 1-35.","Bender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021, March). On the dangers of stochastic parrots: Can language models be too big?🦜. In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency (pp. 610-623).","mT5 is a massively multilingual variant of the T5 model, pre-trained on a Common Crawl-based dataset in 101 languages, achieving state-of-the-art multilingual performance with a unified text-to-text format.",Supports 101 languages with a unified text-to-text framework.,Achieves state-of-the-art results on multilingual benchmarks.,"Introduces a technique to prevent ""accidental translation.""",Struggles with low-resource languages.,Requires high computational resources for large-scale models.,"Still prone to some ""illegal"" predictions in zero-shot settings.





",2320.0,Accuracy,Accuracy (%),"['mBERT', 'XLM', 'InfoXLM', 'X-STILTs', 'XLM-R', 'VECO', 'RemBERT']","[65.4, 69.1, 81.4, 80.4, 79.2, 70.9, 80.8]",%,70.png
71.0,Generative Adversarial Networks (GANs),v5.0,gan,유안,Generative Adversarial Nets,2014.0,"Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio",Departement d’informatique et de recherche op ´ erationnelle,"Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., ... & Bengio, Y. (2014). Generative adversarial nets. Advances in neural information processing systems, 27.","Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022). High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 10684-10695).","Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., & Aberman, K. (2023). Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 22500-22510).","GANs use two neural networks—a generator that creates samples and a discriminator that distinguishes between real and fake samples. The two networks compete, improving each other iteratively until the generator produces data indistinguishable from real samples.",Adversarial Framework: GANs train with a competitive process between generator and discriminator networks.,"Efficient Sample Generation: No need for Markov chains, allowing faster, direct sample generation.","Flexible Design: Works with differentiable functions, enhancing model versatility and potential applications.","Training Instability: Synchronization between networks is challenging, leading to potential convergence issues.","Mode Collapse: Generator may produce limited diversity, generating similar outputs.",No Direct Probability Estimate: GANs lack explicit probability estimates for generated samples.,74528.0,MNIST,MNIST,"['DBN', 'Stacked CAE', 'Deep GSN', 'Adversarial nets']","[138, 121, 214, 225]",,71.png
72.0,DCGAN,v5.0.1.1,gan,유안,Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks,2015.0,"Alec Radford, Luke Metz, Soumith Chintala",indico Research,"Radford, A. (2015). Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434.","Khan, S., Naseer, M., Hayat, M., Zamir, S. W., Khan, F. S., & Shah, M. (2022). Transformers in vision: A survey. ACM computing surveys (CSUR), 54(10s), 1-41.","Saharia, C., Ho, J., Chan, W., Salimans, T., Fleet, D. J., & Norouzi, M. (2022). Image super-resolution via iterative refinement. IEEE transactions on pattern analysis and machine intelligence, 45(4), 4713-4726.","This paper introduces Deep Convolutional GANs (DCGANs), an architecture for unsupervised image representation learning and generation, demonstrating stability and feature extraction quality in unsupervised learning.",Introduced stable architectural constraints for GANs.,DCGANs perform well in unsupervised image feature extraction.,Demonstrated visualizations of learned filters activating on specific image features.,Some model instability during extended training.,Limited applications to smaller image resolutions.,Manual data preprocessing and architecture tuning required.,18893.0,Error,Error Rate (%),"['KNN', 'TSVM', 'M1+KNN', 'M1+TSVM', 'M1+M2', 'SWWAE without dropout', 'SWWAE with dropout', 'DCGAN (ours) + L2-SVM', 'Supervised CNN with same architecture']
","[77.93, 66.55, 65.63, 54.33, 36.02, 27.83, 23.56, 22.48, 28.87]",%,72.png
73.0,WGAN,v5.0.1.2,gan,유안,Wasserstein Generative Adversarial Networks,2017.0,"Martin Arjovsky, Soumith Chintala, Léon Bottou",,"Arjovsky, M., Chintala, S., & Bottou, L. (2017, July). Wasserstein generative adversarial networks. In International conference on machine learning (pp. 214-223). PMLR.","Song, J., Meng, C., & Ermon, S. (2020). Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502.","Wu, Z., Pan, S., Chen, F., Long, G., Zhang, C., & Philip, S. Y. (2020). A comprehensive survey on graph neural networks. IEEE transactions on neural networks and learning systems, 32(1), 4-24.","Wasserstein GANs (WGANs) improve Generative Adversarial Network (GAN) training stability by minimizing the Wasserstein (Earth Mover) distance instead of traditional divergence measures, addressing issues like mode collapse and providing more reliable loss metrics for tuning and debugging.
",Stable training: WGANs reduce mode collapse and improve stability in GAN training.,Meaningful loss metric: The Wasserstein distance provides interpretable learning curves.,Theoretical soundness: Robust foundation in probability and optimal transport theory.,Weight clipping: Limiting gradients restricts discriminator capacity.,Slow convergence: Training critic to optimality can be computationally intensive.,Sensitive to optimizer: Momentum-based optimizers may destabilize training.,17460.0,,,,,,73.png
74.0,Progressive GAN,v5.0.1.3,gan,유안,"Progressive Growing of GANs for Improved Quality, Stability, and Variation",2018.0,"Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen",NVIDIA,"Karras, T. (2017). Progressive Growing of GANs for Improved Quality, Stability, and Variation. arXiv preprint arXiv:1710.10196.","Shorten, C., & Khoshgoftaar, T. M. (2019). A survey on image data augmentation for deep learning. Journal of big data, 6(1), 1-48.","Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022). High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 10684-10695).","The paper introduces a technique for progressively growing GANs by starting from low resolutions and adding layers, resulting in improved stability and high-quality, high-resolution images.",Progressive growth enhances image quality and stability.,Successfully generates 1024x1024 resolution images.,Achieves high variation and improved inception scores.,Increased training complexity for high resolutions.,"High computational resource requirements.
","Stability may vary across datasets and GAN loss types.




",8838.0,MS-SSIM,MS-SSIM,"['Gulrajani et al. (2017)', '+ Progressive growing', '+ Small minibatch', '+ Revised training parameters', '+ Minibatch discrimination', 'Minibatch stddev', '+ Equalized learning rate', '+ Pixelwise normalization', 'Converged']","[0.0587, 0.0615, 0.1061, 0.0662, 0.0648, 0.0671, 0.0668, 0.0640, 0.0636]",,74.png
75.0,Conditional GAN,v5.0.2.1,gan,유안,Conditional Generative Adversarial Nets,2014.0,"Mehdi Mirza, Simon Osindero",Departement d’informatique et de recherche op ´ erationnelle ,"Mirza, M. (2014). Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784.","Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022). High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 10684-10695).","Dhariwal, P., & Nichol, A. (2021). Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34, 8780-8794.","Conditional Generative Adversarial Networks (CGANs) extend traditional GANs by conditioning both the generator and discriminator on auxiliary information, allowing for targeted generation, such as producing MNIST digits labeled by class or multi-modal image tags.","Targeted Generation: CGANs generate samples conditioned on specified data, like class labels or image features.
","Flexible Training: The model adapts auxiliary inputs for different applications, including image tagging and multimodal learning.","Improved Accuracy: CGANs outperform standard GANs in specific tasks by utilizing extra conditioning information.
",Complexity in Hyperparameters: Effective training requires careful tuning of hyperparameters and architectures.,Limited Efficacy in Large Datasets: CGANs may struggle to scale efficiently to larger datasets without optimization.,"Potential for Mode Collapse: Like GANs, CGANs can suffer from mode collapse, reducing diversity in generated outputs.",14155.0,MNIST,MNIST,"['DBN', 'Stacked CAE', 'Deep GSN', 'Adversarial nets', 'Conditional adversarial nets']","[138, 121, 214, 225, 132]",,75.png
76.0,CycleGAN,v5.0.2.2,gan,유안,Unpaired Image-To-Image Translation Using Cycle-Consistent Adversarial Networks,2017.0,"Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A. Efros","Berkeley AI Research (BAIR) laboratory, UC Berkeley","Zhu, J. Y., Park, T., Isola, P., & Efros, A. A. (2017). Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proceedings of the IEEE international conference on computer vision (pp. 2223-2232).","Croitoru, F. A., Hondru, V., Ionescu, R. T., & Shah, M. (2023). Diffusion models in vision: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(9), 10850-10869.","Zhang, L., Rao, A., & Agrawala, M. (2023). Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 3836-3847).","The CycleGAN model enables image-to-image translation between two domains without paired examples, using cycle-consistency and adversarial losses to learn mappings that preserve the characteristics of each domain while translating images.",Cycle-consistency loss: Ensures translation back to original form after round-trip transformation.,Unpaired data usage: Trains without needing paired images.,"Diverse applications: Applicable to style transfer, season change, and more.",Geometry limitations: Struggles with transformations requiring structural changes.,"Mode collapse risk: Can map different inputs to identical outputs.
",Semantic ambiguity: May confuse similar categories without weak supervision.,24951.0,Class IOU,Class IOU,"['CoGAN', 'BiGAN/ALI', 'Pixel loss + GAN', 'Feature loss + GAN', 'CycleGAN', 'pix2pix']
","[0.08, 0.07, 0.07, 0.06, 0.16, 0.32]",,76.png
77.0,Pix2Pix,v5.0.2.3,gan,유안,Image-To-Image Translation With Conditional Adversarial Networks,2017.0,"Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros","Berkeley AI Research (BAIR) Laboratory, UC Berkeley","Isola, P., Zhu, J. Y., Zhou, T., & Efros, A. A. (2017). Image-to-image translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1125-1134).","Zhang, L., Rao, A., & Agrawala, M. (2023). Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 3836-3847).","Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022). High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 10684-10695).","The model is a conditional Generative Adversarial Network (cGAN) used for translating input images into corresponding output images across various applications (e.g., black-and-white to color, labels to photos) by learning mappings and suitable loss functions, yielding realistic and detailed images.",General-purpose framework for diverse image-to-image tasks,Learns both mappings and loss functions automatically,"Generates sharp, realistic images without hand-engineered losses",Limited stochasticity in output variations,Struggles with precise geometric transformations,High computational requirements,24742.0,Class IOU,Class IOU,"['L1', 'GAN', 'cGAN', 'L1+GAN', 'L1+cGAN', 'Ground truth']","[0.11, 0.01, 0.16, 0.15, 0.17, 0.21]",,77.png
78.0,StyleGAN,v5.0.3.1,gan,유안,A Style-Based Generator Architecture for Generative Adversarial Networks,2019.0,"Tero Karras, Samuli Laine, Timo Aila",NVIDIA,"Karras, T., Laine, S., & Aila, T. (2019). A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 4401-4410).","Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022). High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 10684-10695).","Zhang, L., Rao, A., & Agrawala, M. (2023). Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 3836-3847).","This paper introduces a new generator architecture for GANs that enhances control over image synthesis by separating high-level attributes from stochastic details. By modifying the generator structure, it enables better disentanglement in the latent space, resulting in improved image quality, interpolation, and attribute control.",Enables scale-specific control of high-level attributes and stochastic details.,"Improves interpolation and disentanglement of latent factors.
","Introduces FFHQ, a new, varied dataset of human faces.",Dependence on high-quality dataset like FFHQ for optimal performance.,Increased complexity due to additional intermediate layers and noise inputs.,"Some disentanglement metrics require additional classifiers, increasing computational cost.",12400.0,FID,FID,"['Traditional 0 Z', 'Traditional 8 Z', 'Traditional 8 W', 'Style-based 0 Z', 'Style-based 1 W', 'Style-based 2 W', 'Style-based 8 W']","[5.25, 4.87, 4.87, 5.06, 4.60, 4.43, 4.40]",,78.png
79.0,StyleGAN2,v5.0.3.2,gan,유안,Analyzing and Improving the Image Quality of StyleGAN,2020.0,"Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, Timo Aila",NVIDIA,"Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J., & Aila, T. (2020). Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 8110-8119).","Khan, S., Naseer, M., Hayat, M., Zamir, S. W., Khan, F. S., & Shah, M. (2022). Transformers in vision: A survey. ACM computing surveys (CSUR), 54(10s), 1-41.","Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022). High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 10684-10695).","The improved StyleGAN model, StyleGAN2, enhances image quality by addressing artifacts in the original architecture through refined normalization, training techniques, and network adjustments.",Enhanced image quality through path length regularization.,New architecture without progressive growing removes artifacts.,Easier inversion for source attribution.,High computational resource demands.,Limited to data-rich environments.,Metrics may overlook shape-based image quality improvements.,6790.0,FID,FID,"['Baseline StyleGAN', '+ Weight demodulation', '+ Lazy regularization', '+ Path length regularization', '+ No growing, new G & D arch.', '+ Large networks (StyleGAN2)', 'Config A with large networks']"," [4.40, 4.39, 4.38, 4.34, 3.31, 2.84, 3.98]",,79.png
80.0,StyleGAN3,v5.0.3.3,gan,유안,Alias-Free Generative Adversarial Networks,2021.0,"Tero Karras, Miika Aittala, Samuli Laine, Erik Härkönen, Janne Hellsten, Jaakko Lehtinen, Timo Aila",NVIDIA,"Karras, T., Aittala, M., Laine, S., Härkönen, E., Hellsten, J., Lehtinen, J., & Aila, T. (2021). Alias-free generative adversarial networks. Advances in neural information processing systems, 34, 852-863.","Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., & Aberman, K. (2023). Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 22500-22510).","Karras, T., Aittala, M., Aila, T., & Laine, S. (2022). Elucidating the design space of diffusion-based generative models. Advances in neural information processing systems, 35, 26565-26577.","Alias-Free Generative Adversarial Networks (StyleGAN3) eliminate spatial aliasing in image generation by enforcing continuous translation and rotation equivariance, producing more natural transformations and avoiding ""texture sticking"" issues present in previous GAN architectures.
","Equivariant to translation and rotation even at sub-pixel scales.
","Solves ""texture sticking,"" creating smoother, more coherent image transformations.",Matches StyleGAN2 in FID while improving animation and video generation suitability.,Increased computational costs and training times.,Sensitive to aliasing in the training data.,"Improvements only applied to the generator, not the discriminator.",1640.0,FID,FID,"['StyleGAN2', '+ Fourier features', '+ No noise inputs', '+ Simplified generator', '+ Boundaries & upsampling', '+ Filtered nonlinearities', '+ Non-critical sampling', '+ Transformed Fourier features', '+ Flexible layers (StyleGAN3-T)', '+ Rotation equiv. (StyleGAN3-R)']"," [5.14, 4.79, 4.54, 5.21, 6.02, 6.35, 4.78, 4.64, 4.62, 4.50]",,80.png
81.0,DDPM,v5.1.1,diffusion,유안,Denoising Diffusion Probabilistic Models,2020.0,"Jonathan Ho, Ajay Jain, Pieter Abbeel",UC Berkeley,"Ho, J., Jain, A., & Abbeel, P. (2020). Denoising diffusion probabilistic models. Advances in neural information processing systems, 33, 6840-6851.","Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., & Aberman, K. (2023). Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 22500-22510).","Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E. L., ... & Norouzi, M. (2022). Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35, 36479-36494.","Denoising Diffusion Probabilistic Models (DDPMs) are generative models that create high-quality images by learning to reverse a noisy diffusion process. This approach combines score matching with Langevin dynamics, training a neural network to iteratively denoise data, yielding realistic samples.",High-quality samples: Achieves competitive Inception and FID scores on CIFAR-10 and LSUN datasets.,Connection to score matching: Links diffusion models with denoising score matching and Langevin dynamics.,Progressive generation: Supports lossy image decompression with fine-to-coarse details during sampling.,Lower likelihood performance: Does not achieve competitive log likelihoods with other likelihood-based models.,Training instability: Sensitive to parameter choices and variance schedule settings.,"High computation cost: Requires extensive neural network evaluations, impacting sampling speed.",14543.0,FID,FID,"[‘EBM’, ‘JEM’, ‘BigGAN’, ‘StyleGAN2 + ADA (v1)’]","[37.9, 38.4, 14.73, 2.67]",,81.png
82.0,Improved DDPM,v5.1.2,diffusion,유안,Improved Denoising Diffusion Probabilistic Models,2021.0,"Alexander Quinn Nichol, Prafulla Dhariwal",,"Nichol, A. Q., & Dhariwal, P. (2021, July). Improved denoising diffusion probabilistic models. In International conference on machine learning (pp. 8162-8171). PMLR.","Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E. L., ... & Norouzi, M. (2022). Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35, 36479-36494.","Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., ... & Chen, M. (2021). Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741.","Improved Denoising Diffusion Probabilistic Models (DDPMs) are generative models that reverse a multi-step noising process. This improved DDPM variant learns the reverse process more efficiently, achieving high-quality samples with faster sampling and competitive log-likelihoods, making it suitable for scalable, practical applications.
","Achieves competitive log-likelihoods with high sample quality.
",Allows fast sampling with fewer forward passes.,"Improved mode coverage over GANs, enhancing distribution coverage.",Requires large compute resources for training.,Slower sampling compared to GANs.,High gradient noise during optimization affects training stability.,3090.0,FID,FID,"['BigGAN-deep', 'Improved Diffusion (small)', 'Improved Diffusion (large)']","[4.06, 6.92, 2.92]",,82.png
83.0,Stable Diffusion,v5.1.3.1,diffusion,유안,High-Resolution Image Synthesis With Latent Diffusion Models,2022.0,"Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer","Ludwig Maximilian University of Munich & IWR, Heidelberg University, Germany","Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022). High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 10684-10695).","Liu, H., Li, C., Wu, Q., & Lee, Y. J. (2024). Visual instruction tuning. Advances in neural information processing systems, 36.","Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., ... & Jitsev, J. (2022). Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35, 25278-25294.","Latent Diffusion Models (LDMs) generate high-resolution images by training diffusion models in a compressed latent space, significantly reducing computational resources without sacrificing quality. This allows for efficient and versatile applications, including super-resolution, inpainting, and text-to-image synthesis.",Reduced computational demands with latent-space training.,High-quality synthesis through minimal downsampling.,Supports diverse conditioning inputs like text and layout.,Requires a pretrained autoencoder.,Still computationally demanding for very high resolutions.,Limited generalization to unconventional conditioning setups.,12514.0,FID,FID,"['DALL-E', 'CogView', 'Lafite', 'LDM-KL-8', 'LDM-KL-8-G']","[27.50, 27.10, 26.94, 23.35, 12.61]",,83.png
84.0,Stable Diffusion XL,v5.1.4.1,diffusion,유안,SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis,2023.0,"Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, Robin Rombach","Stability AI, Applied Research","Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Müller, J., ... & Rombach, R. (2023). Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952.","Yang, Z., Li, L., Lin, K., Wang, J., Lin, C. C., Liu, Z., & Wang, L. (2023). The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint arXiv:2309.17421, 9(1), 1.","Yang, L., Zhang, Z., Song, Y., Hong, S., Xu, R., Zhao, Y., ... & Yang, M. H. (2023). Diffusion models: A comprehensive survey of methods and applications. ACM Computing Surveys, 56(4), 1-39.","SDXL is an improved text-to-image latent diffusion model using a larger UNet backbone, two text encoders, and novel conditioning techniques to generate high-quality, high-resolution images with better detail and composition.",Larger UNet model with 2.6B parameters enhances image synthesis quality.,Dual text encoders improve prompt understanding and adherence.,Refinement model enhances image detail and fidelity post-generation.,High VRAM and compute requirements for two-stage generation.,"Struggles with rendering fine details, like hands and intricate text.",May exhibit concept blending in complex scenes.,1132.0,FID-5k,FID-5k," ['CIN-512-only', 'CIN-nocond', 'CIN-size-cond']","[43.84, 39.76, 36.53]",,84.png
85.0,SDXL Turbo,v5.1.4.2,diffusion,유안,Adversarial Diffusion Distillation,2023.0,"Axel Sauer, Dominik Lorenz, Andreas Blattmann, Robin Rombach
",Stability AI,"Sauer, A., Lorenz, D., Blattmann, A., & Rombach, R. (2025). Adversarial diffusion distillation. In European Conference on Computer Vision (pp. 87-103). Springer, Cham.","Liu, Y., Zhang, K., Li, Y., Yan, Z., Gao, C., Chen, R., ... & Sun, L. (2024). Sora: A review on background, technology, limitations, and opportunities of large vision models. arXiv preprint arXiv:2402.17177.","Wang, J., Yue, Z., Zhou, S., Chan, K. C., & Loy, C. C. (2024). Exploiting diffusion prior for real-world image super-resolution. International Journal of Computer Vision, 1-21.","Adversarial Diffusion Distillation (ADD) reduces high-fidelity image diffusion model sampling steps to just 1–4 steps by combining adversarial and distillation losses, enabling real-time, high-quality generation.","Achieves single-step, high-quality image generation.",Outperforms comparable few-step models in image fidelity.,Retains quality across iterative sampling steps.,Limited diversity in single-step samples.,Pretrained model initialization required.,"Reduced effectiveness with certain teacher models (e.g., SDXL).",196.0,FID,FID,"['DPM Solver', 'Progressive Distillation', 'CFG-Aware Distillation', 'InstaFlow-0.9B', 'InstaFlow-1.7B', 'UFOGen', 'ADD-M']","[20.1, 37.2, 24.2, 23.4, 22.4, 22.5, 19.7]",,85.png
86.0,IP-Adapter,v5.1.4.3,diffusion,유안,IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models,2023.0,"Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, Wei Yang",Tencent AI Lab,"Ye, H., Zhang, J., Liu, S., Han, X., & Yang, W. (2023). Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721.","Chen, X., Huang, L., Liu, Y., Shen, Y., Zhao, D., & Zhao, H. (2024). Anydoor: Zero-shot object-level image customization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 6593-6602).","Han, Z., Gao, C., Liu, J., Zhang, J., & Zhang, S. Q. (2024). Parameter-efficient fine-tuning for large models: A comprehensive survey. arXiv preprint arXiv:2403.14608.","The IP-Adapter enables image prompts in text-to-image diffusion models by adding separate cross-attention layers for text and image features. It maintains high generation quality with minimal parameters, allowing multimodal image generation using both text and image prompts without modifying the original model.","Lightweight design with only 22M parameters, comparable to fully fine-tuned models.","Compatible with text prompts, enabling multimodal generation.",Works with existing structural control tools like ControlNet.,Limited ability to capture highly specific subject features.,Only approximate faithfulness to reference images.,Multimodal generation lacks full customization consistency.,337.0,CLIP-T,CLIP-T,"['Open unCLIP', 'Kandinsky-2-1', 'Versatile Diffusion', 'SD Image Variations', 'SD unCLIP', 'Uni-ControlNet (Global Control)', 'T2I-Adapter (Style)', 'ControlNet Shuffle', 'IP-Adapter']","[0.608, 0.599, 0.587, 0.548, 0.584, 0.506, 0.485, 0.421, 0.588]",,86.png
87.0,Stable Video Diffusion,v5.1.5.1,diffusion,유안,Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets,2023.0,"Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, Robin Rombach",Stability AI,"Blattmann, A., Dockhorn, T., Kulal, S., Mendelevitch, D., Kilian, M., Lorenz, D., ... & Rombach, R. (2023). Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127.","Li, C., Gan, Z., Yang, Z., Yang, J., Li, L., Wang, L., & Gao, J. (2024). Multimodal foundation models: From specialists to general-purpose assistants. Foundations and Trends® in Computer Graphics and Vision, 16(1-2), 1-214.","Liu, Y., Zhang, K., Li, Y., Yan, Z., Gao, C., Chen, R., ... & Sun, L. (2024). Sora: A review on background, technology, limitations, and opportunities of large vision models. arXiv preprint arXiv:2402.17177.","Stable Video Diffusion is a high-resolution video generation model that finetunes pre-trained image diffusion models with a three-stage video-specific training strategy, including data curation.",Systematic data curation for high-quality video training.,Strong baseline model for diverse video generation tasks.,Effective multi-view and motion consistency in video synthesis.,Limited long-form video generation capabilities.,High computational and memory demands.,Motion sometimes lacks adequate dynamism.,432.0,CLIP-S,CLIP-S,"['SyncDreamer', 'Zero123', 'Zero123XL', 'Scratch-MV', 'SD2.1-MV', 'SVD-MV']","[0.88, 0.87, 0.87, 0.76, 0.83, 0.89]",,87.png
88.0,Sora,v5.1.5.2,diffusion,유안,Video generation models as world simulators,2024.0,"Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, Aditya Ramesh",Open AI,"Brooks, T., Peebles, B., Holmes, C., DePue, W., Guo, Y., Jing, L., ... & Ramesh, A. (2024). Video generation models as world simulators. 2024-03-03]. https://openai. com/research/video-generation-modelsas-world-simulators.","Han, Z., Gao, C., Liu, J., Zhang, J., & Zhang, S. Q. (2024). Parameter-efficient fine-tuning for large models: A comprehensive survey. arXiv preprint arXiv:2403.14608.","Firoozi, R., Tucker, J., Tian, S., Majumdar, A., Sun, J., Liu, W., ... & Schwager, M. (2023). Foundation models in robotics: Applications, challenges, and the future. The International Journal of Robotics Research, 02783649241281508.","Sora is a video-generation model using diffusion transformers trained on variable-duration, resolution, and aspect ratio videos and images. It generates high-quality, text-conditional videos by segmenting data into spacetime patches, allowing scalable and flexible video generation across diverse visual formats.",Generates videos up to one minute in HD quality.,Adapts to diverse resolutions and aspect ratios.,Maintains long-range consistency in object and environment rendering.,Struggles with complex physical interactions like glass shattering.,Object coherence issues in long videos.,Occasional unintended object appearances.,134.0,,,,,,88.png
89.0,Lumiere,v5.1.5.3,diffusion,유안,Lumiere: A Space-Time Diffusion Model for Video Generation,2024.0,"Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Guanghui Liu, Amit Raj, Yuanzhen Li, Michael Rubinstein, Tomer Michaeli, Oliver Wang, Deqing Sun, Tali Dekel, Inbar Mosseri",Google Research,"Bar-Tal, O., Chefer, H., Tov, O., Herrmann, C., Paiss, R., Zada, S., ... & Mosseri, I. (2024). Lumiere: A space-time diffusion model for video generation. arXiv preprint arXiv:2401.12945.","Liu, Y., Zhang, K., Li, Y., Yan, Z., Gao, C., Chen, R., ... & Sun, L. (2024). Sora: A review on background, technology, limitations, and opportunities of large vision models. arXiv preprint arXiv:2402.17177.","Zhang, D. J., Wu, J. Z., Liu, J. W., Zhao, R., Ran, L., Gu, Y., ... & Shou, M. Z. (2024). Show-1: Marrying pixel and latent diffusion models for text-to-video generation. International Journal of Computer Vision, 1-15.","Lumiere is a text-to-video diffusion model that uses a Space-Time U-Net architecture to generate coherent, realistic motion in videos. Unlike traditional models, it processes an entire video sequence at once, ensuring temporal consistency without relying on cascaded temporal super-resolution.
",Generates coherent motion by processing entire video duration at once.,"Facilitates varied applications: image-to-video, inpainting, stylized generation.",Uses efficient space-time downsampling to reduce computational demands.,"Limited to single-shot, continuous scene videos.",Challenges with high-resolution generation due to memory constraints.,Not optimized for scene transitions or multi-shot videos.,118.0,FVD,FVD,"['MagicVideo', 'Emu Video', 'Video LDM', 'Show-1', 'Make-A-Video', 'PYoCo', 'SVD', 'Lumiere']","[655.00, 606.20, 550.61, 394.46, 367.23, 355.19, 242.02, 332.49]",,89.png
90.0,Gen-2,v5.1.5.4,diffusion,유안,Structure and Content-Guided Video Synthesis with Diffusion Models,2023.0,"Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, Anastasis Germanidis",Runway,"Esser, P., Chiu, J., Atighehchian, P., Granskog, J., & Germanidis, A. (2023). Structure and content-guided video synthesis with diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 7346-7356).","Blattmann, A., Rombach, R., Ling, H., Dockhorn, T., Kim, S. W., Fidler, S., & Kreis, K. (2023). Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 22563-22575).","Liu, R., Wu, R., Van Hoorick, B., Tokmakov, P., Zakharov, S., & Vondrick, C. (2023). Zero-1-to-3: Zero-shot one image to 3d object. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 9298-9309).","The paper presents a diffusion-based video synthesis model that edits video content guided by text or images while preserving the video’s structure. It introduces depth-based structure control and temporal consistency, allowing customized edits without per-video training.",Combines structure control with depth maps and content guidance through text or images.,Enables fine-grained video edits with high temporal consistency.,"Supports diverse applications like style transfer, character replacement, and environment changes.",Requires substantial computational resources for training.,Limited by dataset quality and availability of diverse video-text pairs.,May struggle with intricate structural changes in dynamic scenes.,400.0,,,,,,90.png
91.0,Point-E,v5.1.6.1,diffusion,유안,Point-E: A System for Generating 3D Point Clouds from Complex Prompts,2022.0,"Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, Mark Chen",Open AI,"Nichol, A., Jun, H., Dhariwal, P., Mishkin, P., & Chen, M. (2022). Point-e: A system for generating 3d point clouds from complex prompts. arXiv preprint arXiv:2212.08751.","Liu, R., Wu, R., Van Hoorick, B., Tokmakov, P., Zakharov, S., & Vondrick, C. (2023). Zero-1-to-3: Zero-shot one image to 3d object. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 9298-9309).","Jun, H., & Nichol, A. (2023). Shap-e: Generating conditional 3d implicit functions. arXiv preprint arXiv:2305.02463.","Point·E is a fast, efficient system for generating 3D point clouds from text prompts by combining text-to-image and image-to-3D diffusion models.",Generates 3D point clouds in under 2 minutes.,Uses efficient two-step text-to-3D generation.,Requires only a single GPU.,Lower fidelity than state-of-the-art methods.,Requires synthetic image generation.,Limited to low-resolution point clouds.,437.0,ViT-B/32,ViT-B/32 (%),"['DreamFields', 'CLIP-Mesh', 'DreamFusion', 'Point-E (40M, text-only)', 'Point-E (40M)', 'Point-E (300M)', 'Point-E (1B)']","[78.6, 67.8, 75.1, 15.4, 36.5, 40.3, 41.1]",%,91.png
92.0,GET3D,v5.1.6.2,diffusion,유안,GET3D: A Generative Model of High Quality 3D Textured Shapes Learned from Images,2023.0,"Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen, Kangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, Sanja Fidler",NVIDIA,"Gao, J., Shen, T., Wang, Z., Chen, W., Yin, K., Li, D., ... & Fidler, S. (2022). Get3d: A generative model of high quality 3d textured shapes learned from images. Advances In Neural Information Processing Systems, 35, 31841-31854.","Lin, C. H., Gao, J., Tang, L., Takikawa, T., Zeng, X., Huang, X., ... & Lin, T. Y. (2023). Magic3d: High-resolution text-to-3d content creation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 300-309).","Liu, R., Wu, R., Van Hoorick, B., Tokmakov, P., Zakharov, S., & Vondrick, C. (2023). Zero-1-to-3: Zero-shot one image to 3d object. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 9298-9309).","GET3D is a generative model that synthesizes high-quality, textured 3D meshes from 2D image collections, meeting standards for diverse applications. It uses differentiable surface and texture modeling to produce geometrically detailed, textured, and arbitrarily topological meshes usable in common 3D software.",Produces textured 3D meshes directly usable in 3D software.,"Trains on 2D images, creating detailed geometry and textures.","Supports complex, varied mesh topologies and lighting effects.",Requires 2D silhouettes and known camera angles for training.,"Currently trained per-category, limiting multi-category application.","Evaluated on synthetic data only, not real-world data.",407.0,FID,FID,"[Car 128², Car 512², Car 1024², Chair 128², Chair 512², Chair 1024², Mbike 512², Mbike 1024², Animal 512², Animal 1024²]","[39.21, 13.19, 10.25, 43.04, 30.16, 23.28, 74.04, 65.60, 29.75, 28.33]",,92.png
93.0,Show and Tell,v6.0.1,multimodal,유안,Show and Tell: A Neural Image Caption Generator,2015.0,"Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan",Google,"Vinyals, O., Toshev, A., Bengio, S., & Erhan, D. (2015). Show and tell: A neural image caption generator. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 3156-3164).","Khan, S., Naseer, M., Hayat, M., Zamir, S. W., Khan, F. S., & Shah, M. (2022). Transformers in vision: A survey. ACM computing surveys (CSUR), 54(10s), 1-41.","Alayrac, J. B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., ... & Simonyan, K. (2022). Flamingo: a visual language model for few-shot learning. Advances in neural information processing systems, 35, 23716-23736.","The ""Show and Tell"" model generates natural language descriptions for images by combining a Convolutional Neural Network (CNN) to process images with a Recurrent Neural Network (RNN) to produce sentences, trained end-to-end to maximize the probability of accurate image captions.",Integrates image processing and language generation seamlessly.,Achieves state-of-the-art performance on multiple benchmarks.,"Trained end-to-end, eliminating the need for manual feature engineering.",Relies on large labeled datasets.,Struggles with unseen object compositions.,Limited diversity in generated captions.,7845.0,BLEU-4,BLEU-4,"['NIC', 'Random', 'Nearest Neighbor', 'Human']","[27.7, 4.6, 9.9, 21.7]",,93.png
