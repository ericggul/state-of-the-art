idx,name,version,category,assignedTo,paperName,year,authors,institution,citations_APA,relatedPapers_1,relatedPapers_1.1,explanation,highlights-1,highlights-2,highlights-3,limitations-1,limitations-2,limitations-3,citationsNumber,stats_135,stats_217,metric,yAxisLabel,labels,data,format,model_image,graph_image
9,Beta-VAE,v2.1.2.1,vae,세준,B-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework,2017,"Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, Alexander Lerchner",Google DeepMind,"Higgins, I., Matthey, L., Pal, A., Burgess, C. P., Glorot, X., Botvinick, M. M., ... & Lerchner, A. (2017). beta-vae: Learning basic visual concepts with a constrained variational framework. ICLR (Poster), 3.","Higgins, I., Matthey, L., Glorot, X., Pal, A., Uria, B., Blundell, C., ... & Lerchner, A. (2016). Early visual concept learning with unsupervised deep learning. arXiv preprint arXiv:1606.05579.","Karaletsos, T., Belongie, S., & Rätsch, G. (2015). Bayesian representation learning with oracle constraints. arXiv preprint arXiv:1506.05011.","The β-VAE model extends the traditional variational autoencoder (VAE) by introducing a hyperparameter β, which encourages learning disentangled representations of independent factors in visual data.",Disentangles data without supervision.,Improved interpretability of learned factors.,Achieves stable training across datasets.,May blur reconstructions at high β.,Optimal β varies by dataset.,Struggles with low data continuity.,5412.0,Adagrad_learningrate_1e-2,Adam_learningrate_1e-4,Disentanglement Metric Score,Comparison (%),"['VAE untrained', 'VAE', 'B-VAE']","[44,14, 61.58, 99.23]",%,m9.png,g9.png
10,SimCLR,v2.2.1,self_supervised,세준,A Simple Framework for Contrastive Learning of Visual Representations,2020,"Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton",Google Research Brain Team,"Chen, T., Kornblith, S., Norouzi, M., & Hinton, G. (2020, November). A simple framework for contrastive learning of visual representations. In International conference on machine learning (pp. 1597-1607). PMLR.","Khosla, P., Teterwak, P., Wang, C., Sarna, A., Tian, Y., Isola, P., ... & Krishnan, D. (2020). Supervised contrastive learning. Advances in neural information processing systems, 33, 18661-18673.","Wu, J., Hobbs, J., & Hovakimyan, N. (2023). Hallucination improves the performance of unsupervised visual representation learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 16132-16143).","SimCLR is a simple contrastive learning framework that improves unsupervised visual representation learning by maximizing agreement between augmented views of images, requiring no specialized architectures or memory bank.",High performance without labels.,Improves with larger batch sizes.,Stronger augmentation benefits learning.,Needs large computational resources.,High memory demand for large batches.,Sensitive to augmentation strategies​. ,20280.0,parameters_24000000,trainingtime_hours_1.5,Top-1,Comparison (%),"['SimCLR using ResNet-50', 'SimCLR using other architectures']","[69.3, 76.5]",%,m10.png,g10.png
11,MoCo,v2.2.2,self_supervised,세준,Momentum Contrast for Unsupervised Visual Representation Learning,2020,"Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick",Facebook AI Research,"He, K., Fan, H., Wu, Y., Xie, S., & Girshick, R. (2020). Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 9729-9738).","Wei, C., Wang, H., Shen, W., & Yuille, A. (2020). Co2: Consistent contrast for unsupervised visual representation learning. arXiv preprint arXiv:2010.02217.","Pang, B., Zhang, Y., Li, Y., Cai, J., & Lu, C. (2022, October). Unsupervised visual representation learning by synchronous momentum grouping. In European Conference on Computer Vision (pp. 265-282). Cham: Springer Nature Switzerland.","MoCo is an unsupervised learning framework using contrastive learning to train visual representations. It builds a dynamic dictionary with a momentum-updated encoder and queue, enabling consistent, large-scale dictionaries for better representations, particularly in downstream tasks like detection and segmentation.","Builds a large, consistent dynamic dictionary with a queue.",Uses momentum-based updates for the key encoder.,Transfers effectively to various vision tasks.,High memory requirements due to large dictionary size.,Limited by the complexity of instance discrimination tasks.,Performance gain decreases with scale limitations.,13540.0,parameters_24000000,trainingtime_hours_53,Accuracy,Comparison (%),"['Exemplar', 'RelativePosition', 'Jigsaw', 'Rotation', 'Colorization', 'DeepCluster', 'BigBiGAN', 'InstDisc', 'LocalAgg', 'CPCv1', 'CPCv2', 'CMC', 'AMDIM', 'MoCo']","[46.0, 51.4, 44.6, 55.4, 39.6, 48.4, 56.6, 54.0, 58.8, 48.7, 65.9, 64.1, 63.5, 60.6]",%,m11.png,g11.png
12,DINO,v2.3.1,self_supervised,세준,Emerging Properties in Self-Supervised Vision Transformers,2021,"Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, Armand Joulin",Facebook AI Research,"Caron, M., Touvron, H., Misra, I., Jégou, H., Mairal, J., Bojanowski, P., & Joulin, A. (2021). Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 9650-9660).","Chen, X., Xie, S., & He, K. (2021). An empirical study of training self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 9640-9649).","d’Ascoli, S., Touvron, H., Leavitt, M. L., Morcos, A. S., Biroli, G., & Sagun, L. (2021, July). Convit: Improving vision transformers with soft convolutional inductive biases. In International conference on machine learning (pp. 2286-2296). PMLR.","DINO (self-distillation with no labels) trains a Vision Transformer in a self-supervised manner. Using a student-teacher framework, the student network learns to predict the teacher's output through self-attention, enabling effective learning without labeled data.",Learns visual features without labeled data.,Enables object segmentation from attention maps.,Achieves strong performance in classification benchmarks.,Computationally intensive training.,"Requires specific configurations (momentum encoder, multi-crop).",Limited performance gains in smaller-scale applications.,5429.0,parameters_24000000,trainingtime_days_3,k-NN evaluation,Accuracy (%),"[‘Supervised RN50’, ‘SCLR’, ‘MoCov2’, ‘InfoMin’, ‘BarlowT’, ‘OBoW’, ‘BYOL’, ‘DCv2’, ‘SwAV’, ‘DINO RN50’]","[79.3, 69.1, 71.1, 73.0, 73.2, 73.8, 74.4, 75.2, 75.3, 75.3]",%,m12.png,g12.png
13,DINOv2,v2.3.2,self_supervised,세준,DINOv2: Learning Robust Visual Features without Supervision,2023,"Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Hervé Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, Piotr Bojanowski",Meta AI Research,"Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V., ... & Bojanowski, P. (2023). Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193.","Newell, A., & Deng, J. (2020). How useful is self-supervised pretraining for visual tasks?. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 7345-7354).","Fang, Y., Dong, L., Bao, H., Wang, X., & Wei, F. (2022). Corrupted image modeling for self-supervised visual pre-training. arXiv preprint arXiv:2202.03382.","DINOv2 is a large-scale self-supervised Vision Transformer model for learning robust, general-purpose visual features without supervision. It leverages a curated dataset and a suite of optimized training techniques to produce visual representations that work across various computer vision tasks without fine-tuning.",Uses curated dataset for enhanced feature quality.,Incorporates scalable training optimizations for efficiency.,Matches or surpasses weakly-supervised model benchmarks.,Requires significant computational resources.,"Bias remains toward high-income, Western data sources.",Environmental impact due to large-scale model training.,1648.0,parameters_1100000000,trainingdata_142000000,Top-1,Linear Evaluation (%),"[‘MAE ViT-H/14’, ‘DINO ViT-B/8’, ‘iBOT ViT-L/16’, ‘DINOv2 ViT-S/14’, ‘DINOv2 ViT-B/14’, ‘DINOv2 ViT-L/14’, ‘DINOv2 ViT-g/14’]","[76.6, 79.2, 82.3, 81.1, 84.5, 86.3, 86.5]",%,m13.png,g13.png
18,VGGNet,v3.2.2,cnn,유안,Very Deep Convolutional Networks For Large-Scale Image Recognition,2014,"Karen Simonyan, Andrew Zisserman",University of Oxford,"Simonyan, K., & Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556.","Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022). High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 10684-10695).","Wang, C. Y., Bochkovskiy, A., & Liao, H. Y. M. (2023). YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 7464-7475).","The paper presents deep convolutional networks (up to 19 layers) using small 3x3 filters for large-scale image recognition, achieving state-of-the-art performance on the ImageNet dataset by focusing on depth rather than complex network structures.",Significant accuracy boost by deepening network.,Uses only 3x3 filters for simplification.,Outperforms previous models on ImageNet.,High computational cost and memory usage.,Longer training time with increased depth.,Dependent on large-scale labeled datasets.,132477.0,parameters_682000000,weightlayers_86,Top-1,Comparison (%),"['D_dense', 'D_multi-crop', 'D_multi-crop & dense']","[24.8, 24.6, 24.4]",%,m18.png,g18.png
19,GoogLeNet,v3.2.3,cnn,유안,Going Deeper with Convolutions,2014,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich",Google Inc.,"Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., ... & Rabinovich, A. (2015). Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1-9).","Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., ... & Guo, B. (2021). Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 10012-10022).","Wang, C. Y., Bochkovskiy, A., & Liao, H. Y. M. (2023). YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 7464-7475).",The GoogLeNet model (Inception architecture) improves deep network efficiency by using modular layers with mixed kernel sizes for multi-scale feature extraction while keeping computational cost low.,Multi-scale feature extraction via Inception modules.,Optimized for efficient computation with reduced parameters.,Superior performance in classification and detection tasks.,High memory demand during training.,Limited applicability to non-vision tasks.,Complexity in designing and tuning Inception modules.,62896.0,,,,,,,,,
20,ResNet,v3.2.4,cnn,유안,Deep Residual Learning for Image Recognition,2015,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",Microsoft Research,"He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).","Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., ... & Girshick, R. (2023). Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 4015-4026).","He, K., Chen, X., Xie, S., Li, Y., Dollár, P., & Girshick, R. (2022). Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 16000-16009).","The paper introduces a deep residual learning framework to address difficulties in training very deep neural networks. Instead of learning direct mappings, residual networks (ResNets) learn residual functions with shortcut connections, which improves optimization and enables deeper models. ResNets show significant gains in accuracy and outperform shallower networks on multiple benchmarks.",Enables efficient training of extremely deep networks (up to 152 layers).,Uses residual connections to address degradation in deeper models.,Achieves state-of-the-art performance on ImageNet and COCO datasets.,Residual connections don't fully eliminate overfitting in very large datasets.,Requires substantial computational resources for deep models.,"May face diminishing returns with extreme depth (e.g., over 1000 layers).",242492.0,,,,,,,,,
21,DenseNet,v3.2.5,cnn,유안,Densely Connected Convolutional Networks,2017,"Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger",Cornell University,"Huang, G., Liu, Z., Van Der Maaten, L., & Weinberger, K. Q. (2017). Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 4700-4708).","Liu, Z., Mao, H., Wu, C. Y., Feichtenhofer, C., Darrell, T., & Xie, S. (2022). A convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 11976-11986).","Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., ... & Guo, B. (2021). Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 10012-10022).","DenseNet is a deep neural network architecture where each layer is connected to every other layer to improve feature reuse, gradient flow, and efficiency, enabling powerful performance with fewer parameters.",Improved feature reuse through dense connections.,"Mitigates vanishing gradient, making training easier.",Achieves high accuracy with fewer parameters.,Increased memory usage due to dense connections.,May require specific growth rate tuning.,Dense connectivity can slow down computations.,48567.0,,,,,,,,,
22,EfficientNet,v3.2.6,cnn,유안,EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,2019,"Mingxing Tan, Quoc Le",Google Research,"Tan, M., & Le, Q. (2019, May). Efficientnet: Rethinking model scaling for convolutional neural networks. In International conference on machine learning (pp. 6105-6114). PMLR.","Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., ... & Sutskever, I. (2021, July). Learning transferable visual models from natural language supervision. In International conference on machine learning (pp. 8748-8763). PMLR.","Bochkovskiy, A., Wang, C. Y., & Liao, H. Y. M. (2020). Yolov4: Optimal speed and accuracy of object detection. arXiv preprint arXiv:2004.10934.","EfficientNet is a family of Convolutional Neural Networks that scales width, depth, and resolution using a compound scaling method, achieving high accuracy with fewer parameters. This balanced scaling approach, coupled with a new baseline network, allows EfficientNet to outperform previous models in accuracy and efficiency across various tasks and datasets.","Balances width, depth, and resolution with compound scaling.",Achieves high accuracy with fewer parameters than prior models.,State-of-the-art performance on ImageNet and transfer learning datasets.,Limited to single-scaling baseline network.,Heavy reliance on neural architecture search.,High computational resources needed for model training.,23775.0,,,,,,,,,
23,Vision Transformer (ViT),v3.3.1,transformer,민혁,An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,2020,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby",,"Dosovitskiy, A. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.","Bao, H., Dong, L., Piao, S., & Wei, F. (2021). Beit: Bert pre-training of image transformers. arXiv preprint arXiv:2106.08254.","Chen, C. F. R., Fan, Q., & Panda, R. (2021). Crossvit: Cross-attention multi-scale vision transformer for image classification. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 357-366).","The Vision Transformer (ViT) divides images into patches, treats them as tokens like words in NLP, and uses self-attention to capture relationships across the image. Trained on large datasets, it outperforms convolutional networks in classification accuracy with fewer computational resources",Applies Transformers directly to image patches as input tokens.,"Achieves strong accuracy, especially when trained on large datasets.",Reduces dependence on convolutional neural networks (CNNs).,Requires large datasets for optimal performance.,"Lacks CNNs’ inherent inductive biases, like spatial locality.","Not as effective on small datasets without pre-training.




",47287.0,MLPsize_5120,parameters_632000000,Mean,Accuracy (%),"['ViT-H/14', 'ViT-L/16', 'ViT-L/16', 'ResNet52x4', 'EfficientNet-L2']","[88.55, 87.76, 85.30, 87.54, 88.4]",%,m23.png,g23.png
24,Swin Transformer,v3.3.2,transformer,민혁,Swin Transformer: Hierarchical Vision Transformer using Shifted Windows,2021,"Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo","Microsoft Research Asia, University of Science and Technology of China, Xian Jiaotong University, Tsinghua University","Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., ... & Guo, B. (2021). Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 10012-10022).","Guo, J., Han, K., Wu, H., Tang, Y., Chen, X., Wang, Y., & Xu, C. (2022). Cmt: Convolutional neural networks meet vision transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 12175-12185).","Chen, Z., Duan, Y., Wang, W., He, J., Lu, T., Dai, J., & Qiao, Y. (2022). Vision transformer adapter for dense predictions. arXiv preprint arXiv:2205.08534.","The Swin Transformer builds hierarchical feature maps with local self-attention in shifted windows, which connects neighboring windows efficiently. This enables strong performance on image classification, object detection, and segmentation tasks by modeling at various scales with linear computational complexity relative to image size.",Uses shifted windows for local self-attention.,Achieves state-of-the-art results on multiple vision tasks.,Scales efficiently with linear computational complexity.,Requires significant computational resources for training.,Limited adaptability for very small datasets.,Challenging optimization for certain dense prediction tasks.,23368.0,FLOPs_103900000000,parameters_197000000,Average,Precision (%),"['DeiT-S', 'R50', 'Swin-T']","[48.0, 46.3, 50.5]",%,,
25,DeiT,v3.3.3,transformer,민혁,Training data-efficient image transformers & distillation through attention,2021,"Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Hervé Jegou",,"Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., & Jégou, H. (2021, July). Training data-efficient image transformers & distillation through attention. In International conference on machine learning (pp. 10347-10357). PMLR.","Yuan, L., Chen, Y., Wang, T., Yu, W., Shi, Y., Jiang, Z. H., ... & Yan, S. (2021). Tokens-to-token vit: Training vision transformers from scratch on imagenet. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 558-567).","Chen, C. F. R., Fan, Q., & Panda, R. (2021). Crossvit: Cross-attention multi-scale vision transformer for image classification. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 357-366).","DeiT (Data-efficient Image Transformers) leverages a teacher-student distillation approach, with a unique ""distillation token"" to improve data efficiency and achieve strong results on ImageNet without large pre-training datasets. It enables Vision Transformers (ViT) to be trained more effectively on smaller datasets, overcoming prior data limitations​.",Introduces a distillation token for efficient learning.,Matches CNNs’ performance on ImageNet using smaller data.,Achieves high accuracy with minimal pre-training requirements.,Limited to classification tasks initially.,Requires a strong teacher model for distillation.,Performance depends on extensive data augmentation.,6997.0,layers_12,parameters_86000000,Top-1,Accuracy (%),,,,,
26,EVA,v3.3.4,transformer,민혁,EVA: Exploring the Limits of Masked Visual Representation Learning at Scale,2023,"Zhang Ge, Yuxin Fang, Yuxin Wang, Kunchang Li, Ziyi Lin, Wenguang Zhou, Fengwei Yu, Xiangyu Zhang, Tong Lu, Jianbin Jiao, Yu Qiao, Xiaogang Wang, Jianmin Bao, Han Hu","Huazhong University of Science and Technology, Beijing Academy of Artificial Intelligence, Zhejiang University, Beijing Institute of Technology","Fang, Y., Wang, W., Xie, B., Sun, Q., Wu, L., Wang, X., ... & Cao, Y. (2023). Eva: Exploring the limits of masked visual representation learning at scale. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 19358-19369).","Gao, P., Geng, S., Zhang, R., Ma, T., Fang, R., Zhang, Y., ... & Qiao, Y. (2024). Clip-adapter: Better vision-language models with feature adapters. International Journal of Computer Vision, 132(2), 581-595.","Wang, Z., Yu, J., Yu, A. W., Dai, Z., Tsvetkov, Y., & Cao, Y. (2021). Simvlm: Simple visual language model pretraining with weak supervision. arXiv preprint arXiv:2108.10904.","EVA is a large-scale vision model designed to maximize the power of masked image modeling (MIM) by using image-text aligned features. It efficiently scales up to one billion parameters, achieving state-of-the-art results across various tasks, including object detection, segmentation, and classification, using only publicly accessible data​.",Scales MIM with image-text aligned features.,Achieves top results across multiple vision tasks.,Uses efficient training with only public data.,"Relies heavily on large, unlabeled datasets.",High computational requirements for training.,Less effective on smaller or labeled datasets.,579.0,MLPdimension_6144,parameters_10000000000,Top-1,Accuracy (%),"['CoAtNet-4', 'MaxViT-XL', 'MViTv2-H', 'FD-CLIP-L', 'BEiT-3', 'EVA']","[88.6, 88.7, 88.8, 89.0, 89.6, 89.6]",%,,
27,U-Net,v3.4.1,cnn,민혁,U-Net: Convolutional Networks for Biomedical Image Segmentation,2015,"Olaf Ronneberger, Philipp Fischer, and Thomas Brox","University of Freiburg, Germany","Ronneberger, O., Fischer, P., & Brox, T. (2015). U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted intervention–MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18 (pp. 234-241). Springer International Publishing.","Zunair, H., & Hamza, A. B. (2021). Sharp U-Net: Depthwise convolutional network for biomedical image segmentation. Computers in biology and medicine, 136, 104699.","Kaul, C., Manandhar, S., & Pears, N. (2019, April). Focusnet: An attention-based fully convolutional network for medical image segmentation. In 2019 IEEE 16th international symposium on biomedical imaging (ISBI 2019) (pp. 455-458). IEEE.",U-Net is a convolutional network designed for biomedical image segmentation. It has a U-shaped architecture with a contracting path for context capture and an expansive path for precise localization. This structure enables accurate segmentation with limited training data by using data augmentation and efficient feature propagation.,Efficient segmentation with minimal training data.,U-shaped design for context and localization.,Uses extensive data augmentation techniques.,Primarily tested on biomedical images.,Performance sensitive to data augmentation.,Limited generalization beyond specific domains.,95843.0,layerdepth_23,parameters_78000000,Average,IOU,"['IMCB-SG', 'KTH-SE', 'HOUS-US', 'second-best', 'u-net']","[0.2669, 0.7953, 0.5323, 0.83, 0.9203]",,,
