idx,name,version,category,assignedTo,paperName,year,authors,institution,citations_APA,relatedPapers_1,relatedPapers_2,explanation,highlights-1,highlights-2,highlights-3,limitations-1,limitations-2,limitations-3,citationsNumber,metric,yAxisLabel,labels,data,format,model_image,
1,McCulloch-Pitts Neuron,v1.0,basic_nn,정윤,A logical calculus of the ideas immanent in nervous activity,1943,"WS McCulloch, W Pitts ",,"McCulloch, W. S., & Pitts, W. (1943). A logical calculus of the ideas immanent in nervous activity. The bulletin of mathematical biophysics, 5, 115-133.","Hansen, B. B., Spittle, S., Chen, B., Poe, D., Zhang, Y., Klein, J. M., ... & Sangoro, J. R. (2020). Deep eutectic solvents: A review of fundamentals and applications. Chemical reviews, 121(3), 1232-1285.","Zuo, C., Qian, J., Feng, S., Yin, W., Li, Y., Fan, P., ... & Chen, Q. (2022). Deep learning in optical metrology: a review. Light: Science & Applications, 11(1), 1-54.","Propose a logical framework to model neural activity using propositional logic, highlighting neural networks' computational properties",Logical Framework: Neural activity modeled using propositional logic.,Computational Equivalence: Neural networks simulate logical expressions.,Neural Dynamics: Circuits affect temporal behavior.,"Simplistic Assumptions: Oversimplifies neuron behavior, ignoring biological complexities.",Static Structure: Assumes unchanging neural networks.,Time Indeterminacy: Struggles with precise timing in feedback loops.,31925,,,,,,1.png,
2,Perceptron,v1.1,basic_nn,세준,The perceptron: A probabilistic model for information storage and organization in the brain.,1958,F. Rosenblatt,Cornell Aeronautical Laboratory,"Rosenblatt, F. (1958). The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review, 65(6), 386.","Anderson, J. A., Silverstein, J. W., Ritz, S. A., & Jones, R. S. (1977). Distinctive features, categorical perception, and probability learning: Some applications of a neural model. Psychological review, 84(5), 413.","Block, H. D. (1962). The perceptron: A model for brain functioning. i. Reviews of Modern Physics, 34(1), 123.","The perceptron is a probabilistic model inspired by the brain's neural network, designed to learn associations, recognize patterns, and generalize based on the organization and statistical separability of input stimuli.",Mimics biological neuron firing for learning.,Introduced weight adjustment for learning.,Showcased statistical separability concept.,Fails with non-linearly separable data.,Highly dependent on feature representation.,Limited expressive power without layers.,20597,,,,,,2.png,
3,Multi-Layer Perceptron (MLP),v1.2,basic_nn,세준,Cybernetic Predicting Devices,1965,"Alexey Grigoryevich Ivakhnenko, Valentin Grigor'evich Lapa",U.S. Department of Commerce,"Ivakhnenko, A. G., & Lapa, V. G. (1966). Cybernetic predicting devices. (No Title).","Azar, A. T., & Vaidyanathan, S. (Eds.). (2015). Computational intelligence applications in modeling and control. Heidelberg: Springer International Publishing.","Gallo, C. (2015). Artificial neural networks tutorial. In Encyclopedia of Information Science and Technology, Third Edition (pp. 6369-6378). IGI Global.","The MLP is a feedforward neural network consisting of an input layer, one or more hidden layers, and an output layer. It utilizes non-linear activation functions and backpropagation for training, enabling it to model complex, non-linear relationships in classification, regression, and feature extraction tasks.","Handles complex, non-linear relationships.",Suitable for multi-class classification.,Uses hidden layers for feature extraction.,Requires large labeled datasets for training.,Computationally intensive for large networks.,Prone to overfitting without regularization.,404,,,,,,3.png,
4,Backpropagation Algorithm,v1.2.1,basic_nn,세준,Learning representations by back-propagating errors,1986,"David E. Rumelhart, Geoffrey E. Hinton, Ronald J. Williams",UCSD,"Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning representations by back-propagating errors. nature, 323(6088), 533-536.","Crick, F. (1989). The recent excitement about neural networks. Nature, 337(6203), 129-132.","Plaut, D. C. (1986). Experiments on Learning by Back Propagation.","Backpropagation is a supervised learning algorithm used to train neural networks. It calculates gradients by propagating errors backward through the network layers, updating weights to minimize error, enabling deep networks to learn complex tasks.",Enables multi-layer gradient-based optimization.,Efficiently updates network parameters.,Scales well to deep and large networks.,Can get stuck in local minima.,Sensitive to hyperparameter choices.,Computationally intensive for deep networks.,40430,,,,,,4.png,
5,Hopfield Network,v2.0.1,hopfield_boltzmann,세준,Neural networks and physical systems with emergent collective computational abilities,1982,J. J. Hopfield,California Institute of Technology,"Hopfield, J. J. (1982). Neural networks and physical systems with emergent collective computational abilities. Proceedings of the national academy of sciences, 79(8), 2554-2558.","Tank, D. W., & Hopfield, J. J. (1987). Collective computation in neuronlike circuits. Scientific American, 257(6), 104-115.","Anderson, J. A. (1983). Cognitive and psychological computation with neural models. IEEE transactions on systems, man, and cybernetics, (5), 799-815.",The Hopfield Network is a recurrent neural network designed for associative memory. It stores patterns as energy states and retrieves them by dynamically converging to stable states from noisy or incomplete inputs.,Implements energy-based memory retrieval.,Robust to noisy or partial input patterns.,Offers content-addressable memory capabilities.,Limited storage capacity for patterns.,Prone to local minima convergence.,Sensitive to similarity between stored patterns.,28105,,,,,,5.png,
6,Boltzmann Machine,v2.0.2,hopfield_boltzmann,세준,A Learning Algorithm for Boltzmann Machines,1985,"David H. Ackley, Geoffrey E. Hinton, Terrence J. Sejnowski",Carnegie-Mellon University,"Ackley, D. H., Hinton, G. E., & Sejnowski, T. J. (1985). A learning algorithm for Boltzmann machines. Cognitive science, 9(1), 147-169.","Hinton, G. E., Sejnowski, T. J., & Ackley, D. H. (1984). Boltzmann machines: Constraint satisfaction networks that learn. Pittsburgh, PA: Carnegie-Mellon University, Department of Computer Science.","Hinton, G. E., & Sejnowski, T. J. (1986). Learning and relearning in Boltzmann machines. Parallel distributed processing: Explorations in the microstructure of cognition, 1(282-317), 2.","The Boltzmann Machine is a stochastic neural network based on statistical mechanics, using probabilistic updates to learn constraints and encode complex data distributions through energy minimization.",Learns probabilistic representations.,Efficient for solving constraint satisfaction tasks.,Can represent high-order correlations in data.,Slow convergence for large datasets.,Requires long equilibrium periods.,Susceptible to local minima.,5916,,,,,,6.png,
7,Basic Autoencoder,v2.1.1,vae,세준,Reducing the dimensionality of data with neural networks,1986,"G. E. Hinton, R. R. Salakhutdinov",University of Toronto,"Hinton, G. E., & Salakhutdinov, R. R. (2006). Reducing the dimensionality of data with neural networks. science, 313(5786), 504-507.","Ansuini, A., Laio, A., Macke, J. H., & Zoccolan, D. (2019). Intrinsic dimension of data representations in deep neural networks. Advances in Neural Information Processing Systems, 32.","Recanatesi, S., Farrell, M., Advani, M., Moore, T., Lajoie, G., & Shea-Brown, E. (2019). Dimensionality compression and expansion in deep neural networks. arXiv preprint arXiv:1906.00443.",An autoencoder is a neural network for unsupervised learning that encodes high-dimensional data into a lower-dimensional latent space and decodes it back to reconstruct the original input. Pretraining and backpropagation optimize its weights for efficient data compression and reconstruction.,Performs non-linear dimensionality reduction.,Outperforms PCA for complex data distributions.,Learns robust representations for various tasks.,Struggles without proper initialization.,Sensitive to network depth and architecture.,Prone to reconstructing training data average.,23900,Reconstruction error,Mean squared reconstruction error rates,"[‘Autoencoder’, ‘PCA’, ‘Random Guess’]","[1.44, 7.64, 13.87]",%,7.png,
8,Variational Autoencoder (VAE),v2.1.2,vae,정윤,Auto-encoding variational bayes,2013,"Diederik P Kingma, Max Welling",Universiteit van Amsterdam,"Kingma, D. P. (2013). Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114.","Yang, L., Zhang, Z., Song, Y., Hong, S., Xu, R., Zhao, Y., ... & Yang, M. H. (2023). Diffusion models: A comprehensive survey of methods and applications. ACM Computing Surveys, 56(4), 1-39.","Zhang, C., Zhang, C., Zheng, S., Qiao, Y., Li, C., Zhang, M., ... & Hong, C. S. (2023). A complete survey on generative ai (aigc): Is chatgpt from gpt-4 to gpt-5 all you need?. arXiv preprint arXiv:2303.11717.","The paper introduces Variational Autoencoders (VAEs), which use the reparameterization trick for efficient optimization and generative modeling of complex data with continuous latent variables.",Reparameterization Trick: Enables efficient gradient optimization.,Generative Power: Combines deep learning with variational inference,Scalability: Works well with large datasets.,Approximate Posterior: Limited to simple distributions.,High Variance: Unstable gradient estimators.,Model Expressiveness: Dependent on neural network architecture.,39468,,,,,,8.png,
9,Beta-VAE,v2.1.2.1,vae,세준,B-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework,2017,"Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, Alexander Lerchner",Google DeepMind,"Higgins, I., Matthey, L., Pal, A., Burgess, C. P., Glorot, X., Botvinick, M. M., ... & Lerchner, A. (2017). beta-vae: Learning basic visual concepts with a constrained variational framework. ICLR (Poster), 3.","Higgins, I., Matthey, L., Glorot, X., Pal, A., Uria, B., Blundell, C., ... & Lerchner, A. (2016). Early visual concept learning with unsupervised deep learning. arXiv preprint arXiv:1606.05579.","Karaletsos, T., Belongie, S., & Rätsch, G. (2015). Bayesian representation learning with oracle constraints. arXiv preprint arXiv:1506.05011.","The β-VAE model extends the traditional variational autoencoder (VAE) by introducing a hyperparameter β, which encourages learning disentangled representations of independent factors in visual data.",Disentangles data without supervision.,Improved interpretability of learned factors.,Achieves stable training across datasets.,May blur reconstructions at high β.,Optimal β varies by dataset.,Struggles with low data continuity.,5412,Disentanglement Metric Score,Comparison (%),"['VAE untrained', 'VAE', 'B-VAE']","[44,14, 61.58, 99.23]",%,9.png,
10,SimCLR,v2.2.1,self_supervised,세준,A Simple Framework for Contrastive Learning of Visual Representations,2020,"Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton",Google Research Brain Team,"Chen, T., Kornblith, S., Norouzi, M., & Hinton, G. (2020, November). A simple framework for contrastive learning of visual representations. In International conference on machine learning (pp. 1597-1607). PMLR.","Khosla, P., Teterwak, P., Wang, C., Sarna, A., Tian, Y., Isola, P., ... & Krishnan, D. (2020). Supervised contrastive learning. Advances in neural information processing systems, 33, 18661-18673.","Wu, J., Hobbs, J., & Hovakimyan, N. (2023). Hallucination improves the performance of unsupervised visual representation learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 16132-16143).","SimCLR is a simple contrastive learning framework that improves unsupervised visual representation learning by maximizing agreement between augmented views of images, requiring no specialized architectures or memory bank.",High performance without labels.,Improves with larger batch sizes.,Stronger augmentation benefits learning.,Needs large computational resources.,High memory demand for large batches.,Sensitive to augmentation strategies​. ,20280,Top-1,Comparison (%),"['SimCLR using ResNet-50', 'SimCLR using other architectures']","[69.3, 76.5]",%,10.png,
11,MoCo,v2.2.2,self_supervised,세준,Momentum Contrast for Unsupervised Visual Representation Learning,2020,"Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick",Facebook AI Research,"He, K., Fan, H., Wu, Y., Xie, S., & Girshick, R. (2020). Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 9729-9738).","Wei, C., Wang, H., Shen, W., & Yuille, A. (2020). Co2: Consistent contrast for unsupervised visual representation learning. arXiv preprint arXiv:2010.02217.","Pang, B., Zhang, Y., Li, Y., Cai, J., & Lu, C. (2022, October). Unsupervised visual representation learning by synchronous momentum grouping. In European Conference on Computer Vision (pp. 265-282). Cham: Springer Nature Switzerland.","MoCo is an unsupervised learning framework using contrastive learning to train visual representations. It builds a dynamic dictionary with a momentum-updated encoder and queue, enabling consistent, large-scale dictionaries for better representations, particularly in downstream tasks like detection and segmentation.","Builds a large, consistent dynamic dictionary with a queue.",Uses momentum-based updates for the key encoder.,Transfers effectively to various vision tasks.,High memory requirements due to large dictionary size.,Limited by the complexity of instance discrimination tasks.,Performance gain decreases with scale limitations.,13540,Accuracy,Comparison (%),"['Exemplar', 'RelativePosition', 'Jigsaw', 'Rotation', 'Colorization', 'DeepCluster', 'BigBiGAN', 'InstDisc', 'LocalAgg', 'CPCv1', 'CPCv2', 'CMC', 'AMDIM', 'MoCo']","[46.0, 51.4, 44.6, 55.4, 39.6, 48.4, 56.6, 54.0, 58.8, 48.7, 65.9, 64.1, 63.5, 60.6]",%,11.png,
12,DINO,v2.3.1,self_supervised,세준,Emerging Properties in Self-Supervised Vision Transformers,2021,"Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, Armand Joulin",Facebook AI Research,"Caron, M., Touvron, H., Misra, I., Jégou, H., Mairal, J., Bojanowski, P., & Joulin, A. (2021). Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 9650-9660).","Chen, X., Xie, S., & He, K. (2021). An empirical study of training self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 9640-9649).","d’Ascoli, S., Touvron, H., Leavitt, M. L., Morcos, A. S., Biroli, G., & Sagun, L. (2021, July). Convit: Improving vision transformers with soft convolutional inductive biases. In International conference on machine learning (pp. 2286-2296). PMLR.","DINO (self-distillation with no labels) trains a Vision Transformer in a self-supervised manner. Using a student-teacher framework, the student network learns to predict the teacher's output through self-attention, enabling effective learning without labeled data.",Learns visual features without labeled data.,Enables object segmentation from attention maps.,Achieves strong performance in classification benchmarks.,Computationally intensive training.,"Requires specific configurations (momentum encoder, multi-crop).",Limited performance gains in smaller-scale applications.,5429,k-NN evaluation,Accuracy (%),"[‘Supervised RN50’, ‘SCLR’, ‘MoCov2’, ‘InfoMin’, ‘BarlowT’, ‘OBoW’, ‘BYOL’, ‘DCv2’, ‘SwAV’, ‘DINO RN50’]","[79.3, 69.1, 71.1, 73.0, 73.2, 73.8, 74.4, 75.2, 75.3, 75.3]",%,12.png,
13,DINOv2,v2.3.2,self_supervised,세준,DINOv2: Learning Robust Visual Features without Supervision,2023,"Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Hervé Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, Piotr Bojanowski",Meta AI Research,"Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V., ... & Bojanowski, P. (2023). Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193.","Newell, A., & Deng, J. (2020). How useful is self-supervised pretraining for visual tasks?. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 7345-7354).","Fang, Y., Dong, L., Bao, H., Wang, X., & Wei, F. (2022). Corrupted image modeling for self-supervised visual pre-training. arXiv preprint arXiv:2202.03382.","DINOv2 is a large-scale self-supervised Vision Transformer model for learning robust, general-purpose visual features without supervision. It leverages a curated dataset and a suite of optimized training techniques to produce visual representations that work across various computer vision tasks without fine-tuning.",Uses curated dataset for enhanced feature quality.,Incorporates scalable training optimizations for efficiency.,Matches or surpasses weakly-supervised model benchmarks.,Requires significant computational resources.,"Bias remains toward high-income, Western data sources.",Environmental impact due to large-scale model training.,1648,Top-1,Linear Evaluation (%),"[‘MAE ViT-H/14’, ‘DINO ViT-B/8’, ‘iBOT ViT-L/16’, ‘DINOv2 ViT-S/14’, ‘DINOv2 ViT-B/14’, ‘DINOv2 ViT-L/14’, ‘DINOv2 ViT-g/14’]","[76.6, 79.2, 82.3, 81.1, 84.5, 86.3, 86.5]",%,13.png,
14,Neocognitron,v3.0.1,cnn,세준,Neocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position,1980,Kunihiko Fukushima,NHK Broadcasting Science Research Laboratorie,"Fukushima, K. (1980). Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. Biological cybernetics, 36(4), 193-202.","Fukushima, K. (1989). Analysis of the process of visual pattern recognition by the neocognitron. Neural Networks, 2(6), 413-420.","Fukushima, K., Miyake, S., & Ito, T. (1983). Neocognitron: A neural network model for a mechanism of visual pattern recognition. IEEE transactions on systems, man, and cybernetics, (5), 826-834.","The Neocognitron is a hierarchical, self-organizing neural network inspired by the human visual system. It uses layers of S-cells and C-cells to extract features and recognize visual patterns invariant to position, size, and small distortions. Self-organization occurs through unsupervised learning of repeated stimuli.",Handles position-invariant pattern recognition.,Inspired by the hierarchical structure of the visual cortex.,Learns robust feature representations without supervision.,Requires careful parameter tuning for self-organization.,Sensitive to the number of layers and cells.,Computational limitations for large datasets.,9660,,,,,,14.png,
15,LeNet-1,v3.1.1,cnn,세준,Handwritten Digit Recognition with a Back-Propagation Network,1990,"Y. Le Cun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, L. D. Jackel",AT&T Bell Laboratories,"LeCun, Y., Boser, B., Denker, J., Henderson, D., Howard, R., Hubbard, W., & Jackel, L. (1989). Handwritten digit recognition with a back-propagation network. Advances in neural information processing systems, 2.","LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., & Jackel, L. D. (1989). Backpropagation applied to handwritten zip code recognition. Neural computation, 1(4), 541-551.","Martin, G. L., & Pittman, J. A. (1991). Recognizing hand-printed letters and digits using backpropagation learning. Neural Computation, 3(2), 258-267.","LeNet-1 is a convolutional neural network designed for handwritten digit recognition. It uses convolutional layers with weight sharing and subsampling layers to extract shift-invariant features, followed by fully connected layers for classification, enabling efficient digit recognition with minimal preprocessing.",Incorporates convolution and subsampling layers.,Achieves shift-invariance with weight sharing.,Performs digit recognition with minimal preprocessing.,High reliance on labeled data for training.,Limited to small image datasets.,Requires significant computation for training.,6760,Recognition error rate,Error (%),"[‘training set’, ‘whole test set’]","[1.1, 9.0]",%,15.png,
16,LeNet-5,v3.1.2,cnn,세준,Gradient-Based Learning Applied to Document Recognition,1998,"Y LeCun, L Bottou, Y Bengio, P Haffner",IEEE,"LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278-2324.","Forsyth, D. A., Mundy, J. L., di Gesú, V., Cipolla, R., LeCun, Y., Haffner, P., ... & Bengio, Y. (1999). Object recognition with gradient-based learning. Shape, contour and grouping in computer vision, 319-345.","Chellapilla, K., Puri, S., & Simard, P. (2006, October). High performance convolutional neural networks for document processing. In Tenth international workshop on frontiers in handwriting recognition. Suvisoft.","LeNet-5 is an advanced convolutional neural network designed for recognizing handwritten digits. It employs multiple convolutional and subsampling layers, combined with fully connected layers, to achieve robustness to distortions, shift-invariance, and scalability for larger datasets like MNIST.",Introduces scalable convolutional architectures.,Excels in handling noise and distortions.,Achieves state-of-the-art accuracy for digit recognition.,High computational cost for training on large datasets.,Requires extensive training data for generalization.,Sensitive to initialization and hyperparameters.,70544,Recognition error rate,Error (%),"[‘LeNet-5’, ‘Baseline’]","[0.95, 5.0]",%,16.png,
17,AlexNet,v3.2.1,cnn,정윤,Imagenet classification with deep convolutional neural networks,2012,"Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton",University of Toronto,"Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25.","Sarker, Iqbal H. ""Machine learning: Algorithms, real-world applications and research directions."" SN computer science 2.3 (2021): 160.","Sarker, Iqbal H. ""Deep learning: a comprehensive overview on techniques, taxonomy, applications and research directions."" SN computer science 2.6 (2021): 420.","AlexNet is a deep convolutional neural network that achieved breakthrough performance on ImageNet using ReLU activation, dropout regularization, GPU training, and data augmentation to reduce overfitting.",Utilized GPUs for faster training.,Introduced ReLU for non-saturating activations.,Effective dropout regularization method.,High computational resource requirements.,Susceptible to overfitting without techniques.,Limited by GPU memory constraints.,135156,Top-1,Comparison (%),"['Sparse coding', 'SIFT + FVs', 'CNN']","[47.1, 45.7, 37.5]",%,17.png,
18,VGGNet,v3.2.2,cnn,유안,Very Deep Convolutional Networks For Large-Scale Image Recognition,2014,"Karen Simonyan, Andrew Zisserman",University of Oxford,"Simonyan, K., & Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556.","Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022). High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 10684-10695).","Wang, C. Y., Bochkovskiy, A., & Liao, H. Y. M. (2023). YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 7464-7475).","The paper presents deep convolutional networks (up to 19 layers) using small 3x3 filters for large-scale image recognition, achieving state-of-the-art performance on the ImageNet dataset by focusing on depth rather than complex network structures.",Significant accuracy boost by deepening network.,Uses only 3x3 filters for simplification.,Outperforms previous models on ImageNet.,High computational cost and memory usage.,Longer training time with increased depth.,Dependent on large-scale labeled datasets.,132477,Top-1,Comparison (%),"['D_dense', 'D_multi-crop', 'D_multi-crop & dense']","[24.8, 24.6, 24.4]",%,18.png,
19,GoogLeNet,v3.2.3,cnn,유안,Going Deeper with Convolutions,2014,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich",Google Inc.,"Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., ... & Rabinovich, A. (2015). Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1-9).","Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., ... & Guo, B. (2021). Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 10012-10022).","Wang, C. Y., Bochkovskiy, A., & Liao, H. Y. M. (2023). YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 7464-7475).",The GoogLeNet model (Inception architecture) improves deep network efficiency by using modular layers with mixed kernel sizes for multi-scale feature extraction while keeping computational cost low.,Multi-scale feature extraction via Inception modules.,Optimized for efficient computation with reduced parameters.,Superior performance in classification and detection tasks.,High memory demand during training.,Limited applicability to non-vision tasks.,Complexity in designing and tuning Inception modules.,62896,Top-5,Error (%),"['SuperVision', 'Clarifai', 'MSRA', 'VGG', 'GoogLeNet']","[16.4, 11.7, 7.35, 7.32, 6.67]",%,19.png,
20,ResNet,v3.2.4,cnn,유안,Deep Residual Learning for Image Recognition,2015,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",Microsoft Research,"He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).","Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., ... & Girshick, R. (2023). Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 4015-4026).","He, K., Chen, X., Xie, S., Li, Y., Dollár, P., & Girshick, R. (2022). Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 16000-16009).","The paper introduces a deep residual learning framework to address difficulties in training very deep neural networks. Instead of learning direct mappings, residual networks (ResNets) learn residual functions with shortcut connections, which improves optimization and enables deeper models. ResNets show significant gains in accuracy and outperform shallower networks on multiple benchmarks.",Enables efficient training of extremely deep networks (up to 152 layers).,Uses residual connections to address degradation in deeper models.,Achieves state-of-the-art performance on ImageNet and COCO datasets.,Residual connections don't fully eliminate overfitting in very large datasets.,Requires substantial computational resources for deep models.,"May face diminishing returns with extreme depth (e.g., over 1000 layers).",242492,Top-1,Error (%),"['VGG-16', 'GoogLeNet', 'PReLU-net', 'plain-34', 'ResNet-34 A', 'ResNet-34 B', 'ResNet-34 C', 'ResNet-50', 'ResNet-101', 'ResNet-152']","[28.07, '-', 24.27, 28.54, 25.03, 24.52, 24.19, 22.85, 21.75, 21.43]",%,20.png,
21,DenseNet,v3.2.5,cnn,유안,Densely Connected Convolutional Networks,2017,"Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger",Cornell University,"Huang, G., Liu, Z., Van Der Maaten, L., & Weinberger, K. Q. (2017). Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 4700-4708).","Liu, Z., Mao, H., Wu, C. Y., Feichtenhofer, C., Darrell, T., & Xie, S. (2022). A convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 11976-11986).","Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., ... & Guo, B. (2021). Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 10012-10022).","DenseNet is a deep neural network architecture where each layer is connected to every other layer to improve feature reuse, gradient flow, and efficiency, enabling powerful performance with fewer parameters.",Improved feature reuse through dense connections.,"Mitigates vanishing gradient, making training easier.",Achieves high accuracy with fewer parameters.,Increased memory usage due to dense connections.,May require specific growth rate tuning.,Dense connectivity can slow down computations.,48567,Top-1 (single-crop),Error (%),"['DenseNet-121 (k=32)', 'DenseNet-169 (k=32)', 'DenseNet-201 (k=32)', 'DenseNet-161 (k=48)']","[25.02, 23.80, 22.58, 22.33]",%,21.png,
22,EfficientNet,v3.2.6,cnn,유안,EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,2019,"Mingxing Tan, Quoc Le",Google Research,"Tan, M., & Le, Q. (2019, May). Efficientnet: Rethinking model scaling for convolutional neural networks. In International conference on machine learning (pp. 6105-6114). PMLR.","Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., ... & Sutskever, I. (2021, July). Learning transferable visual models from natural language supervision. In International conference on machine learning (pp. 8748-8763). PMLR.","Bochkovskiy, A., Wang, C. Y., & Liao, H. Y. M. (2020). Yolov4: Optimal speed and accuracy of object detection. arXiv preprint arXiv:2004.10934.","EfficientNet is a family of Convolutional Neural Networks that scales width, depth, and resolution using a compound scaling method, achieving high accuracy with fewer parameters. This balanced scaling approach, coupled with a new baseline network, allows EfficientNet to outperform previous models in accuracy and efficiency across various tasks and datasets.","Balances width, depth, and resolution with compound scaling.",Achieves high accuracy with fewer parameters than prior models.,State-of-the-art performance on ImageNet and transfer learning datasets.,Limited to single-scaling baseline network.,Heavy reliance on neural architecture search.,High computational resources needed for model training.,23775,Top-1,Accuracy (%),"['EfficientNet-B0', 'ResNet-50', 'DenseNet-169', 'EfficientNet-B1', 'ResNet-152', 'DenseNet-264', 'Inception-v3', 'Xception', 'EfficientNet-B2', 'Inception-v4', 'Inception-resnet-v2', 'EfficientNet-B3', 'ResNeXt-101', 'PolyNet', 'EfficientNet-B4', 'SENet', 'NASNet-A', 'AmoebaNet-A', 'PNASNet', 'EfficientNet-B5', 'AmoebaNet-C', 'EfficientNet-B6', 'EfficientNet-B7', 'GPipe']","[76.3, 76.0, 76.2, 78.8, 77.8, 77.9, 78.8, 79.0, 79.8, 80.0, 80.1, 81.1, 80.9, 81.3, 82.6, 82.7, 82.7, 82.8, 82.9, 83.3, 83.5, 84.0, 84.4, 84.3]",%,22.png,
23,Vision Transformer (ViT),v3.3.1,transformer,민혁,An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,2020,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby","Google Research, Brain Team","Dosovitskiy, A. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.","Bao, H., Dong, L., Piao, S., & Wei, F. (2021). Beit: Bert pre-training of image transformers. arXiv preprint arXiv:2106.08254.","Chen, C. F. R., Fan, Q., & Panda, R. (2021). Crossvit: Cross-attention multi-scale vision transformer for image classification. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 357-366).","The Vision Transformer (ViT) divides images into patches, treats them as tokens like words in NLP, and uses self-attention to capture relationships across the image. Trained on large datasets, it outperforms convolutional networks in classification accuracy with fewer computational resources",Applies Transformers directly to image patches as input tokens.,"Achieves strong accuracy, especially when trained on large datasets.",Reduces dependence on convolutional neural networks (CNNs).,Requires large datasets for optimal performance.,"Lacks CNNs’ inherent inductive biases, like spatial locality.","Not as effective on small datasets without pre-training.




",47287,Mean,Accuracy (%),"['ViT-H/14', 'ViT-L/16', 'ViT-L/16', 'ResNet52x4', 'EfficientNet-L2']","[88.55, 87.76, 85.30, 87.54, 88.4]",%,23.png,
24,Swin Transformer,v3.3.2,transformer,민혁,Swin Transformer: Hierarchical Vision Transformer using Shifted Windows,2021,"Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo","Microsoft Research Asia, University of Science and Technology of China, Xian Jiaotong University, Tsinghua University","Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., ... & Guo, B. (2021). Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 10012-10022).","Guo, J., Han, K., Wu, H., Tang, Y., Chen, X., Wang, Y., & Xu, C. (2022). Cmt: Convolutional neural networks meet vision transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 12175-12185).","Chen, Z., Duan, Y., Wang, W., He, J., Lu, T., Dai, J., & Qiao, Y. (2022). Vision transformer adapter for dense predictions. arXiv preprint arXiv:2205.08534.","The Swin Transformer builds hierarchical feature maps with local self-attention in shifted windows, which connects neighboring windows efficiently. This enables strong performance on image classification, object detection, and segmentation tasks by modeling at various scales with linear computational complexity relative to image size.",Uses shifted windows for local self-attention.,Achieves state-of-the-art results on multiple vision tasks.,Scales efficiently with linear computational complexity.,Requires significant computational resources for training.,Limited adaptability for very small datasets.,Challenging optimization for certain dense prediction tasks.,23368,Average,Precision (%),"['DeiT-S', 'R50', 'Swin-T']","[48.0, 46.3, 50.5]",%,24.png,
25,DeiT,v3.3.3,transformer,민혁,Training data-efficient image transformers & distillation through attention,2021,"Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Hervé Jegou",,"Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., & Jégou, H. (2021, July). Training data-efficient image transformers & distillation through attention. In International conference on machine learning (pp. 10347-10357). PMLR.","Yuan, L., Chen, Y., Wang, T., Yu, W., Shi, Y., Jiang, Z. H., ... & Yan, S. (2021). Tokens-to-token vit: Training vision transformers from scratch on imagenet. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 558-567).","Chen, C. F. R., Fan, Q., & Panda, R. (2021). Crossvit: Cross-attention multi-scale vision transformer for image classification. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 357-366).","DeiT (Data-efficient Image Transformers) leverages a teacher-student distillation approach, with a unique ""distillation token"" to improve data efficiency and achieve strong results on ImageNet without large pre-training datasets. It enables Vision Transformers (ViT) to be trained more effectively on smaller datasets, overcoming prior data limitations​.",Introduces a distillation token for efficient learning.,Matches CNNs’ performance on ImageNet using smaller data.,Achieves high accuracy with minimal pre-training requirements.,Limited to classification tasks initially.,Requires a strong teacher model for distillation.,Performance depends on extensive data augmentation.,6997,Top-1,Accuracy (%),"['RegNetY-16GF', 'DeiT-B', 'DeiT-B⚗']","[98.0, 97.5, 98.5]",%,25.png,
26,EVA,v3.3.4,transformer,민혁,EVA: Exploring the Limits of Masked Visual Representation Learning at Scale,2023,"Zhang Ge, Yuxin Fang, Yuxin Wang, Kunchang Li, Ziyi Lin, Wenguang Zhou, Fengwei Yu, Xiangyu Zhang, Tong Lu, Jianbin Jiao, Yu Qiao, Xiaogang Wang, Jianmin Bao, Han Hu","Huazhong University of Science and Technology, Beijing Academy of Artificial Intelligence, Zhejiang University, Beijing Institute of Technology","Fang, Y., Wang, W., Xie, B., Sun, Q., Wu, L., Wang, X., ... & Cao, Y. (2023). Eva: Exploring the limits of masked visual representation learning at scale. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 19358-19369).","Gao, P., Geng, S., Zhang, R., Ma, T., Fang, R., Zhang, Y., ... & Qiao, Y. (2024). Clip-adapter: Better vision-language models with feature adapters. International Journal of Computer Vision, 132(2), 581-595.","Wang, Z., Yu, J., Yu, A. W., Dai, Z., Tsvetkov, Y., & Cao, Y. (2021). Simvlm: Simple visual language model pretraining with weak supervision. arXiv preprint arXiv:2108.10904.","EVA is a large-scale vision model designed to maximize the power of masked image modeling (MIM) by using image-text aligned features. It efficiently scales up to one billion parameters, achieving state-of-the-art results across various tasks, including object detection, segmentation, and classification, using only publicly accessible data​.",Scales MIM with image-text aligned features.,Achieves top results across multiple vision tasks.,Uses efficient training with only public data.,"Relies heavily on large, unlabeled datasets.",High computational requirements for training.,Less effective on smaller or labeled datasets.,579,Top-1,Accuracy (%),"['CoAtNet-4', 'MaxViT-XL', 'MViTv2-H', 'FD-CLIP-L', 'BEiT-3', 'EVA']","[88.6, 88.7, 88.8, 89.0, 89.6, 89.6]",%,26.png,
27,U-Net,v3.4.1,cnn,민혁,U-Net: Convolutional Networks for Biomedical Image Segmentation,2015,"Olaf Ronneberger, Philipp Fischer, and Thomas Brox","University of Freiburg, Germany","Ronneberger, O., Fischer, P., & Brox, T. (2015). U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted intervention–MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18 (pp. 234-241). Springer International Publishing.","Zunair, H., & Hamza, A. B. (2021). Sharp U-Net: Depthwise convolutional network for biomedical image segmentation. Computers in biology and medicine, 136, 104699.","Kaul, C., Manandhar, S., & Pears, N. (2019, April). Focusnet: An attention-based fully convolutional network for medical image segmentation. In 2019 IEEE 16th international symposium on biomedical imaging (ISBI 2019) (pp. 455-458). IEEE.",U-Net is a convolutional network designed for biomedical image segmentation. It has a U-shaped architecture with a contracting path for context capture and an expansive path for precise localization. This structure enables accurate segmentation with limited training data by using data augmentation and efficient feature propagation.,Efficient segmentation with minimal training data.,U-shaped design for context and localization.,Uses extensive data augmentation techniques.,Primarily tested on biomedical images.,Performance sensitive to data augmentation.,Limited generalization beyond specific domains.,95843,Average,IOU,"['IMCB-SG', 'KTH-SE', 'HOUS-US', 'second-best', 'u-net']","[0.2669, 0.7953, 0.5323, 0.83, 0.9203]",,27.png,
28,SegNet,v3.4.2,cnn,민혁,SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation,2017,"Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla",,"Badrinarayanan, V., Kendall, A., & Cipolla, R. (2017). Segnet: A deep convolutional encoder-decoder architecture for image segmentation. IEEE transactions on pattern analysis and machine intelligence, 39(12), 2481-2495.","Long, J., Shelhamer, E., & Darrell, T. (2015). Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 3431-3440).","Chen, L. C., Zhu, Y., Papandreou, G., Schroff, F., & Adam, H. (2018). Encoder-decoder with atrous separable convolution for semantic image segmentation. In Proceedings of the European conference on computer vision (ECCV) (pp. 801-818).","SegNet is a deep convolutional encoder-decoder model designed for efficient image segmentation. It uses max-pooling indices in the decoder to improve boundary accuracy and reduce memory, making it suitable for scene understanding applications, particularly in memory-constrained environments.",Uses encoder's max-pooling indices for efficient upsampling.,Balances memory efficiency with segmentation accuracy.,Ideal for road and indoor scene understanding tasks.,"Limited accuracy on cluttered, complex scenes.",Less effective on very small or infrequent classes.,"Performance depends on large, well-labeled datasets.",20698,Global Average,Accuracy (%),"['SegNet', 'DeepLab-LargeFOV', 'FCN', 'FCN(learnt deconv)', 'DeconvNet']","[88.81, 85.95, 81.97, 83.21, 85.26]",%,28.png,
29,YOLO (You Only Look Once),v3.4.3,cnn,민혁,"You Only Look Once: Unified, Real-Time Object Detection",2016,"Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi","University of Washington, Allen Institute for AI, Facebook AI Research","Redmon, J. (2016). You only look once: Unified, real-time object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition.","Lin, T. Y., Dollár, P., Girshick, R., He, K., Hariharan, B., & Belongie, S. (2017). Feature pyramid networks for object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 2117-2125).","Cai, Z., & Vasconcelos, N. (2018). Cascade r-cnn: Delving into high quality object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 6154-6162).","YOLO (You Only Look Once) is a real-time object detection model that unifies detection as a single regression problem, predicting bounding boxes and class probabilities from an image in one evaluation.",Detects objects in real-time at 45 FPS.,Trained end-to-end for optimal performance.,Reduces background false positives effectively.,"Struggles with small, closely packed objects.",Limited accuracy on unusual object shapes/aspects.,"Treats errors in all box sizes equally, affecting localization.",54448,mean Average,Precision (%),"['100Hz DPM', '30Hz DPM', 'Fast YOLO', 'YOLO', 'Fastest DPM', 'R-CNN Minus R', 'Fast R-CNN', 'Faster R-CNN VGG-16', 'Faster R-CNN ZF', 'YOLO VGG-16']","[16.0, 26.1, 52.7, 63.4, 30.4, 53.5, 70.0, 73.2, 62.1, 66.4]",%,29.png,
30,MobileNetV1,v3.5.1,cnn,민혁,MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications,2017,"Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam",Google Inc.,"Howard, A. G. (2017). Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861.
","Khan, S., Rahmani, H., Shah, S. A. A., Bennamoun, M., Medioni, G., & Dickinson, S. (2018). A guide to convolutional neural networks for computer vision.","Kokkinos, I. (2017). Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 6129-6138).","MobileNetV1 uses depthwise separable convolutions to reduce computational cost and model size, making it ideal for mobile and embedded vision tasks. It introduces width and resolution multipliers for flexible trade-offs between accuracy and efficiency.","Efficient depthwise separable convolutions reduce computations significantly.
",Customisable width and resolution multipliers optimise speed and accuracy.,"Performs well across object detection, classification, and geolocation tasks.",Slight accuracy drop compared to standard convolutions.,Limited scalability for high-end performance requirements.,Requires fine-tuning for specific use cases.,27889,Top-1,Accuracy (%),"['1.0 MobileNet-224', 'GoogleNet', 'VGG 16']","[70.6, 69.8, 71.5]​",%,30.png,
31,MobileNetV2,v3.5.2,cnn,민혁,MobileNetV2: Inverted Residuals and Linear Bottlenecks,2018,"Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen",Google Inc.,"Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., & Chen, L. C. (2018). Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 4510-4520).","Zhang, X., Zhou, X., Lin, M., & Sun, J. (2018). Shufflenet: An extremely efficient convolutional neural network for mobile devices. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 6848-6856).","Howard, A., Zhmoginov, A., Chen, L. C., Sandler, M., & Zhu, M. (2018, June). Inverted residuals and linear bottlenecks: Mobile networks for classification, detection and segmentation. In Proc. CVPR (pp. 4510-4520).","MobileNetV2 introduces inverted residual blocks and linear bottlenecks for efficient feature extraction. It reduces memory usage and computational cost while maintaining accuracy, making it suitable for mobile applications.",Uses inverted residuals to optimise efficiency.,Introduces linear bottlenecks to preserve information.,Supports flexible trade-offs with width and resolution multipliers.,Requires careful tuning of expansion factors.,Limited flexibility for very high-accuracy demands.,Slightly increased complexity compared to MobileNetV1.,25578,Top-1,Accuracy (%),"['MobileNetV1', 'ShuffleNet (1.5)', 'ShuffleNet (x2)', 'NasNet-A', 'MobileNetV2', 'MobileNetV2 (1.4)']","[70.6, 71.5, 73.7, 74.0, 72.0, 74.7]",%,31.png,
32,MobileNetV3,v3.5.3,cnn,민혁,Searching for MobileNetV3,2019,"Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, Quoc V. Le, and Hartwig Adam","Google AI, Google Brain","Howard, A., Sandler, M., Chu, G., Chen, L. C., Chen, B., Tan, M., ... & Adam, H. (2019). Searching for mobilenetv3. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 1314-1324).","Wang, Q., Wu, B., Zhu, P., Li, P., Zuo, W., & Hu, Q. (2020). ECA-Net: Efficient channel attention for deep convolutional neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 11534-11542).","Tan, M., Chen, B., Pang, R., Vasudevan, V., Sandler, M., Howard, A., & Le, Q. V. (2019). Mnasnet: Platform-aware neural architecture search for mobile. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 2820-2828).","MobileNetV3 combines Neural Architecture Search and NetAdapt with advanced design elements like squeeze-and-excite blocks and hard-swish activation. It achieves state-of-the-art efficiency for mobile devices across classification, detection, and segmentation tasks.",Combines NAS and NetAdapt for optimal efficiency.,Introduces hard-swish for better quantization compatibility.,Offers large and small models tailored for resource levels.,Tailored for mobile; less suited for high-end tasks.,Requires extensive hardware-specific tuning.,Relatively complex compared to previous MobileNet versions.,9045,Top-1,Accuracy (%),"['V3-Large 1.0', 'V3-Large 0.75', 'MnasNet-A1', 'ProxylessNAS', 'V2 1.0', 'V3-Small 1.0', 'V3-Small 0.75', 'Mnas-small', 'V2 0.35']","[75.2, 73.3, 75.2, 74.6, 72.0, 67.4, 65.4, 64.9, 60.8]​",%,32.png,
33,SAM (Segment Anything Model),v3.6.1,transformer,민혁,Segment Anything,2023,"Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafon, Tete Xiao, and Piotr Dollar","Meta AI Research, FAIR","Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., ... & Girshick, R. (2023). Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 4015-4026).","Ding, J., Xue, N., Xia, G. S., & Dai, D. (2022). Decoupling zero-shot semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 11583-11592).","Zhou, Z., Lei, Y., Zhang, B., Liu, L., & Liu, Y. (2023). Zegclip: Towards adapting clip for zero-shot semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 11175-11185).","SAM is a promptable segmentation model that predicts valid masks from diverse prompts (points, boxes, text) with zero-shot generalisation. It leverages a massive dataset of 1.1 billion masks and introduces a scalable model architecture for flexible, real-time applications.",Generalises segmentation with zero-shot capability.,Uses a large-scale SA-1B dataset with 1.1 billion masks.,Provides real-time interactive segmentation across prompts.,Struggles with fine boundaries and small structures.,Heavily reliant on training dataset quality.,Not optimised for semantic and panoptic segmentation tasks.,6661,mean,IoU(%),"['SAM', 'RITM', 'SimpleClick', 'FocalClick']","[50, 40, 35, 33]",%,33.png,
34,SEEM,v3.6.2,transformer,민혁,Segment Everything Everywhere All at Once,2023,"Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Wang, Lijuan Wang, Jianfeng Gao, and Yong Jae Lee","University of Wisconsin-Madison, Microsoft Research, Redmond, HKUST, Microsoft Cloud & AI","Zou, X., Yang, J., Zhang, H., Li, F., Li, L., Wang, J., ... & Lee, Y. J. (2024). Segment everything everywhere all at once. Advances in Neural Information Processing Systems, 36.","Lüddecke, T., & Ecker, A. (2022). Image segmentation using text and image prompts. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 7086-7096).","Liang, F., Wu, B., Dai, X., Li, K., Zhao, Y., Zhang, H., ... & Marculescu, D. (2023). Open-vocabulary semantic segmentation with mask-adapted clip. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 7061-7070).","SEEM unifies various segmentation tasks using a promptable architecture. It integrates text, visual, and interactive prompts into a joint visual-semantic space, enabling versatile, compositional, and semantic-aware segmentation across generic, referring, and interactive tasks.","Supports multiple input prompts (text, scribbles, regions).",Generalises well to unseen tasks and domains.,Achieves competitive performance across 9 segmentation benchmarks.,"Requires large, diverse training data for generalisation.",Higher complexity compared to specialised segmentation models.,Limited for extremely fine-grained boundary segmentation.,422,mean,IoU(%),"['SEEM (T)', 'SEEM (B)', 'SEEM (L)', 'SAM (B)', 'SAM (L)', 'SimpleClick (H)']","[63.4, 68.2, 72.8, 57.1, 60.7, 71.5]",%,34.png,
35,RNN (Recurrent Neural Network),v4.0.1,rnn,민혁,Learning representations by back-propagating errors,1986,"David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams","Institute for Cognitive Science, C-015, University of California,  Department of Computer Science, Carnegie-Mellon University","Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning representations by back-propagating errors. nature, 323(6088), 533-536.","Moody, J., & Darken, C. J. (1989). Fast learning in networks of locally-tuned processing units. Neural computation, 1(2), 281-294.","Hinton, G. E., Dayan, P., Frey, B. J., & Neal, R. M. (1995). The"" wake-sleep"" algorithm for unsupervised neural networks. Science, 268(5214), 1158-1161.","RNNs process sequential data using feedback loops in hidden layers, enabling dynamic temporal behaviour. The model propagates errors backward through time for effective weight updates, creating internal memory for past information.",Processes sequential data with feedback connections.,Learns temporal dependencies through backpropagation.,Captures dynamic patterns in time-series and language.,Suffers from vanishing/exploding gradients.,Struggles with long-term dependencies.,Computationally expensive for long sequences.,40394,,,,,,35.png,
36,Long Short-Term Memory (LSTM),v4.0.2,rnn,민혁,Long Short-Term Memory,1997,Sepp Hochreiter and Jürgen Schmidhuber,Technische Universitat Munchen,"Hochreiter, S. (1997). Long Short-term Memory. Neural Computation MIT-Press.","Beran, J., Feng, Y., Ghosh, S., & Kulik, R. (2013). Long-memory processes. Long-Mem. Process.","Schmidhuber, J. (1992). Learning to control fast-weight memories: An alternative to dynamic recurrent networks. Neural Computation, 4(1), 131-139.","LSTM overcomes the vanishing gradient problem in RNNs with memory cells and gating mechanisms that enable learning long-term dependencies. It ensures constant error flow through time, enabling efficient learning in sequences with long time lags.",Solves vanishing/exploding gradient problems.,Enables learning with long-term dependencies in sequences.,Uses memory cells with input/output gates for control.,Computationally intensive compared to basic RNNs.,Inefficient for tasks requiring precise step counting.,Requires careful parameter tuning for optimal performance.,115630,Percentage of successful trials,Successful Trials(%),"['RTRL', 'BPTT', 'CH', 'LSTM']","[0, 0, 33, 100]",%,36.png,
37,Transformer Architecture,v4.1,transformer,민혁,Attention Is All You Need,2017,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin","Google Brain, Google Research, University of Toronto","Vaswani, A. (2017). Attention is all you need. Advances in Neural Information Processing Systems.","Posner, M. I., & Dehaene, S. (1994). Attentional networks. Trends in neurosciences, 17(2), 75-79.","Raz, A., & Buhle, J. (2006). Typologies of attentional networks. Nature reviews neuroscience, 7(5), 367-379.","The Transformer is a sequence-to-sequence model that uses only self-attention mechanisms to handle input and output dependencies. It eliminates the need for recurrence or convolution, allowing parallelisation and better handling of long-range dependencies, achieving state-of-the-art performance in translation tasks.",Eliminates recurrence for full parallelisation.,Uses multi-head attention for global dependencies.,Sets new benchmarks for machine translation tasks.,Computationally expensive for long sequences.,High memory requirements for attention mechanisms.,Requires large-scale data for effective training.,141723,BLEU,Score(points),"['ByteNet', 'Deep-Att + PosUnk', 'GNMT + RL', 'ConvS2S', 'MoE', 'Transformer (base model)', 'Transformer (big)']","[23.75, 39.2, 24.6, 25.16, 26.03, 27.3, 28.4]",points,37.png,
38,T5,v4.1.1.1,transformer,민혁,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,2020,"Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu.",Google Research,"Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., ... & Liu, P. J. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140), 1-67.","He, J., Zhou, C., Ma, X., Berg-Kirkpatrick, T., & Neubig, G. (2021). Towards a unified view of parameter-efficient transfer learning. arXiv preprint arXiv:2110.04366.","Ruder, S., Peters, M. E., Swayamdipta, S., & Wolf, T. (2019, June). Transfer learning in natural language processing. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: Tutorials (pp. 15-18).","T5 unifies NLP tasks by treating them all as text-to-text problems, enabling a single model to handle diverse tasks using the same architecture and objective. It achieves state-of-the-art performance on numerous benchmarks by scaling pre-training, leveraging a denoising objective, and employing the Colossal Clean Crawled Corpus (C4).",Unifies NLP tasks under a text-to-text framework.,Scales to 11 billion parameters for unprecedented results.,Uses C4 dataset for effective pre-training.,Requires significant computational resources for training.,Performance heavily depends on data quality.,Limited by text-only pre-training in some multilingual tasks.,19380,Average,Accuracy(%),"['BERT-Large', 'RoBERTa-Large', 'XLNet-Large', 'ALBERT-XXL', 'T5-Small', 'T5-Base', 'T5-Large', 'T5-3B', 'T5-11B']","[86.2, 88.5, 89.0, 89.4, 77.4, 82.7, 86.4, 88.5, 90.3]​",%,38.png,
39,BART,v4.1.1.2,transformer,민혁,"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",2019,"Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer",Facebook AI,"Lewis, M. (2019). Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461.","Rothe, S., Narayan, S., & Severyn, A. (2020). Leveraging pre-trained checkpoints for sequence generation tasks. Transactions of the Association for Computational Linguistics, 8, 264-280.","Provilkov, I., Emelianenko, D., & Voita, E. (2019). BPE-dropout: Simple and effective subword regularization. arXiv preprint arXiv:1910.13267.","BART is a denoising sequence-to-sequence pre-training model combining a bidirectional encoder and an autoregressive decoder. It generalises masked language modelling by reconstructing corrupted text, excelling in text generation, summarisation, and translation tasks with flexible noise transformations.",Combines bidirectional encoding with autoregressive decoding.,Excels in abstractive summarisation and text generation tasks.,Supports versatile noise transformations for robust training.,High computational cost for large models.,Dependent on large-scale datasets for performance.,Struggles with long-term coherence in generative tasks.,11236,ROUGE,Points,"['PTGEN', 'PTGEN+COV', 'UniLM', 'BERTSUMABS', 'BERTSUMEXTABS', 'ROBERTASHARE', 'BART']","[33.42, 36.38, 40.51, 38.76, 39.18, 37.62, 40.90]",points,39.png,
40,Megatron-LM,v4.1.1.3,transformer,민혁,Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism,2019,"Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro",,"Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., & Catanzaro, B. (2019). Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053.","Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., ... & Sifre, L. (2022). Training compute-optimal large language models. arXiv preprint arXiv:2203.15556.","Smith, S., Patwary, M., Norick, B., LeGresley, P., Rajbhandari, S., Casper, J., ... & Catanzaro, B. (2022). Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990.","Megatron-LM is a scalable transformer-based language model using efficient intra-layer model parallelism to train models with billions of parameters. It leverages distributed tensor computation and optimised GPU memory management for tasks like language modelling, achieving state-of-the-art results in multiple benchmarks.",Enables training up to 8.3B parameter models.,Achieves state-of-the-art results on WikiText103 and LAMBADA.,Scales efficiently across 512 GPUs.,"Requires extensive computational resources (e.g., 512 GPUs).",Performance heavily depends on advanced infrastructure.,Not feasible for smaller-scale training setups.,1766,Perplexity,Perplexity,"['Megatron-LM 355M', 'Megatron-LM 2.5B', 'Megatron-LM 8.3B', 'Previous SOTA']","[19.31, 12.76, 10.81, 15.79]",Perplexity,40.png,
41,BERT,v4.1.2.1,transformer,민혁,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova",Google AI Language,"Devlin, J. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.","Jiao, X., Yin, Y., Shang, L., Jiang, X., Chen, X., Li, L., ... & Liu, Q. (2019). Tinybert: Distilling bert for natural language understanding. arXiv preprint arXiv:1909.10351.","Ainslie, J., Ontanon, S., Alberti, C., Cvicek, V., Fisher, Z., Pham, P., ... & Yang, L. (2020). ETC: Encoding long and structured inputs in transformers. arXiv preprint arXiv:2004.08483.",BERT (Bidirectional Encoder Representations from Transformers) pre-trains deep bidirectional representations by masking tokens and predicting them using context. It enables state-of-the-art performance across various NLP tasks by fine-tuning a unified model on specific tasks with minimal modifications.,Bidirectional Transformer for context-aware learning.,Pre-trained on massive text data for generalisation.,Achieves state-of-the-art results across 11 NLP benchmarks.,Computationally expensive due to bidirectionality.,Requires large-scale datasets for pre-training.,Performance depends heavily on task-specific fine-tuning.,118772,Average,Accuracy(%),"['Pre-OpenAI SOTA', 'BiLSTM+ELMo+Attn', 'OpenAI GPT', 'BERTBASE', 'BERTLARGE']","[74.0, 71.0, 75.1, 79.6, 82.1]",%,41.png,
42,RoBERTa,v4.1.2.2,transformer,민혁,RoBERTa: A Robustly Optimized BERT Pretraining Approach,2019,"Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov","Paul G. Allen School of Computer Science & Engineering, University of Washington, Seattle, WA, Facebook AI","Liu, Y. (2019). Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 364.","You, Y., Li, J., Reddi, S., Hseu, J., Kumar, S., Bhojanapalli, S., ... & Hsieh, C. J. (2019). Large batch optimization for deep learning: Training bert in 76 minutes. arXiv preprint arXiv:1904.00962.","Han, X., Zhang, Z., Ding, N., Gu, Y., Liu, X., Huo, Y., ... & Zhu, J. (2021). Pre-trained models: Past, present and future. AI Open, 2, 225-250.","RoBERTa improves upon BERT by optimising pretraining strategies, including longer training on more data, dynamic masking, and removing the Next Sentence Prediction objective. It achieves state-of-the-art performance on multiple benchmarks without architectural changes.",Removes Next Sentence Prediction for efficiency.,Pretrained with 160GB of data for robustness.,Introduces dynamic token masking during training.,Requires significant computational resources for training.,Performance depends heavily on dataset quality.,Not designed for low-resource environments.,16222,Average,Accuracy(%),"['BERTLARGE', 'XLNetLARGE', 'RoBERTa']","[86.6, 88.4, 88.5]",%,42.png,
43,DeBERTa,v4.1.2.3,transformer,민혁,DeBERTa: Decoding-enhanced BERT with Disentangled Attention,2020,"Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen","Microsoft Dynamics 365 AI, Microsoft Research","He, P., Liu, X., Gao, J., & Chen, W. (2020). Deberta: Decoding-enhanced bert with disentangled attention. arXiv preprint arXiv:2006.03654.","Song, K., Tan, X., Qin, T., Lu, J., & Liu, T. Y. (2020). Mpnet: Masked and permuted pre-training for language understanding. Advances in neural information processing systems, 33, 16857-16867.","Cui, Y., Che, W., Liu, T., Qin, B., Wang, S., & Hu, G. (2020). Revisiting pre-trained models for Chinese natural language processing. arXiv preprint arXiv:2004.13922.","DeBERTa introduces disentangled attention mechanisms to separate content and positional embeddings, improving understanding of word relationships. It also uses an enhanced mask decoder for better masked token predictions and virtual adversarial training to improve generalisation, achieving state-of-the-art performance in NLP tasks with efficient pre-training.",Disentangled attention separates content and positional embeddings.,Enhanced mask decoder improves token predictions.,Achieves superior results on GLUE and SuperGLUE benchmarks.,Computationally intensive pre-training process.,Requires large-scale high-quality datasets.,Dependency on extensive hyperparameter tuning for optimal performance.,2662,Average,Accuracy(%),"['BERTLARGE', 'RoBERTaLARGE', 'XLNetLARGE', 'ELECTRALARGE', 'DeBERTaLARGE']","[84.05, 88.82, 89.15, 89.46, 90.00]​",%,43.png,
44,GPT,v4.1.3.1,transformer,민혁,Improving Language Understanding by Generative Pre-training,2018,"Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever",OpenAI,"Radford, A. (2018). Improving language understanding by generative pre-training.","Song, K., Tan, X., Qin, T., Lu, J., & Liu, T. Y. (2020). Mpnet: Masked and permuted pre-training for language understanding. Advances in neural information processing systems, 33, 16857-16867.","Sun, Y., Wang, S., Li, Y., Feng, S., Tian, H., Wu, H., & Wang, H. (2020, April). Ernie 2.0: A continual pre-training framework for language understanding. In Proceedings of the AAAI conference on artificial intelligence (Vol. 34, No. 05, pp. 8968-8975).",GPT is a unidirectional Transformer-based model pre-trained on unsupervised data with a generative objective and fine-tuned for specific tasks. It demonstrated the effectiveness of transfer learning by achieving state-of-the-art performance in multiple NLP tasks using task-specific input transformations.,Pre-trained generative language model.,Effective task-specific fine-tuning with minimal modifications.,State-of-the-art results on 9 out of 12 benchmarks.,Limited bidirectional context in pre-training.,Computationally expensive for large-scale fine-tuning.,Relies on large annotated datasets for fine-tuning.,11620,Average,Accuracy(%),"['ESIM+ELMo', 'OpenAI GPT', 'Fine-tuned Transformer LM']","[68.9, 72.8, 72.8]​",%,44.png,
45,GPT-2,v4.1.3.2,transformer,민혁,Language Models are Unsupervised Multitask Learners,2019,"Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever",OpenAI,"Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language models are unsupervised multitask learners. OpenAI blog, 1(8), 9.","Guu, K., Lee, K., Tung, Z., Pasupat, P., & Chang, M. (2020, November). Retrieval augmented language model pre-training. In International conference on machine learning (pp. 3929-3938). PMLR.","Schick, T., Dwivedi-Yu, J., Dessì, R., Raileanu, R., Lomeli, M., Hambro, E., ... & Scialom, T. (2024). Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36.","GPT-2 is a generative Transformer model trained on WebText, demonstrating zero-shot learning capabilities for multiple NLP tasks. It uses a larger dataset and model size (1.5B parameters) than GPT, achieving state-of-the-art performance in language modelling and demonstrating coherent, contextually aware text generation.",Excels in zero-shot multitask learning.,Scales to 1.5B parameters for high performance.,Trained on diverse WebText data.,Struggles with tasks requiring domain-specific knowledge.,Computationally expensive to train and use.,Performance depends on dataset quality and size.,14283,LAMBADA,Accuracy(%),"['SOTA', 'GPT (117M)', 'GPT-2 (345M)', 'GPT-2 (762M)', 'GPT-2 (1542M)']","[59.23, 45.99, 55.48, 60.12, 63.24]",%,45.png,
46,GPT-3,v4.1.3.3,transformer,민혁,Language Models are Few-Shot Learners,2020,"Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, and Dario Amodei",OpenAI,"Brown, T. B. (2020). Language models are few-shot learners. arXiv preprint arXiv:2005.14165.","Gao, T., Fisch, A., & Chen, D. (2020). Making pre-trained language models better few-shot learners. arXiv preprint arXiv:2012.15723.","Zhao, Z., Wallace, E., Feng, S., Klein, D., & Singh, S. (2021, July). Calibrate before use: Improving few-shot performance of language models. In International conference on machine learning (pp. 12697-12706). PMLR.","GPT-3 is a 175-billion parameter autoregressive language model that excels in zero-, one-, and few-shot learning tasks. It demonstrates strong performance across diverse NLP tasks by using in-context learning without task-specific fine-tuning, leveraging pre-trained knowledge efficiently.",Few-shot learning achieves near SOTA performance.,Trained on a 570GB curated dataset with Common Crawl.,Scales to 175 billion parameters for unmatched capabilities.,Requires substantial computational resources to train.,Lacks bidirectional context understanding.,Challenges with coherence in extended text generation.,35502,LAMBADA,Accuracy(%),"['GPT', 'GPT-2 (1.5B)', 'GPT-3 (2.7B)', 'GPT-3 (6.7B)', 'GPT-3 (13B)', 'GPT-3 (175B)']","[63.24, 68.7, 72.5, 76.2, 78.1, 86.4]",%,46.png,
47,GPT-4,v4.1.3.4,transformer,민혁,GPT-4 Technical Report,2023,OpenAI,OpenAI,"Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., ... & McGrew, B. (2023). Gpt-4 technical report. arXiv preprint arXiv:2303.08774.","Sanderson, K. (2023). GPT-4 is here: what scientists think. Nature, 615(7954), 773.","Gimpel, H., Hall, K., Decker, S., Eymann, T., Lämmermann, L., Mädche, A., ... & Vandrik, S. (2023). Unlocking the power of generative AI models and systems such as GPT-4 and ChatGPT for higher education: A guide for students and lecturers (No. 02-2023). Hohenheim Discussion Papers in Business, Economics and Social Sciences.","GPT-4 is a large multimodal language model capable of processing both text and image inputs and generating text outputs. It achieves human-level performance on various benchmarks, leveraging pre-training and alignment through Reinforcement Learning from Human Feedback (RLHF).",Multimodal input capability (text and images).,Excels in human-like performance on academic and professional benchmarks.,Robust alignment using RLHF improves safety and factual accuracy.,Susceptible to hallucinations and reasoning errors.,Computationally expensive to train and deploy.,Limited context window for extended input sequences.,5553,MMLU,Accuracy(%),"['GPT-3.5', 'GPT-4', 'Chinchilla', 'PaLM']","[70.0, 86.4, 70.7, 75.2]",%,47.png,
48,PaLM,v4.1.3.5,transformer,민혁,PaLM: Scaling Language Modeling with Pathways,2022,"Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al.",Google Research,"Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., ... & Fiedel, N. (2023). Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240), 1-113.","Gao, T., Fisch, A., & Chen, D. (2020). Making pre-trained language models better few-shot learners. arXiv preprint arXiv:2012.15723.","Wang, S., Fang, H., Khabsa, M., Mao, H., & Ma, H. (2021). Entailment as few-shot learner. arXiv preprint arXiv:2104.14690.","PaLM is a 540-billion parameter dense Transformer language model trained on 780 billion tokens using the Pathways system. It demonstrates state-of-the-art performance in NLP, reasoning, and code generation, leveraging efficient scaling and novel techniques like multi-query attention and SwiGLU activations.",Achieves state-of-the-art results in reasoning and code generation tasks.,Efficiently scales across 6144 TPU v4 chips.,Introduces SwiGLU activations and parallel Transformer layers.,High computational requirements for training and deployment.,Challenges in multilingual tasks with limited data.,Prone to memorisation and data contamination issues.,5001,MMLU,Accuracy(%),"['PaLM 8B', 'PaLM 62B', 'PaLM 540B', 'Chinchilla']","[25.3, 53.7, 69.3, 67.5]",%,48.png,
49,PaLM 2,v4.1.3.6,transformer,민혁,PaLM 2 Technical Report,2023,Google,Google,"Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., ... & Wu, Y. (2023). Palm 2 technical report. arXiv preprint arXiv:2305.10403.","Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., ... & Irving, G. (2021). Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446.","Hsieh, C. Y., Li, C. L., Yeh, C. K., Nakhost, H., Fujii, Y., Ratner, A., ... & Pfister, T. (2023). Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes. arXiv preprint arXiv:2305.02301.","PaLM 2 is a state-of-the-art Transformer-based language model, integrating improved multilingual data, enhanced architectural objectives, and optimal compute scaling. It surpasses its predecessor in language comprehension, reasoning, and multilingual tasks, while maintaining computational efficiency for faster and broader deployment.",Enhanced multilingual understanding across hundreds of languages.,Outperforms PaLM in reasoning and language proficiency tasks.,"Smaller model, more efficient, and lower inference cost.",Requires significant computational resources for training.,Multilingual performance varies for underrepresented languages.,Susceptible to hallucinations in generated outputs.,1391,Super GLUE,Average Score(%),"['Gopher 280B', 'Chinchilla 70B', 'U-PaLM-540B', 'Flan-U-PaLM-540B', 'PaLM 2-Large', 'Flan-PaLM 2-Large']","[60, 67.6, 71.5, 74.1, 78.3, 81.2]",%,49.png,
50,Claude,v4.1.3.7,transformer,민혁,Constitutional AI: Harmlessness from AI Feedback,2022,Anthropic,Anthropic,"Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., ... & Kaplan, J. (2022). Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073.","Zhang, Y., Liao, Q. V., & Bellamy, R. K. (2020, January). Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making. In Proceedings of the 2020 conference on fairness, accountability, and transparency (pp. 295-305).","Zhang, Y., Liao, Q. V., & Bellamy, R. K. (2020, January). Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making. In Proceedings of the 2020 conference on fairness, accountability, and transparency (pp. 295-305).","Constitutional AI trains models to be helpful, honest, and harmless using AI-driven critiques and revisions guided by natural language principles. This method minimises reliance on human labels, ensuring ethical alignment and transparent decision-making through supervised learning and reinforcement learning with AI feedback.",Reduces reliance on human labels by using constitutional principles.,Incorporates chain-of-thought reasoning for improved decision-making.,Trains non-evasive models that explain objections to harmful requests.,Effectiveness depends heavily on quality and coverage of principles.,May struggle with nuanced or context-specific harms.,Risk of overgeneralisation in critique and revision processes.,1101,Elo score,Points,"['SL-CAI', 'RLHF-Helpful', 'RLHF-HH', 'RL-CAI']","[150, 100, 50, 200]",points,50.png,
51,LaMDA,v4.1.3.8,transformer,민혁,LaMDA: Language Models for Dialog Applications,2022,Google,Google,"Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H. T., ... & Le, Q. (2022). Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239.","Miller, A. H., Feng, W., Fisch, A., Lu, J., Batra, D., Bordes, A., ... & Weston, J. (2017). Parlai: A dialog research software platform. arXiv preprint arXiv:1705.06476.","Glaese, A., McAleese, N., Trębacz, M., Aslanides, J., Firoiu, V., Ewalds, T., ... & Irving, G. (2022). Improving alignment of dialogue agents via targeted human judgements. arXiv preprint arXiv:2209.14375.","LaMDA (Language Model for Dialogue Applications) is a Transformer-based neural language model optimized for dialogues. It uses fine-tuning and external knowledge sources to improve conversational quality, safety, and factual grounding, achieving human-like dialogue capabilities through sensibleness, specificity, and interestingness metrics.",Fine-tuned for dialogue safety and groundedness.,Scales to 137 billion parameters with high-quality multi-turn conversations.,Uses external tools like retrieval systems for factual accuracy.,"Relies on crowdworker annotations, limiting scalability.",Struggles with subtle quality and safety issues.,Requires external sources for consistent factual accuracy.,1520,Sensibleness,Percentage of sensible responses(%),"['PT (2B)', 'PT (8B)', 'PT (137B)', 'FT quality-safety (137B)', 'LaMDA (2B)', 'LaMDA (8B)', 'LaMDA (137B)']","[76.6, 79.1, 80.2, 92.8, 81.8, 88, 92.3]",%,51.png,
52,LLaMA,v4.1.3.9.1,transformer,민혁,LLaMA: Open and Efficient Foundation Language Models,2023,"Hugo Touvron,Thibaut Lavril, Gautier Izacard,Xavier Martinet Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozière, Naman Goyal Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin Edouard Grave, Guillaume Lample",Meta AI,"Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M. A., Lacroix, T., ... & Lample, G. (2023). Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.","Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., ... & Sifre, L. (2022). Training compute-optimal large language models. arXiv preprint arXiv:2203.15556.","Hsieh, C. Y., Li, C. L., Yeh, C. K., Nakhost, H., Fujii, Y., Ratner, A., ... & Pfister, T. (2023). Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes. arXiv preprint arXiv:2305.02301.","LLaMA is a family of Transformer-based language models (7B to 65B parameters) trained on publicly available datasets. Designed for efficiency, LLaMA achieves state-of-the-art performance across benchmarks with significantly fewer parameters than proprietary models like GPT-3 and Chinchilla.",Outperforms GPT-3 despite being 10× smaller.,Uses only publicly available data for training.,Achieves competitive results with Chinchilla and PaLM.,Limited pre-training data compared to proprietary models.,Struggles with specific domains requiring extensive book data.,High computational cost for large-scale models.,10520,Exact Match,Percentage(%),"['GPT-3 175B', 'Gopher 280B', 'Chinchilla 70B', 'PaLM 540B', 'LLaMA 7B', 'LLaMA 13B', 'LLaMA 33B', 'LLaMA 65B']","[14.6, 10.1, 16.6, 21.2, 16.8, 20.1, 24.9, 23.8]",%,52.png,
53,LLaMA 2,v4.1.3.9.2,transformer,민혁,Llama 2: Open Foundation and Fine-Tuned Chat Models,2023,"GenAI, Meta","GenAI, Meta","Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., ... & Scialom, T. (2023). Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.","Ding, N., Chen, Y., Xu, B., Qin, Y., Zheng, Z., Hu, S., ... & Zhou, B. (2023). Enhancing chat language models by scaling high-quality instructional conversations. arXiv preprint arXiv:2305.14233.","Xu, C., Guo, D., Duan, N., & McAuley, J. (2023). Baize: An open-source chat model with parameter-efficient tuning on self-chat data. arXiv preprint arXiv:2304.01196.","LLaMA 2 is a family of open-source language models, ranging from 7B to 70B parameters, optimized for dialogue and general applications. Fine-tuned with Reinforcement Learning with Human Feedback (RLHF), it outperforms many open-source and some proprietary models on helpfulness and safety benchmarks.",Open-sourced models with up to 70B parameters.,Enhanced dialogue capabilities through RLHF and fine-tuning.,Achieves competitive performance against proprietary models.,Limited multilingual training data affects non-English performance.,Significant computational cost for training and fine-tuning.,Potential safety concerns in specific edge cases.,10084,Exact Match,Percentage(%),"['PaLM 540B', 'Falcon 40B', 'MPT 30B', 'Llama 2 7B', 'Llama 2 13B', 'Llama 2 70B']","[68.3, 59.1, 50.4, 64.7, 68.9, 72.8]",%,53.png,
54,Mistral,v4.1.3.9.3,transformer,민혁,Mistral 7B,2023,"Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed",,"Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. D. L., ... & Sayed, W. E. (2023). Mistral 7B. arXiv preprint arXiv:2310.06825.","Black, S., Biderman, S., Hallahan, E., Anthony, Q., Gao, L., Golding, L., ... & Weinbach, S. (2022). Gpt-neox-20b: An open-source autoregressive language model. arXiv preprint arXiv:2204.06745.","Qin, Y., Liang, S., Ye, Y., Zhu, K., Yan, L., Lu, Y., ... & Sun, M. (2023). Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789.","Mistral 7B is a 7-billion-parameter Transformer-based language model optimized for performance and efficiency. It outperforms larger models like LLaMA 2 (13B) across benchmarks, leveraging innovations like Grouped Query Attention (GQA) and Sliding Window Attention (SWA) to enhance speed, scalability, and sequence handling.","Surpasses LLaMA 2 (13B) across reasoning, mathematics, and code tasks.",Utilizes GQA for faster inference and reduced memory requirements.,Handles long sequences efficiently with Sliding Window Attention.,Limited parameter count restricts knowledge storage.,Performance slightly lags behind specialised fine-tuned models.,High computational cost for extended sequence tasks.,858,MMLU,Accuracy(%),"['LLaMA 2 7B', 'LLaMA 2 13B', 'Code-LLaMA 7B', 'Mistral 7B']","[44.4, 55.6, 36.9, 60.1]",%,54.png,
55,Mixtral,v4.1.3.9.4,transformer,민혁,Mixtral of Experts,2023,"Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, EmmaBouHanna,Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed",,"Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., ... & Sayed, W. E. (2024). Mixtral of experts. arXiv preprint arXiv:2401.04088.","Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. D. L., ... & Sayed, W. E. (2023). Mistral 7B. arXiv preprint arXiv:2310.06825.","Jiang, D., Ren, X., & Lin, B. Y. (2023). Llm-blender: Ensembling large language models with pairwise ranking and generative fusion. arXiv preprint arXiv:2306.02561.","Mixtral 8x7B is a sparse Mixture of Experts (SMoE) language model, utilising only two active experts per token, achieving state-of-the-art performance across benchmarks. It combines 47 billion parameters with efficient inference, excelling in mathematics, code generation, and multilingual tasks while outperforming larger models like LLaMA 2 70B and GPT-3.5.","Outperforms LLaMA 2 70B in mathematics, code, and multilingual tasks.",Utilises only 13B active parameters per token for inference.,Fine-tuned Instruct model surpasses GPT-3.5 Turbo and Claude-2.1.,Increased memory demands due to sparse parameter count.,Routing mechanism introduces additional computational overhead.,"Bias challenges persist, requiring fine-tuning for mitigation.",1017,MMLU,Accuracy(%),"['LLaMA 2 7B', 'LLaMA 2 13B', 'LLaMA 2 70B', 'Mistral 7B', 'Mixtral 8x7B']","[44.4, 55.6, 69.9, 62.5, 70.6]",%,55.png,
56,Phi-2,v4.1.3.9.5,transformer,민혁,Phi-2: The surprising power of small language models,2023,Microsoft GenAI,Microsoft GenAI,"Javaheripi, M., Bubeck, S., Abdin, M., Aneja, J., Bubeck, S., Mendes, C. C. T., ... & Gopi, S. (2023). Phi-2: The surprising power of small language models. Microsoft Research Blog, 1, 3.","Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., ... & Sifre, L. (2022). An empirical analysis of compute-optimal large language model training. Advances in Neural Information Processing Systems, 35, 30016-30030.","McCoy, R. T., Yao, S., Friedman, D., Hardy, M., & Griffiths, T. L. (2023). Embers of autoregression: Understanding large language models through the problem they are trained to solve. arXiv preprint arXiv:2309.13638.","Phi-2 is a 2.7-billion-parameter Transformer-based language model, excelling in reasoning, math, and coding. It leverages high-quality data and innovative scaling techniques to achieve performance comparable to larger models like LLaMA 2 (70B), while remaining compact and efficient.",Matches or outperforms models 25× larger in reasoning tasks.,Excels in math and coding with high data efficiency.,Compact size promotes ease of fine-tuning and deployment.,Lack of alignment via RLHF impacts safety guarantees.,Limited multilingual performance compared to specialised models.,Performance on domain-specific benchmarks remains underexplored.,157,BBH,Average Performance(%),"['LLaMA 2 7B', 'LLaMA 2 13B', 'LLaMA 2 70B', 'Mistral 7B', 'Phi-2 2.7B']","[40.0, 47.8, 66.5, 57.2, 59.2]",%,56.png,
57,Gemini,v4.1.3.10.1,transformer,민혁,Gemini: A Family of Highly Capable Multimodal Models,2023,"Gemini Team, Google",Google,"Team, G., Anil, R., Borgeaud, S., Alayrac, J. B., Yu, J., Soricut, R., ... & Blanco, L. (2023). Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805.","Yang, Z., Li, L., Lin, K., Wang, J., Lin, C. C., Liu, Z., & Wang, L. (2023). The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint arXiv:2309.17421, 9(1), 1.","Yue, X., Ni, Y., Zhang, K., Zheng, T., Liu, R., Zhang, G., ... & Chen, W. (2024). Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 9556-9567).","Gemini is a family of multimodal models excelling in text, image, audio, and video processing. With sizes tailored to diverse applications, its Ultra model surpasses human-expert benchmarks like MMLU, showcasing exceptional reasoning and cross-modal understanding capabilities.",Achieves state-of-the-art performance on 30 of 32 benchmarks.,First model to exceed human-expert MMLU score.,Enables seamless cross-modal reasoning and output generation.,Requires significant computational resources for training.,Challenges with data contamination on some benchmarks.,Struggles with high-level causal reasoning tasks.,2023,MMLU,Accuracy(%),"['Gemini Ultra', 'Gemini Pro', 'GPT-4', 'PaLM 2-L']","[90.04, 79.13, 87.29, 78.4]",%,57.png,
58,Claude 3,v4.1.3.10.2,transformer,민혁,"The Claude 3 Model Family: Opus, Sonnet, Haiku",2024,Anthropic,Anthropic,"Anthropic. (2024). The Claude 3 model family: Opus, Sonnet, Haiku.","Anthropic. (2024, March 4). Introducing the next generation of Claude.","Anthropic. (2024, March 13). Claude 3 Haiku: our fastest model yet.","Claude 3 models (Opus, Sonnet, and Haiku) excel in multimodal understanding, including reasoning, math, coding, and multilingual tasks. Opus delivers state-of-the-art performance on benchmarks like MMLU, while Sonnet balances speed and intelligence, and Haiku provides the fastest, most cost-effective option for enterprise use.",Opus achieves state-of-the-art on MMLU with 88.2% accuracy.,Multimodal capabilities enable image-text processing for diverse applications.,Significant improvements in non-English fluency and multilingual reasoning.,Performance varies for low-resource languages and smaller datasets.,High computational requirements for Opus and Sonnet models.,Limited support for highly specialised or domain-specific tasks.,208,MMLU,Accuracy(%),"['Claude 3 Opus', 'Claude 3 Sonnet', 'Claude 3 Haiku', 'GPT-4', 'GPT-3.5', 'Gemini 1.0 Ultra', 'Gemini 1.5 Pro', 'Gemini 1.0 Pro']","[86.8, 79.0, 75.2, 86.4, 70.0, 83.7, 81.9, 71.8]",%,58.png,
59,Transformer-XL,v4.2.1,transformer,민혁,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,2019,"Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov","Carnegie Mellon University, Google Brain","Dai, Z. (2019). Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860.","Irie, K., Zeyer, A., Schlüter, R., & Ney, H. (2019). Language modeling with deep transformers. arXiv preprint arXiv:1905.04226.","Hoover, B., Strobelt, H., & Gehrmann, S. (2019). exbert: A visual analysis tool to explore learned representations in transformers models. arXiv preprint arXiv:1910.05276.","Transformer-XL introduces segment-level recurrence and relative positional encodings, enabling it to model long-term dependencies beyond fixed-length contexts. It outperforms state-of-the-art models in language modeling benchmarks while being significantly faster during evaluation.",Enables modeling long-term dependencies beyond fixed-length contexts.,Achieves state-of-the-art perplexity on WikiText-103 and enwik8.,"Provides 1,800× faster evaluation compared to vanilla Transformers.",Increased memory usage with segment-level recurrence.,Requires careful tuning for optimal segment length.,Context fragmentation still present in extreme cases.,4464,Perplexity,Perplexity,"['LSTM', 'TCN', 'GCNN-8', 'GCNN-14', 'QRNN', 'Transformer-XL Standard', 'Transformer-XL Large']","[48.7, 45.2, 44.9, 37.2, 33.0, 24.0, 18.3]",Perplexity,59.png,
60,XLNet,v4.2.2,transformer,민혁,XLNet: Generalized Autoregressive Pretraining for Language Understanding,2019,"Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le","Carnegie Mellon University, Google AI Brain Team","Yang, Z. (2019). XLNet: Generalized Autoregressive Pretraining for Language Understanding. arXiv preprint arXiv:1906.08237.","Song, K., Tan, X., Qin, T., Lu, J., & Liu, T. Y. (2020). Mpnet: Masked and permuted pre-training for language understanding. Advances in neural information processing systems, 33, 16857-16867.","Sun, Y., Wang, S., Li, Y., Feng, S., Tian, H., Wu, H., & Wang, H. (2020, April). Ernie 2.0: A continual pre-training framework for language understanding. In Proceedings of the AAAI conference on artificial intelligence (Vol. 34, No. 05, pp. 8968-8975).","XLNet is a Transformer-based language model leveraging permutation-based training to incorporate bidirectional context. By combining autoregressive modeling with innovations from Transformer-XL, it outperforms BERT on multiple benchmarks, including natural language understanding and reading comprehension tasks.",Uses permutation-based language modeling for bidirectional context.,"Outperforms BERT on GLUE, SQuAD, and RACE benchmarks.",Incorporates Transformer-XL’s segment recurrence and relative encoding.,Requires more computational resources than BERT.,Training complexity increases with permutations.,Lacks efficiency improvements for extremely long texts.,10314,EM,Percentage(%),"['BERT', 'RoBERTa', 'XLNet']","[84.1, 88.9, 89.7]",%,60.png,
61,ELECTRA,v4.2.3,transformer,유안,ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators,2020,"Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning",Google Research,"Clark, K. (2020). Electra: Pre-training text encoders as discriminators rather than generators. arXiv preprint arXiv:2003.10555.","He, K., Chen, X., Xie, S., Li, Y., Dollár, P., & Girshick, R. (2022). Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 16000-16009).","Liu, Z., Mao, H., Wu, C. Y., Feichtenhofer, C., Darrell, T., & Xie, S. (2022). A convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 11976-11986).","ELECTRA is a pre-training method for NLP that trains a model to detect if input tokens were replaced by plausible alternatives, rather than generating tokens as in BERT. This approach achieves higher efficiency and performance, especially on smaller models.","Learns from all tokens, improving compute efficiency.",Outperforms BERT on downstream tasks with less compute.,Particularly effective on small and medium-sized models.,Slightly lower masked language modeling accuracy than BERT.,More complex architecture with generator-discriminator setup.,Limited fine-tuning compatibility across some generative tasks.,4349,GLUE,GLUE Score,"['ELMo', 'GPT', 'BERT-Small', 'BERT-Base', 'ELECTRA-Small', 'ELECTRA-Base']","[71.2, 78.8, 75.1, 82.2, 79.9, 85.1]",,61.png,
62,Longformer,v4.2.3.1,transformer,유안,Longformer: The Long-Document Transformer,2020,"Iz Beltagy, Matthew E. Peters, Arman Cohan","Allen Institute for Artificial Intelligence, Seattle, WA, USA","Beltagy, I., Peters, M. E., & Cohan, A. (2020). Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150.","Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., ... & Liang, P. (2021). On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258.","Zhu, X., Su, W., Lu, L., Li, B., Wang, X., & Dai, J. (2020). Deformable detr: Deformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159.",Longformer is a transformer model optimized for long documents using an efficient attention mechanism that scales linearly with sequence length.,Linear scaling attention for processing long sequences,Outperforms RoBERTa on document-level tasks,Introduces Longformer-Encoder-Decoder (LED) for long document generation tasks,Limited pretrained resources for longer sequences,Dependence on task-specific attention configurations,Computationally intensive fine-tuning,4282,ROUGE-1,ROUGE-1 Score,"['Discourse-aware', 'Extr-Abst-TLM', 'Dancer', 'Pegasus', 'LED-large (seqlen: 4,096)', 'BigBird (seqlen: 4,096)', 'LED-large (seqlen: 16,384)']","[35.80, 41.62, 42.70, 44.21, 44.40, 46.63, 46.63]",,62.png,
63,Reformer,v4.2.3.2,transformer,유안,Reformer: The Efficient Transformer,2020,"Nikita Kitaev, Łukasz Kaiser, Anselm Levskaya",Google Research,"Kitaev, N., Kaiser, Ł., & Levskaya, A. (2020). Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451.","Zhou, H., Zhang, S., Peng, J., Zhang, S., Li, J., Xiong, H., & Zhang, W. (2021, May). Informer: Beyond efficient transformer for long sequence time-series forecasting. In Proceedings of the AAAI conference on artificial intelligence (Vol. 35, No. 12, pp. 11106-11115).","Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., ... & Fiedel, N. (2023). Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240), 1-113.","Reformer is an efficient variant of Transformer that reduces memory and computational costs for long sequences by using locality-sensitive hashing for attention and reversible layers, allowing it to train faster with less memory on large datasets.","Efficient Attention: LSH reduces attention complexity from 
𝑂(𝐿^2) to 𝑂(𝐿log⁡𝐿).",Memory Savings: Reversible layers eliminate layer-wise memory buildup.,"Scalability: Handles long sequences, applicable for text, image, and music generation.
",Approximation in Attention: LSH attention can slightly reduce accuracy.,Limited Hash Control: Hash collisions may miss some context.,Performance: Optimal only with specific hyperparameter tuning.,2825,LSH-4,Accuracy (%),"['Full Attention', 'LSH-8', 'LSH-4', 'LSH-2', 'LSH-1']","[0.8, 100, 99.9, 99.4, 91.9]",%,63.png,
64,BigBird,v4.2.3.3,transformer,유안,Big Bird: Transformers for Longer Sequences,2020,"Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed
",Google Research,"Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti, C., Ontanon, S., ... & Ahmed, A. (2020). Big bird: Transformers for longer sequences. Advances in neural information processing systems, 33, 17283-17297.","Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., ... & Fiedel, N. (2023). Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240), 1-113.","Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., ... & Liang, P. (2021). On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258.","BIGBIRD is a transformer model designed to handle longer sequences by using sparse attention patterns, which reduce memory and computation costs from quadratic to linear complexity. This approach maintains the expressivity of full self-attention, enabling improved performance on long-sequence NLP and genomics tasks.",Scales to sequences 8x longer than BERT.,Maintains Turing completeness and universal approximation properties.,"Achieves state-of-the-art in QA, summarization, and genomics.",Requires specialized sparse attention structure.,Performance advantage varies with task and sequence length.,Sparse attention models may need more layers for complex tasks.,2245,Arxiv ROUGE-1,ROUGE Scores,"['SumBasic', 'LexRank', 'LSA', 'Attn-Seq2Seq', 'Pntr-Gen-Seq2Seq', 'Long-Doc-Seq2Seq', 'Sent-CLF', 'Sent-PTR', 'Extr-Abst-TLM', 'Dancer']","[29.47, 33.85, 29.91, 29.30, 32.06, 35.80, 34.01, 42.32, 41.62, 42.70]",,64.png,
65,Switch Transformer,v4.2.4,transformer,유안,Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity,2021,"William Fedus, Barret Zoph, Noam Shazeer",Google Research,"Fedus, W., Zoph, B., & Shazeer, N. (2022). Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120), 1-39.","Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., ... & Lowe, R. (2022). Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35, 27730-27744.","Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., ... & Fiedel, N. (2023). Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240), 1-113.","The Switch Transformer is a sparse, scalable neural network model using Mixture of Experts (MoE) to route different parameters per input, significantly increasing model size without raising computational costs. This architecture improves training efficiency, enables multi-language learning, and achieves high performance even at large model scales.",Achieves up to 7x speedup in pre-training compared to dense models.,Scales efficiently up to a trillion parameters.,Maintains high performance with limited computational resources.,"Training can be unstable, especially with large models.",Significant communication costs in distributed setups.,Sparse model fine-tuning requires complex regularization strategies.,1804,GLUE,GLUE Score,"['T5-Base (d=0.1)', 'Switch-Base (d=0.1)', 'Switch-Base (d=0.2)', 'Switch-Base (d=0.3)', 'Switch-Base (d=0.1, ed=0.4)']","[82.9, 84.7, 84.4, 83.9, 85.2]",,65.png,
66,ALBERT,v4.2.5,transformer,유안,ALBERT: A Lite BERT for Self-supervised Learning of Language Representations,2019,"Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut",Google Research,"Lan, Z. (2019). Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942.","Bender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021, March). On the dangers of stochastic parrots: Can language models be too big?🦜. In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency (pp. 610-623).","Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., & Neubig, G. (2023). Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9), 1-35.","ALBERT is an optimized, lightweight variant of BERT for language understanding that reduces parameters through factorized embedding and cross-layer parameter sharing, maintaining performance while increasing efficiency. It also replaces BERT's NSP loss with a Sentence Order Prediction (SOP) task for coherence modeling.","Achieves high accuracy on GLUE, SQuAD, and RACE benchmarks.","Significantly fewer parameters than BERT, reducing memory usage.",Sentence Order Prediction (SOP) enhances performance in multi-sentence tasks.,Training is computationally intensive for larger configurations.,Limited by cross-layer sharing in capturing deep-layer interactions.,"Optimal hyperparameter tuning required for peak performance.





",7861,GLUE,GLUE Score,"['BERT-base', 'BERT-large', 'ALBERT-base', 'ALBERT-large', 'ALBERT-xlarge', 'ALBERT-xxlarge']","[84.5, 86.6, 81.6, 83.5, 86.4, 88.0]",,66.png,
67,DistilBERT,v4.2.6,transformer,유안,"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",2019,"Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf",Hugging Face,"Sanh, V. (2019). DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108.","Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., ... & Liang, P. (2021). On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258.","Bender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021, March). On the dangers of stochastic parrots: Can language models be too big?🦜. In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency (pp. 610-623).","DistilBERT is a smaller, faster, and more efficient version of BERT, created through knowledge distillation, where a student model learns from a larger ""teacher"" BERT model. It retains 97% of BERT's performance with 40% fewer parameters, making it suitable for low-compute environments like mobile devices.",40% smaller with 60% faster inference than BERT.,Retains 97% of BERT's language understanding capabilities.,Suitable for on-device and edge applications.,"Slightly lower accuracy than BERT in certain NLP tasks.
",Performance limited by reduced model complexity.,May need additional fine-tuning for specific tasks.,7456,GLUE,GLUE Score,"['ELMo', 'BERT-base', 'DistilBERT']","[44.1, 56.3, 51.3]",,67.png,
68,ERNIE,v4.3.1,transformer,유안,ERNIE: Enhanced Representation through Knowledge Integration,2019,"Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, Hua Wu",Baidu Inc.,"Sun, Y., Wang, S., Li, Y., Feng, S., Chen, X., Zhang, H., ... & Wu, H. (2019). Ernie: Enhanced representation through knowledge integration. arXiv preprint arXiv:1904.09223.","Bender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021, March). On the dangers of stochastic parrots: Can language models be too big?🦜. In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency (pp. 610-623).","Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., & Neubig, G. (2023). Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9), 1-35.","ERNIE is a language representation model that improves on BERT by incorporating knowledge-based masking strategies, including phrase and entity-level masking. This approach enables ERNIE to capture deeper semantic and syntactic information, enhancing performance on various Chinese NLP tasks.","Knowledge-enhanced masking strategies (phrase and entity levels) improve language understanding.
",Achieves state-of-the-art results on five Chinese NLP tasks.,Demonstrates improved knowledge inference through a novel dialogue language model (DLM).,"Designed specifically for Chinese, limiting generalization across languages.","Heavy reliance on large, heterogeneous datasets.",Limited testing on non-dialogue NLP applications.,1153,F1,Accuracy (%),"['BERT (dev)', 'BERT (test)', 'ERNIE (dev)', 'ERNIE (test)']","[78.1, 77.2, 79.9, 78.4]",%,68.png,
69,XLM,v4.3.2,transformer,유안,Cross-lingual Language Model Pretraining,2019,"Alexis CONNEAU, Guillaume Lample",Facebook AI Research,"Conneau, A., & Lample, G. (2019). Cross-lingual language model pretraining. Advances in neural information processing systems, 32.","Xue, L. (2020). mt5: A massively multilingual pre-trained text-to-text transformer. arXiv preprint arXiv:2010.11934.","Qiu, X., Sun, T., Xu, Y., Shao, Y., Dai, N., & Huang, X. (2020). Pre-trained models for natural language processing: A survey. Science China technological sciences, 63(10), 1872-1897.","This paper introduces cross-lingual language models (XLMs) that leverage both monolingual and parallel data for language understanding across multiple languages. The model is trained using Causal Language Modeling (CLM), Masked Language Modeling (MLM), and a Translation Language Modeling (TLM) objective to improve performance on cross-lingual tasks.",State-of-the-art accuracy on XNLI cross-lingual classification and machine translation benchmarks.,Effective in zero-shot cross-lingual transfer for various NLP tasks.,Publicly available code and pretrained models on GitHub.,Requires large computational resources for training.,"Performance decrease on distant language pairs without shared vocabulary.
",Limited effectiveness in very low-resource language settings.,1655,BLEU,BLEU Score,"['- -', 'EMB EMB', 'CLM CLM', 'MLM MLM', 'CLM -', 'MLM -', '- CLM', '- MLM', 'CLM MLM', 'MLM CLM']","[13.0, 29.4, 30.4, 33.4, 28.7, 31.6, 25.3, 29.2, 32.3, 33.4]",,69.png,
70,mT5,v4.3.3,transformer,유안,mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer,2021,"Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua,  Colin Raffel",Google Research,"Xue, L. (2020). mt5: A massively multilingual pre-trained text-to-text transformer. arXiv preprint arXiv:2010.11934.","Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., & Neubig, G. (2023). Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9), 1-35.","Bender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021, March). On the dangers of stochastic parrots: Can language models be too big?🦜. In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency (pp. 610-623).","mT5 is a massively multilingual variant of the T5 model, pre-trained on a Common Crawl-based dataset in 101 languages, achieving state-of-the-art multilingual performance with a unified text-to-text format.",Supports 101 languages with a unified text-to-text framework.,Achieves state-of-the-art results on multilingual benchmarks.,"Introduces a technique to prevent ""accidental translation.""",Struggles with low-resource languages.,Requires high computational resources for large-scale models.,"Still prone to some ""illegal"" predictions in zero-shot settings.





",2320,Accuracy,Accuracy (%),"['mBERT', 'XLM', 'InfoXLM', 'X-STILTs', 'XLM-R', 'VECO', 'RemBERT']","[65.4, 69.1, 81.4, 80.4, 79.2, 70.9, 80.8]",%,70.png,
71,Generative Adversarial Networks (GANs),v5.0,gan,유안,Generative Adversarial Nets,2014,"Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio",Departement d’informatique et de recherche op ´ erationnelle,"Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., ... & Bengio, Y. (2014). Generative adversarial nets. Advances in neural information processing systems, 27.","Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022). High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 10684-10695).","Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., & Aberman, K. (2023). Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 22500-22510).","GANs use two neural networks—a generator that creates samples and a discriminator that distinguishes between real and fake samples. The two networks compete, improving each other iteratively until the generator produces data indistinguishable from real samples.",Adversarial Framework: GANs train with a competitive process between generator and discriminator networks.,"Efficient Sample Generation: No need for Markov chains, allowing faster, direct sample generation.","Flexible Design: Works with differentiable functions, enhancing model versatility and potential applications.","Training Instability: Synchronization between networks is challenging, leading to potential convergence issues.","Mode Collapse: Generator may produce limited diversity, generating similar outputs.",No Direct Probability Estimate: GANs lack explicit probability estimates for generated samples.,74528,MNIST,MNIST,"['DBN', 'Stacked CAE', 'Deep GSN', 'Adversarial nets']","[138, 121, 214, 225]",,71.png,
72,DCGAN,v5.0.1.1,gan,유안,Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks,2015,"Alec Radford, Luke Metz, Soumith Chintala",indico Research,"Radford, A. (2015). Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434.","Khan, S., Naseer, M., Hayat, M., Zamir, S. W., Khan, F. S., & Shah, M. (2022). Transformers in vision: A survey. ACM computing surveys (CSUR), 54(10s), 1-41.","Saharia, C., Ho, J., Chan, W., Salimans, T., Fleet, D. J., & Norouzi, M. (2022). Image super-resolution via iterative refinement. IEEE transactions on pattern analysis and machine intelligence, 45(4), 4713-4726.","This paper introduces Deep Convolutional GANs (DCGANs), an architecture for unsupervised image representation learning and generation, demonstrating stability and feature extraction quality in unsupervised learning.",Introduced stable architectural constraints for GANs.,DCGANs perform well in unsupervised image feature extraction.,Demonstrated visualizations of learned filters activating on specific image features.,Some model instability during extended training.,Limited applications to smaller image resolutions.,Manual data preprocessing and architecture tuning required.,18893,Error,Error Rate (%),"['KNN', 'TSVM', 'M1+KNN', 'M1+TSVM', 'M1+M2', 'SWWAE without dropout', 'SWWAE with dropout', 'DCGAN (ours) + L2-SVM', 'Supervised CNN with same architecture']","[77.93, 66.55, 65.63, 54.33, 36.02, 27.83, 23.56, 22.48, 28.87]",%,72.png,
73,WGAN,v5.0.1.2,gan,유안,Wasserstein Generative Adversarial Networks,2017,"Martin Arjovsky, Soumith Chintala, Léon Bottou",,"Arjovsky, M., Chintala, S., & Bottou, L. (2017, July). Wasserstein generative adversarial networks. In International conference on machine learning (pp. 214-223). PMLR.","Song, J., Meng, C., & Ermon, S. (2020). Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502.","Wu, Z., Pan, S., Chen, F., Long, G., Zhang, C., & Philip, S. Y. (2020). A comprehensive survey on graph neural networks. IEEE transactions on neural networks and learning systems, 32(1), 4-24.","Wasserstein GANs (WGANs) improve Generative Adversarial Network (GAN) training stability by minimizing the Wasserstein (Earth Mover) distance instead of traditional divergence measures, addressing issues like mode collapse and providing more reliable loss metrics for tuning and debugging.
",Stable training: WGANs reduce mode collapse and improve stability in GAN training.,Meaningful loss metric: The Wasserstein distance provides interpretable learning curves.,Theoretical soundness: Robust foundation in probability and optimal transport theory.,Weight clipping: Limiting gradients restricts discriminator capacity.,Slow convergence: Training critic to optimality can be computationally intensive.,Sensitive to optimizer: Momentum-based optimizers may destabilize training.,17460,,,,,,73.png,
74,Progressive GAN,v5.0.1.3,gan,유안,"Progressive Growing of GANs for Improved Quality, Stability, and Variation",2018,"Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen",NVIDIA,"Karras, T. (2017). Progressive Growing of GANs for Improved Quality, Stability, and Variation. arXiv preprint arXiv:1710.10196.","Shorten, C., & Khoshgoftaar, T. M. (2019). A survey on image data augmentation for deep learning. Journal of big data, 6(1), 1-48.","Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022). High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 10684-10695).","The paper introduces a technique for progressively growing GANs by starting from low resolutions and adding layers, resulting in improved stability and high-quality, high-resolution images.",Progressive growth enhances image quality and stability.,Successfully generates 1024x1024 resolution images.,Achieves high variation and improved inception scores.,Increased training complexity for high resolutions.,"High computational resource requirements.
","Stability may vary across datasets and GAN loss types.




",8838,MS-SSIM,MS-SSIM,"['Gulrajani et al. (2017)', '+ Progressive growing', '+ Small minibatch', '+ Revised training parameters', '+ Minibatch discrimination', 'Minibatch stddev', '+ Equalized learning rate', '+ Pixelwise normalization', 'Converged']","[0.0587, 0.0615, 0.1061, 0.0662, 0.0648, 0.0671, 0.0668, 0.0640, 0.0636]",,74.png,
75,Conditional GAN,v5.0.2.1,gan,유안,Conditional Generative Adversarial Nets,2014,"Mehdi Mirza, Simon Osindero",Departement d’informatique et de recherche op ´ erationnelle ,"Mirza, M. (2014). Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784.","Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022). High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 10684-10695).","Dhariwal, P., & Nichol, A. (2021). Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34, 8780-8794.","Conditional Generative Adversarial Networks (CGANs) extend traditional GANs by conditioning both the generator and discriminator on auxiliary information, allowing for targeted generation, such as producing MNIST digits labeled by class or multi-modal image tags.","Targeted Generation: CGANs generate samples conditioned on specified data, like class labels or image features.
","Flexible Training: The model adapts auxiliary inputs for different applications, including image tagging and multimodal learning.","Improved Accuracy: CGANs outperform standard GANs in specific tasks by utilizing extra conditioning information.
",Complexity in Hyperparameters: Effective training requires careful tuning of hyperparameters and architectures.,Limited Efficacy in Large Datasets: CGANs may struggle to scale efficiently to larger datasets without optimization.,"Potential for Mode Collapse: Like GANs, CGANs can suffer from mode collapse, reducing diversity in generated outputs.",14155,MNIST,MNIST,"['DBN', 'Stacked CAE', 'Deep GSN', 'Adversarial nets', 'Conditional adversarial nets']","[138, 121, 214, 225, 132]",,75.png,
76,CycleGAN,v5.0.2.2,gan,유안,Unpaired Image-To-Image Translation Using Cycle-Consistent Adversarial Networks,2017,"Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A. Efros","Berkeley AI Research (BAIR) laboratory, UC Berkeley","Zhu, J. Y., Park, T., Isola, P., & Efros, A. A. (2017). Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proceedings of the IEEE international conference on computer vision (pp. 2223-2232).","Croitoru, F. A., Hondru, V., Ionescu, R. T., & Shah, M. (2023). Diffusion models in vision: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(9), 10850-10869.","Zhang, L., Rao, A., & Agrawala, M. (2023). Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 3836-3847).","The CycleGAN model enables image-to-image translation between two domains without paired examples, using cycle-consistency and adversarial losses to learn mappings that preserve the characteristics of each domain while translating images.",Cycle-consistency loss: Ensures translation back to original form after round-trip transformation.,Unpaired data usage: Trains without needing paired images.,"Diverse applications: Applicable to style transfer, season change, and more.",Geometry limitations: Struggles with transformations requiring structural changes.,"Mode collapse risk: Can map different inputs to identical outputs.
",Semantic ambiguity: May confuse similar categories without weak supervision.,24951,Class IOU,Class IOU,"['CoGAN', 'BiGAN/ALI', 'Pixel loss + GAN', 'Feature loss + GAN', 'CycleGAN', 'pix2pix']
","[0.08, 0.07, 0.07, 0.06, 0.16, 0.32]",,76.png,
77,Pix2Pix,v5.0.2.3,gan,유안,Image-To-Image Translation With Conditional Adversarial Networks,2017,"Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros","Berkeley AI Research (BAIR) Laboratory, UC Berkeley","Isola, P., Zhu, J. Y., Zhou, T., & Efros, A. A. (2017). Image-to-image translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1125-1134).","Zhang, L., Rao, A., & Agrawala, M. (2023). Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 3836-3847).","Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022). High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 10684-10695).","The model is a conditional Generative Adversarial Network (cGAN) used for translating input images into corresponding output images across various applications (e.g., black-and-white to color, labels to photos) by learning mappings and suitable loss functions, yielding realistic and detailed images.",General-purpose framework for diverse image-to-image tasks,Learns both mappings and loss functions automatically,"Generates sharp, realistic images without hand-engineered losses",Limited stochasticity in output variations,Struggles with precise geometric transformations,High computational requirements,24742,Class IOU,Class IOU,"['L1', 'GAN', 'cGAN', 'L1+GAN', 'L1+cGAN', 'Ground truth']","[0.11, 0.01, 0.16, 0.15, 0.17, 0.21]",,77.png,
78,StyleGAN,v5.0.3.1,gan,유안,A Style-Based Generator Architecture for Generative Adversarial Networks,2019,"Tero Karras, Samuli Laine, Timo Aila",NVIDIA,"Karras, T., Laine, S., & Aila, T. (2019). A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 4401-4410).","Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022). High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 10684-10695).","Zhang, L., Rao, A., & Agrawala, M. (2023). Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 3836-3847).","This paper introduces a new generator architecture for GANs that enhances control over image synthesis by separating high-level attributes from stochastic details. By modifying the generator structure, it enables better disentanglement in the latent space, resulting in improved image quality, interpolation, and attribute control.",Enables scale-specific control of high-level attributes and stochastic details.,"Improves interpolation and disentanglement of latent factors.
","Introduces FFHQ, a new, varied dataset of human faces.",Dependence on high-quality dataset like FFHQ for optimal performance.,Increased complexity due to additional intermediate layers and noise inputs.,"Some disentanglement metrics require additional classifiers, increasing computational cost.",12400,FID,FID,"['Traditional 0 Z', 'Traditional 8 Z', 'Traditional 8 W', 'Style-based 0 Z', 'Style-based 1 W', 'Style-based 2 W', 'Style-based 8 W']","[5.25, 4.87, 4.87, 5.06, 4.60, 4.43, 4.40]",,78.png,
79,StyleGAN2,v5.0.3.2,gan,유안,Analyzing and Improving the Image Quality of StyleGAN,2020,"Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, Timo Aila",NVIDIA,"Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J., & Aila, T. (2020). Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 8110-8119).","Khan, S., Naseer, M., Hayat, M., Zamir, S. W., Khan, F. S., & Shah, M. (2022). Transformers in vision: A survey. ACM computing surveys (CSUR), 54(10s), 1-41.","Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022). High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 10684-10695).","The improved StyleGAN model, StyleGAN2, enhances image quality by addressing artifacts in the original architecture through refined normalization, training techniques, and network adjustments.",Enhanced image quality through path length regularization.,New architecture without progressive growing removes artifacts.,Easier inversion for source attribution.,High computational resource demands.,Limited to data-rich environments.,Metrics may overlook shape-based image quality improvements.,6790,FID,FID,"['Baseline StyleGAN', '+ Weight demodulation', '+ Lazy regularization', '+ Path length regularization', '+ No growing, new G & D arch.', '+ Large networks (StyleGAN2)', 'Config A with large networks']"," [4.40, 4.39, 4.38, 4.34, 3.31, 2.84, 3.98]",,79.png,
80,StyleGAN3,v5.0.3.3,gan,유안,Alias-Free Generative Adversarial Networks,2021,"Tero Karras, Miika Aittala, Samuli Laine, Erik Härkönen, Janne Hellsten, Jaakko Lehtinen, Timo Aila",NVIDIA,"Karras, T., Aittala, M., Laine, S., Härkönen, E., Hellsten, J., Lehtinen, J., & Aila, T. (2021). Alias-free generative adversarial networks. Advances in neural information processing systems, 34, 852-863.","Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., & Aberman, K. (2023). Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 22500-22510).","Karras, T., Aittala, M., Aila, T., & Laine, S. (2022). Elucidating the design space of diffusion-based generative models. Advances in neural information processing systems, 35, 26565-26577.","Alias-Free Generative Adversarial Networks (StyleGAN3) eliminate spatial aliasing in image generation by enforcing continuous translation and rotation equivariance, producing more natural transformations and avoiding ""texture sticking"" issues present in previous GAN architectures.
","Equivariant to translation and rotation even at sub-pixel scales.
","Solves ""texture sticking,"" creating smoother, more coherent image transformations.",Matches StyleGAN2 in FID while improving animation and video generation suitability.,Increased computational costs and training times.,Sensitive to aliasing in the training data.,"Improvements only applied to the generator, not the discriminator.",1640,FID,FID,"['StyleGAN2', '+ Fourier features', '+ No noise inputs', '+ Simplified generator', '+ Boundaries & upsampling', '+ Filtered nonlinearities', '+ Non-critical sampling', '+ Transformed Fourier features', '+ Flexible layers (StyleGAN3-T)', '+ Rotation equiv. (StyleGAN3-R)']"," [5.14, 4.79, 4.54, 5.21, 6.02, 6.35, 4.78, 4.64, 4.62, 4.50]",,80.png,
81,DDPM,v5.1.1,diffusion,유안,Denoising Diffusion Probabilistic Models,2020,"Jonathan Ho, Ajay Jain, Pieter Abbeel",UC Berkeley,"Ho, J., Jain, A., & Abbeel, P. (2020). Denoising diffusion probabilistic models. Advances in neural information processing systems, 33, 6840-6851.","Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., & Aberman, K. (2023). Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 22500-22510).","Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E. L., ... & Norouzi, M. (2022). Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35, 36479-36494.","Denoising Diffusion Probabilistic Models (DDPMs) are generative models that create high-quality images by learning to reverse a noisy diffusion process. This approach combines score matching with Langevin dynamics, training a neural network to iteratively denoise data, yielding realistic samples.",High-quality samples: Achieves competitive Inception and FID scores on CIFAR-10 and LSUN datasets.,Connection to score matching: Links diffusion models with denoising score matching and Langevin dynamics.,Progressive generation: Supports lossy image decompression with fine-to-coarse details during sampling.,Lower likelihood performance: Does not achieve competitive log likelihoods with other likelihood-based models.,Training instability: Sensitive to parameter choices and variance schedule settings.,"High computation cost: Requires extensive neural network evaluations, impacting sampling speed.",14543,FID,FID,"[‘EBM’, ‘JEM’, ‘BigGAN’, ‘StyleGAN2 + ADA (v1)’]","[37.9, 38.4, 14.73, 2.67]",,81.png,
82,Improved DDPM,v5.1.2,diffusion,유안,Improved Denoising Diffusion Probabilistic Models,2021,"Alexander Quinn Nichol, Prafulla Dhariwal",,"Nichol, A. Q., & Dhariwal, P. (2021, July). Improved denoising diffusion probabilistic models. In International conference on machine learning (pp. 8162-8171). PMLR.","Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E. L., ... & Norouzi, M. (2022). Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35, 36479-36494.","Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., ... & Chen, M. (2021). Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741.","Improved Denoising Diffusion Probabilistic Models (DDPMs) are generative models that reverse a multi-step noising process. This improved DDPM variant learns the reverse process more efficiently, achieving high-quality samples with faster sampling and competitive log-likelihoods, making it suitable for scalable, practical applications.
","Achieves competitive log-likelihoods with high sample quality.
",Allows fast sampling with fewer forward passes.,"Improved mode coverage over GANs, enhancing distribution coverage.",Requires large compute resources for training.,Slower sampling compared to GANs.,High gradient noise during optimization affects training stability.,3090,FID,FID,"['BigGAN-deep', 'Improved Diffusion (small)', 'Improved Diffusion (large)']","[4.06, 6.92, 2.92]",,82.png,
83,Stable Diffusion,v5.1.3.1,diffusion,유안,High-Resolution Image Synthesis With Latent Diffusion Models,2022,"Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer","Ludwig Maximilian University of Munich & IWR, Heidelberg University, Germany","Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022). High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 10684-10695).","Liu, H., Li, C., Wu, Q., & Lee, Y. J. (2024). Visual instruction tuning. Advances in neural information processing systems, 36.","Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., ... & Jitsev, J. (2022). Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35, 25278-25294.","Latent Diffusion Models (LDMs) generate high-resolution images by training diffusion models in a compressed latent space, significantly reducing computational resources without sacrificing quality. This allows for efficient and versatile applications, including super-resolution, inpainting, and text-to-image synthesis.",Reduced computational demands with latent-space training.,High-quality synthesis through minimal downsampling.,Supports diverse conditioning inputs like text and layout.,Requires a pretrained autoencoder.,Still computationally demanding for very high resolutions.,Limited generalization to unconventional conditioning setups.,12514,FID,FID,"['DALL-E', 'CogView', 'Lafite', 'LDM-KL-8', 'LDM-KL-8-G']","[27.50, 27.10, 26.94, 23.35, 12.61]",,83.png,
84,Stable Diffusion XL,v5.1.4.1,diffusion,유안,SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis,2023,"Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, Robin Rombach","Stability AI, Applied Research","Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Müller, J., ... & Rombach, R. (2023). Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952.","Yang, Z., Li, L., Lin, K., Wang, J., Lin, C. C., Liu, Z., & Wang, L. (2023). The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint arXiv:2309.17421, 9(1), 1.","Yang, L., Zhang, Z., Song, Y., Hong, S., Xu, R., Zhao, Y., ... & Yang, M. H. (2023). Diffusion models: A comprehensive survey of methods and applications. ACM Computing Surveys, 56(4), 1-39.","SDXL is an improved text-to-image latent diffusion model using a larger UNet backbone, two text encoders, and novel conditioning techniques to generate high-quality, high-resolution images with better detail and composition.",Larger UNet model with 2.6B parameters enhances image synthesis quality.,Dual text encoders improve prompt understanding and adherence.,Refinement model enhances image detail and fidelity post-generation.,High VRAM and compute requirements for two-stage generation.,"Struggles with rendering fine details, like hands and intricate text.",May exhibit concept blending in complex scenes.,1132,FID-5k,FID-5k," ['CIN-512-only', 'CIN-nocond', 'CIN-size-cond']","[43.84, 39.76, 36.53]",,84.png,
85,SDXL Turbo,v5.1.4.2,diffusion,유안,Adversarial Diffusion Distillation,2023,"Axel Sauer, Dominik Lorenz, Andreas Blattmann, Robin Rombach
",Stability AI,"Sauer, A., Lorenz, D., Blattmann, A., & Rombach, R. (2025). Adversarial diffusion distillation. In European Conference on Computer Vision (pp. 87-103). Springer, Cham.","Liu, Y., Zhang, K., Li, Y., Yan, Z., Gao, C., Chen, R., ... & Sun, L. (2024). Sora: A review on background, technology, limitations, and opportunities of large vision models. arXiv preprint arXiv:2402.17177.","Wang, J., Yue, Z., Zhou, S., Chan, K. C., & Loy, C. C. (2024). Exploiting diffusion prior for real-world image super-resolution. International Journal of Computer Vision, 1-21.","Adversarial Diffusion Distillation (ADD) reduces high-fidelity image diffusion model sampling steps to just 1–4 steps by combining adversarial and distillation losses, enabling real-time, high-quality generation.","Achieves single-step, high-quality image generation.",Outperforms comparable few-step models in image fidelity.,Retains quality across iterative sampling steps.,Limited diversity in single-step samples.,Pretrained model initialization required.,"Reduced effectiveness with certain teacher models (e.g., SDXL).",196,FID,FID,"['DPM Solver', 'Progressive Distillation', 'CFG-Aware Distillation', 'InstaFlow-0.9B', 'InstaFlow-1.7B', 'UFOGen', 'ADD-M']","[20.1, 37.2, 24.2, 23.4, 22.4, 22.5, 19.7]",,85.png,
86,IP-Adapter,v5.1.4.3,diffusion,유안,IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models,2023,"Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, Wei Yang",Tencent AI Lab,"Ye, H., Zhang, J., Liu, S., Han, X., & Yang, W. (2023). Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721.","Chen, X., Huang, L., Liu, Y., Shen, Y., Zhao, D., & Zhao, H. (2024). Anydoor: Zero-shot object-level image customization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 6593-6602).","Han, Z., Gao, C., Liu, J., Zhang, J., & Zhang, S. Q. (2024). Parameter-efficient fine-tuning for large models: A comprehensive survey. arXiv preprint arXiv:2403.14608.","The IP-Adapter enables image prompts in text-to-image diffusion models by adding separate cross-attention layers for text and image features. It maintains high generation quality with minimal parameters, allowing multimodal image generation using both text and image prompts without modifying the original model.","Lightweight design with only 22M parameters, comparable to fully fine-tuned models.","Compatible with text prompts, enabling multimodal generation.",Works with existing structural control tools like ControlNet.,Limited ability to capture highly specific subject features.,Only approximate faithfulness to reference images.,Multimodal generation lacks full customization consistency.,337,CLIP-T,CLIP-T,"['Open unCLIP', 'Kandinsky-2-1', 'Versatile Diffusion', 'SD Image Variations', 'SD unCLIP', 'Uni-ControlNet (Global Control)', 'T2I-Adapter (Style)', 'ControlNet Shuffle', 'IP-Adapter']","[0.608, 0.599, 0.587, 0.548, 0.584, 0.506, 0.485, 0.421, 0.588]",,86.png,
87,Stable Video Diffusion,v5.1.5.1,diffusion,유안,Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets,2023,"Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, Robin Rombach",Stability AI,"Blattmann, A., Dockhorn, T., Kulal, S., Mendelevitch, D., Kilian, M., Lorenz, D., ... & Rombach, R. (2023). Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127.","Li, C., Gan, Z., Yang, Z., Yang, J., Li, L., Wang, L., & Gao, J. (2024). Multimodal foundation models: From specialists to general-purpose assistants. Foundations and Trends® in Computer Graphics and Vision, 16(1-2), 1-214.","Liu, Y., Zhang, K., Li, Y., Yan, Z., Gao, C., Chen, R., ... & Sun, L. (2024). Sora: A review on background, technology, limitations, and opportunities of large vision models. arXiv preprint arXiv:2402.17177.","Stable Video Diffusion is a high-resolution video generation model that finetunes pre-trained image diffusion models with a three-stage video-specific training strategy, including data curation.",Systematic data curation for high-quality video training.,Strong baseline model for diverse video generation tasks.,Effective multi-view and motion consistency in video synthesis.,Limited long-form video generation capabilities.,High computational and memory demands.,Motion sometimes lacks adequate dynamism.,432,CLIP-S,CLIP-S,"['SyncDreamer', 'Zero123', 'Zero123XL', 'Scratch-MV', 'SD2.1-MV', 'SVD-MV']","[0.88, 0.87, 0.87, 0.76, 0.83, 0.89]",,87.png,
88,Sora,v5.1.5.2,diffusion,유안,Video generation models as world simulators,2024,"Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, Aditya Ramesh",Open AI,"Brooks, T., Peebles, B., Holmes, C., DePue, W., Guo, Y., Jing, L., ... & Ramesh, A. (2024). Video generation models as world simulators. 2024-03-03]. https://openai. com/research/video-generation-modelsas-world-simulators.","Han, Z., Gao, C., Liu, J., Zhang, J., & Zhang, S. Q. (2024). Parameter-efficient fine-tuning for large models: A comprehensive survey. arXiv preprint arXiv:2403.14608.","Firoozi, R., Tucker, J., Tian, S., Majumdar, A., Sun, J., Liu, W., ... & Schwager, M. (2023). Foundation models in robotics: Applications, challenges, and the future. The International Journal of Robotics Research, 02783649241281508.","Sora is a video-generation model using diffusion transformers trained on variable-duration, resolution, and aspect ratio videos and images. It generates high-quality, text-conditional videos by segmenting data into spacetime patches, allowing scalable and flexible video generation across diverse visual formats.",Generates videos up to one minute in HD quality.,Adapts to diverse resolutions and aspect ratios.,Maintains long-range consistency in object and environment rendering.,Struggles with complex physical interactions like glass shattering.,Object coherence issues in long videos.,Occasional unintended object appearances.,134,,,,,,88.png,
89,Lumiere,v5.1.5.3,diffusion,유안,Lumiere: A Space-Time Diffusion Model for Video Generation,2024,"Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Guanghui Liu, Amit Raj, Yuanzhen Li, Michael Rubinstein, Tomer Michaeli, Oliver Wang, Deqing Sun, Tali Dekel, Inbar Mosseri",Google Research,"Bar-Tal, O., Chefer, H., Tov, O., Herrmann, C., Paiss, R., Zada, S., ... & Mosseri, I. (2024). Lumiere: A space-time diffusion model for video generation. arXiv preprint arXiv:2401.12945.","Liu, Y., Zhang, K., Li, Y., Yan, Z., Gao, C., Chen, R., ... & Sun, L. (2024). Sora: A review on background, technology, limitations, and opportunities of large vision models. arXiv preprint arXiv:2402.17177.","Zhang, D. J., Wu, J. Z., Liu, J. W., Zhao, R., Ran, L., Gu, Y., ... & Shou, M. Z. (2024). Show-1: Marrying pixel and latent diffusion models for text-to-video generation. International Journal of Computer Vision, 1-15.","Lumiere is a text-to-video diffusion model that uses a Space-Time U-Net architecture to generate coherent, realistic motion in videos. Unlike traditional models, it processes an entire video sequence at once, ensuring temporal consistency without relying on cascaded temporal super-resolution.
",Generates coherent motion by processing entire video duration at once.,"Facilitates varied applications: image-to-video, inpainting, stylized generation.",Uses efficient space-time downsampling to reduce computational demands.,"Limited to single-shot, continuous scene videos.",Challenges with high-resolution generation due to memory constraints.,Not optimized for scene transitions or multi-shot videos.,118,FVD,FVD,"['MagicVideo', 'Emu Video', 'Video LDM', 'Show-1', 'Make-A-Video', 'PYoCo', 'SVD', 'Lumiere']","[655.00, 606.20, 550.61, 394.46, 367.23, 355.19, 242.02, 332.49]",,89.png,
90,Gen-2,v5.1.5.4,diffusion,유안,Structure and Content-Guided Video Synthesis with Diffusion Models,2023,"Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, Anastasis Germanidis",Runway,"Esser, P., Chiu, J., Atighehchian, P., Granskog, J., & Germanidis, A. (2023). Structure and content-guided video synthesis with diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 7346-7356).","Blattmann, A., Rombach, R., Ling, H., Dockhorn, T., Kim, S. W., Fidler, S., & Kreis, K. (2023). Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 22563-22575).","Liu, R., Wu, R., Van Hoorick, B., Tokmakov, P., Zakharov, S., & Vondrick, C. (2023). Zero-1-to-3: Zero-shot one image to 3d object. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 9298-9309).","The paper presents a diffusion-based video synthesis model that edits video content guided by text or images while preserving the video’s structure. It introduces depth-based structure control and temporal consistency, allowing customized edits without per-video training.",Combines structure control with depth maps and content guidance through text or images.,Enables fine-grained video edits with high temporal consistency.,"Supports diverse applications like style transfer, character replacement, and environment changes.",Requires substantial computational resources for training.,Limited by dataset quality and availability of diverse video-text pairs.,May struggle with intricate structural changes in dynamic scenes.,400,,,,,,90.png,
91,Point-E,v5.1.6.1,diffusion,유안,Point-E: A System for Generating 3D Point Clouds from Complex Prompts,2022,"Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, Mark Chen",Open AI,"Nichol, A., Jun, H., Dhariwal, P., Mishkin, P., & Chen, M. (2022). Point-e: A system for generating 3d point clouds from complex prompts. arXiv preprint arXiv:2212.08751.","Liu, R., Wu, R., Van Hoorick, B., Tokmakov, P., Zakharov, S., & Vondrick, C. (2023). Zero-1-to-3: Zero-shot one image to 3d object. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 9298-9309).","Jun, H., & Nichol, A. (2023). Shap-e: Generating conditional 3d implicit functions. arXiv preprint arXiv:2305.02463.","Point·E is a fast, efficient system for generating 3D point clouds from text prompts by combining text-to-image and image-to-3D diffusion models.",Generates 3D point clouds in under 2 minutes.,Uses efficient two-step text-to-3D generation.,Requires only a single GPU.,Lower fidelity than state-of-the-art methods.,Requires synthetic image generation.,Limited to low-resolution point clouds.,437,ViT-B/32,ViT-B/32 (%),"['DreamFields', 'CLIP-Mesh', 'DreamFusion', 'Point-E (40M, text-only)', 'Point-E (40M)', 'Point-E (300M)', 'Point-E (1B)']","[78.6, 67.8, 75.1, 15.4, 36.5, 40.3, 41.1]",%,91.png,
92,GET3D,v5.1.6.2,diffusion,유안,GET3D: A Generative Model of High Quality 3D Textured Shapes Learned from Images,2023,"Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen, Kangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, Sanja Fidler",NVIDIA,"Gao, J., Shen, T., Wang, Z., Chen, W., Yin, K., Li, D., ... & Fidler, S. (2022). Get3d: A generative model of high quality 3d textured shapes learned from images. Advances In Neural Information Processing Systems, 35, 31841-31854.","Lin, C. H., Gao, J., Tang, L., Takikawa, T., Zeng, X., Huang, X., ... & Lin, T. Y. (2023). Magic3d: High-resolution text-to-3d content creation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 300-309).","Liu, R., Wu, R., Van Hoorick, B., Tokmakov, P., Zakharov, S., & Vondrick, C. (2023). Zero-1-to-3: Zero-shot one image to 3d object. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 9298-9309).","GET3D is a generative model that synthesizes high-quality, textured 3D meshes from 2D image collections, meeting standards for diverse applications. It uses differentiable surface and texture modeling to produce geometrically detailed, textured, and arbitrarily topological meshes usable in common 3D software.",Produces textured 3D meshes directly usable in 3D software.,"Trains on 2D images, creating detailed geometry and textures.","Supports complex, varied mesh topologies and lighting effects.",Requires 2D silhouettes and known camera angles for training.,"Currently trained per-category, limiting multi-category application.","Evaluated on synthetic data only, not real-world data.",407,FID,FID,"[Car 128², Car 512², Car 1024², Chair 128², Chair 512², Chair 1024², Mbike 512², Mbike 1024², Animal 512², Animal 1024²]","[39.21, 13.19, 10.25, 43.04, 30.16, 23.28, 74.04, 65.60, 29.75, 28.33]",,92.png,
93,Show and Tell,v6.0.1,multimodal,유안,Show and Tell: A Neural Image Caption Generator,2015,"Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan",Google,"Vinyals, O., Toshev, A., Bengio, S., & Erhan, D. (2015). Show and tell: A neural image caption generator. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 3156-3164).","Khan, S., Naseer, M., Hayat, M., Zamir, S. W., Khan, F. S., & Shah, M. (2022). Transformers in vision: A survey. ACM computing surveys (CSUR), 54(10s), 1-41.","Alayrac, J. B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., ... & Simonyan, K. (2022). Flamingo: a visual language model for few-shot learning. Advances in neural information processing systems, 35, 23716-23736.","The ""Show and Tell"" model generates natural language descriptions for images by combining a Convolutional Neural Network (CNN) to process images with a Recurrent Neural Network (RNN) to produce sentences, trained end-to-end to maximize the probability of accurate image captions.",Integrates image processing and language generation seamlessly.,Achieves state-of-the-art performance on multiple benchmarks.,"Trained end-to-end, eliminating the need for manual feature engineering.",Relies on large labeled datasets.,Struggles with unseen object compositions.,Limited diversity in generated captions.,7845,BLEU-4,BLEU-4,"['NIC', 'Random', 'Nearest Neighbor', 'Human']","[27.7, 4.6, 9.9, 21.7]",,93.png,
94,Visual Question Answering (VQA),v6.0.2,multimodal,세준,VQA: Visual Question Answering,2015,"Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu1, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, Devi Parikh","Virginia Tech, Microsoft Research","Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C. L., & Parikh, D. (2015). Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision (pp. 2425-2433).","Kafle, K., & Kanan, C. (2017). An analysis of visual question answering algorithms. In Proceedings of the IEEE international conference on computer vision (pp. 1965-1973).","Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., & Parikh, D. (2017). Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 6904-6913).","VQA (Visual Question Answering) is a multi-modal AI task where a model answers open-ended questions about images. It combines visual understanding, natural language processing, and commonsense reasoning to generate natural language answers for diverse, real-world scenarios.",Combines vision and language understanding.,Requires reasoning beyond visual data.,"Handles open-ended, free-form questions.",Challenging dataset requiring multi-modal learning.,Sensitive to language biases in questions.,Requires extensive labeled training data.,6446,Model accuracy,Answer correctness,"['VQA', 'Human Performance']","[54.06, 83.30]",%,94.png,
95,CLIP,v6.1.1,multimodal,세준,Learning Transferable Visual Models From Natural Language Supervision,2021,"Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever",OpenAI,"Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., ... & Sutskever, I. (2021, July). Learning transferable visual models from natural language supervision. In International conference on machine learning (pp. 8748-8763). PMLR.","Shen, S., Li, C., Hu, X., Xie, Y., Yang, J., Zhang, P., ... & Gao, J. (2022). K-lite: Learning transferable visual models with external knowledge. Advances in Neural Information Processing Systems, 35, 15558-15573.","Li, C., Liu, H., Li, L., Zhang, P., Aneja, J., Yang, J., ... & Gao, J. (2022). Elevater: A benchmark and toolkit for evaluating language-augmented visual models. Advances in Neural Information Processing Systems, 35, 9287-9301.","CLIP (Contrastive Language–Image Pre-training) is a zero-shot learning model that jointly trains text and image encoders using contrastive learning on large-scale datasets. It maps visual and textual data into a shared embedding space, enabling effective zero-shot transfer across diverse vision tasks.",Learns transferable representations.,Effective zero-shot learning on diverse datasets.,Reduces reliance on task-specific training data.,Requires massive computational resources for training.,Struggles with highly specialized tasks.,Prone to biases in text-image pairs.,24571,Zero-shot accuracy,Accuracy(%),"['CLIP', 'Baseline']","[76.2, 11.5]",%,95.png,
96,Florence,v6.1.2,multimodal,세준,Florence: A New Foundation Model for Computer Vision,2022,"Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, Ce Liu, Mengchen Liu, Zicheng Liu, Yumao Lu, Yu Shi, Lijuan Wang, Jianfeng Wang, Bin Xiao, Zhen Xiao, Jianwei Yang, Michael Zeng, Luowei Zhou, Pengchuan Zhang",Microsoft,"Yuan, L., Chen, D., Chen, Y. L., Codella, N., Dai, X., Gao, J., ... & Zhang, P. (2021). Florence: A new foundation model for computer vision. arXiv preprint arXiv:2111.11432.","Dong, X., Bao, J., Zhang, T., Chen, D., Gu, S., Zhang, W., ... & Yu, N. (2022). Clip itself is a strong fine-tuner: Achieving 85.7% and 88.0% top-1 accuracy with vit-b and vit-l on imagenet. arXiv preprint arXiv:2212.06138.","Schonfeld, E., Ebrahimi, S., Sinha, S., Darrell, T., & Akata, Z. (2019). Generalized zero-and few-shot learning via aligned variational autoencoders. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 8247-8255).","Florence is a foundation model for computer vision trained on large-scale image-text data. It expands visual representations from coarse to fine-grained, static to dynamic, and RGB to multi-modal tasks, achieving state-of-the-art performance across classification, detection, retrieval, VQA, and video understanding.",Expands vision representation to multiple modalities.,Achieves SOTA results on 44 benchmarks.,Adapts easily for various downstream tasks.,Requires massive computational and data resources.,Prone to biases in web-crawled data.,"Limited applicability to novel, unseen domains.",873,Top-1,Accuracy(%),"['Florence', 'CLIP']","[83.74, 76.2]",%,96.png,
97,DALL-E,v6.2.1,multimodal,세준,Zero-Shot Text-to-Image Generation,2021,"Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever",OpenAI,"Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., ... & Sutskever, I. (2021, July). Zero-shot text-to-image generation. In International conference on machine learning (pp. 8821-8831). Pmlr.","Zhou, Y., Zhang, R., Chen, C., Li, C., Tensmeyer, C., Yu, T., ... & Sun, T. (2022). Towards language-free training for text-to-image generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 17907-17917).","Li, A. C., Prabhudesai, M., Duggal, S., Brown, E., & Pathak, D. (2023). Your diffusion model is secretly a zero-shot classifier. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 2206-2217).","DALL-E is a text-to-image generation model based on a 12-billion parameter autoregressive transformer. It generates high-fidelity images from textual descriptions by modeling text and image tokens as a single stream of data, enabling a range of creative applications.",Excels in zero-shot text-to-image generation.,Handles complex compositions and abstractions.,Performs rudimentary image-to-image translations.,Requires massive computational resources.,Struggles with domain-specific accuracy.,Limited to existing language-image pairs.,4966,Human evaluation,User-rated preference on Realism,"['DALL-E', 'Baseline']","[10.0, 90.0]",%,97.png,
98,DALL-E 2,v6.2.2,multimodal,세준,Hierarchical Text-Conditional Image Generation with CLIP Latents,2022,"Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever",OpenAI,"Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., & Chen, M. (2022). Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2), 3.","Dai, X., Hou, J., Ma, C. Y., Tsai, S., Wang, J., Wang, R., ... & Parikh, D. (2023). Emu: Enhancing image generation models using photogenic needles in a haystack. arXiv preprint arXiv:2309.15807.","Mansimov, E., Parisotto, E., Ba, J. L., & Salakhutdinov, R. (2015). Generating images from captions with attention. arXiv preprint arXiv:1511.02793.","DALL-E 2 leverages CLIP embeddings to enhance text-to-image generation, using a two-stage process of generating CLIP image embeddings followed by diffusion-based image decoding. This results in improved photorealism, diversity, and language-guided image manipulation.",Improves upon DALL-E with CLIP-based embeddings.,Generates high-quality photorealistic images.,Enables text-guided image editing and variations.,Struggles with fine-grained object-attribute binding.,Requires massive training datasets and resources.,Limited robustness in complex scenes.,6144,FID,Zero-shot image fidelity,"['DALL-E 2', 'Baseline']","[10.39, 28.00]",FID Score,98.png,
99,DALL-E 3,v6.2.3,multimodal,세준,DALL-E 3 System Card,2023,"James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu, Yunxin Jiao, Aditya Ramesh",OpenAI,"Betker, J., Goh, G., Jing, L., Brooks, T., Wang, J., Li, L., ... & Ramesh, A. (2023). Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3), 8.","Hossain, M. Z., Sohel, F., Shiratuddin, M. F., Laga, H., & Bennamoun, M. (2021). Text to image synthesis for improved image captioning. IEEE Access, 9, 64918-64928.","Bai, S., & An, S. (2018). A survey on automatic image caption generation. Neurocomputing, 311, 291-304.","DALL-E 3 improves text-to-image generation by leveraging highly descriptive synthetic captions for training. It uses GPT-4-based caption enhancement and achieves state-of-the-art results on prompt following, coherence, and style evaluations.",Enhances text-to-image prompt following.,Achieves SOTA coherence and aesthetic quality.,Improves handling of complex compositions.,Struggles with spatial awareness in captions.,Text rendering remains unreliable.,Prone to hallucinating fine-grained details.,592,CLIP Score,Text-image similarity,"['DALL-E 3', 'DALL-E 2', 'Stable Diffusion XL']","[32.0, 31.4, 30.5]",Score,99.png,
100,FLAMINGO,v6.3.1,multimodal,세준,Flamingo: a Visual Language Model for Few-Shot Learning,2022,"Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, Karen Simonyan",DeepMind,"Alayrac, J. B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., ... & Simonyan, K. (2022). Flamingo: a visual language model for few-shot learning. Advances in neural information processing systems, 35, 23716-23736.","Najdenkoska, I., Zhen, X., & Worring, M. (2023). Meta learning to bridge vision and language models for multimodal few-shot learning. arXiv preprint arXiv:2302.14794.","Mahabadi, R. K., Zettlemoyer, L., Henderson, J., Saeidi, M., Mathias, L., Stoyanov, V., & Yazdani, M. (2022). Perfect: Prompt-free and efficient few-shot learning with language models. arXiv preprint arXiv:2204.01172.","Flamingo is a visual language model designed for few-shot learning. It interleaves visual and textual data, leveraging pretrained vision and language encoders. Flamingo excels at adapting to image, video, and text understanding tasks without fine-tuning, achieving strong results in tasks like visual question answering and captioning.",Enables few-shot learning for visual tasks.,Combines visual and language inputs seamlessly.,Outperforms SOTA on several benchmarks.,Requires substantial computational resources.,Struggles with long sequences and scalability.,Sensitive to visual-language pair biases.,3163,Few-shot number,RareAct,"['Few shot SOTA', 'Flamingo-3B', 'Flamingo-9B', 'Flamingo']","[40.7, 58.4, 57.9, 60.8]",Score,100.png,
101,BLIP-2,v6.3.2,multimodal,세준,BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models,2023,"Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi",Salesforce Research,"Li, J., Li, D., Savarese, S., & Hoi, S. (2023, July). Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning (pp. 19730-19742). PMLR.","Li, J., Li, D., Xiong, C., & Hoi, S. (2022, June). Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International conference on machine learning (pp. 12888-12900). PMLR.","Wang, Z., Yu, J., Yu, A. W., Dai, Z., Tsvetkov, Y., & Cao, Y. (2021). Simvlm: Simple visual language model pretraining with weak supervision. arXiv preprint arXiv:2108.10904.",BLIP-2 introduces a vision-language pre-training strategy that utilizes frozen image encoders and large language models (LLMs). It bridges the modality gap with a lightweight querying transformer (Q-Former) that connects visual and textual representations for efficient multi-modal learning.,Combines frozen vision and language models.,Achieves zero-shot image-to-text generation.,Outperforms SOTA on vision-language tasks.,Requires substantial data preprocessing.,Prone to modality misalignment in challenging cases.,Limited scalability with larger modalities.,3909,Zero-shot Visual Question Answering,VQA Accuracy (%),"['BLIP', 'SimVLM', 'BEIT-3', 'Flamingo', 'BLIP-2']","[56.3, 112.2, 94.9, 82.1, 65.2]",%,101.png,
102,CogVLM,v6.3.3,multimodal,세준,CogVLM: Visual Expert for Pretrained Language Models,2023,"Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Keqin Chen, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, Jie Tang","Tsinghua University, Zhipu AI","Wang, W., Lv, Q., Yu, W., Hong, W., Qi, J., Wang, Y., ... & Tang, J. (2023). Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079.","Yuan, Z., Li, Z., Huang, W., Ye, Y., & Sun, L. (2023). Tinygpt-v: Efficient multimodal large language model via small backbones. arXiv preprint arXiv:2312.16862.","Wang, J., Hu, X., Zhang, P., Li, X., Wang, L., Zhang, L., ... & Liu, Z. (2020). Minivlm: A smaller and faster vision-language model. arXiv preprint arXiv:2012.06946.","CogVLM introduces a trainable visual expert module for deep fusion between visual and language data. By integrating trainable visual features into the attention and feedforward layers, CogVLM bridges the modality gap, enabling high performance across 17 benchmarks, including image captioning, VQA, and visual grounding tasks.",Achieves SOTA on 17 cross-modal benchmarks.,Enables deep vision-language feature fusion.,Bridges modality gap in pre-trained models.,High computational and memory requirements.,Prone to catastrophic forgetting during training.,Sensitive to data quality in visual grounding.,417,Visual Question Answering,VQA Accuracy (%),"['MiniGPT-4', 'IDEFICS-Instruct', 'OpenFlamingo', 'DreamLLM', 'InstructBLIP', 'Qwen-VL-Chat', 'CogVLM-Chat']","[56.6, 37.4, 53.0, 56.6, 60.5, 78.2, 82.3]",%,102.png,
103,LLaVA,v6.4.1,multimodal,세준,Visual Instruction Tuning,2023,"Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee","University of Wisconsin-Madison, Microsoft Research, Columbia University","Liu, H., Li, C., Wu, Q., & Lee, Y. J. (2024). Visual instruction tuning. Advances in neural information processing systems, 36.","Li, B., Zhang, Y., Chen, L., Wang, J., Pu, F., Yang, J., ... & Liu, Z. (2023). Mimic-it: Multi-modal in-context instruction tuning. arXiv preprint arXiv:2306.05425.","Lin, J., Yin, H., Ping, W., Molchanov, P., Shoeybi, M., & Han, S. (2024). Vila: On pre-training for visual language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 26689-26699).","LLaVA (Large Language and Vision Assistant) extends instruction-tuning to the multi-modal domain. It integrates a vision encoder (CLIP) and a language decoder (Vicuna), enabling robust visual and textual understanding. By using GPT-4-generated multimodal instruction data, it achieves impressive multimodal reasoning and visual instruction-following capabilities.",First to use GPT-4-generated visual instructions.,Performs well in multimodal reasoning tasks.,Achieves SoTA accuracy on Science QA benchmark.,Limited by the quality of GPT-4-generated data.,Struggles with high-resolution image details.,Prone to biases in training data.,3937,Visual Instruction Tuning,Performance Score (%),"['OpenFlamingo', 'BLIP-2', 'LLaVA']","[19.1, 38.1, 67.3]",%,103.png,
104,PaLM-E,v6.4.2,multimodal,세준,PaLM-E: An Embodied Multimodal Language Model,2023,"Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, Pete Florence.","Google, TU Berlin","Driess, D., Xia, F., Sajjadi, M. S., Lynch, C., Chowdhery, A., Ichter, B., ... & Florence, P. (2023). Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378.","Zheng, S., Liu, J., Feng, Y., & Lu, Z. (2023). Steve-eye: Equipping llm-based embodied agents with visual perception in open worlds. arXiv preprint arXiv:2310.13255.","Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Chen, X., Choromanski, K., ... & Zitkovich, B. (2023). Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818.","PaLM-E is an embodied multimodal language model that integrates visual and physical sensor modalities into a pre-trained LLM. It enables multimodal understanding, reasoning, and planning in real-world robotic tasks while retaining strong general-purpose visual-language performance on benchmarks like OK-VQA and COCO captioning.",Achieves SOTA on OK-VQA with zero-shot learning.,Integrates neural scene representations for robots.,Excels in multi-modal chain-of-thought reasoning.,Requires large-scale pretraining for high performance.,Struggles with out-of-distribution visual scenes.,Sensitive to input encoder fidelity for specific modalities.,1397,Visual Question Answering,Accuracy (%),"['PaLM-E-12B', 'PaLM-E-562B', 'Flamingo', 'PaLI']","[76.2, 80.0, 82.0, 84.3]",%,104.png,
105,GPT-4V,v6.4.3,multimodal,세준,GPT-4V(ision) System Card,2023,"Jamie Kiros, Daniel Levy, Hyeonwoo Noh, Long Ouyang, Raul Puri, Mark Chen, Casey Chu, Jamie Kiros, Christine McLeavey, Hyeonwoo Noh, Raul Puri, Alec Radford, Aditya Ramesh, Trevor Cai, Yunxing Dai, Chris Hesse, Brandon Houghton, Yongjik Kim, Łukasz Kondraciuk, Hyeonwoo Noh, Mikhail Pavlov, Raul Puri, Nikolas Tezak, Amin Tootoonchian, Tianhao Zheng, Alex Karpenko, Jong Wook Kim, David Mélý, Reiichiro Nakano, Hyeonwoo Noh, Long Ouyang, Raul Puri, Alec Radford, Pranav Shyam, Tao Xu, Sandhini Agarwal, Madeline Boyd, Shengli Hu, Andrew Kondrich, Todor Markov, David Mélý, Hyeonwoo Noh, Reiichiro Nakano, Long Ouyang, Cameron Raymond, Filippo Rasso, Chelsea Voss, Lilian Weng, Chong Zhang, Rowan Zellers, Nicholas Turley, Stephanie Lin, Long Ouyang, Chong Zhang, Ilge Akkaya, Diogo Moitinho de Almeida, Mark Chen, Liam Fedus, Yuchen He, Alex Karpenko, Jamie Kiros, Andrew Kondrich, Rachel Lim, Randall Lin, Stephanie Lin, Ryan Lowe, Luke Metz, Reiichiro Nakano, Long Ouyang, Raul Puri, Jiayi Weng, Barret Zoph, Andrew Cann, Rory Carmichael, Christian Gibson, Henri Roussez, Akila Weliwinda, Oleg Boiko, Trevor Cai, Michael Petrov, Alethea Power, Trevor Cai, Kyle Kosic, Daniel Levy, David Mélý, Reiichiro Nakano, Hyeonwoo Noh, Mikhail Pavlov, Raul Puri, Amin Tootoonchian",OpenAI,OpenAI. (2023). GPT-4V(ision) System Card. Retrieved from https://cdn.openai.com/papers/GPTV_System_Card.pdf,"Yang, Z., Li, L., Lin, K., Wang, J., Lin, C. C., Liu, Z., & Wang, L. (2023). The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint arXiv:2309.17421, 9(1), 1.","Fu, C., Zhang, R., Lin, H., Wang, Z., Gao, T., Luo, Y., ... & Ji, R. (2023). A challenger to gpt-4v? early explorations of gemini in visual expertise. arXiv preprint arXiv:2312.12436.","ChatGPT-4V is a multimodal version of GPT-4 that incorporates image analysis capabilities, allowing users to input images for visual understanding tasks. It enhances traditional text-based reasoning with the ability to understand and describe visual data, enabling applications such as visual Q&A, medical imaging analysis, and accessibility tools for the visually impaired.",Combines visual and textual reasoning effectively.,Enhances accessibility with visual analysis tools.,Provides multimodal insights for diverse tasks.,Prone to hallucinations in image descriptions.,Struggles with nuanced visual detail interpretation.,Requires extensive fine-tuning for domain-specific accuracy.,476,Simulated Exam,Uniform Bar Exam,"['GPT-4', 'GPT-4 (no vision)', 'GPT-3.5']","[298, 298, 400]",Score,105.png,image 생략
106,Claude 3 Vision,v6.4.4,multimodal,세준,"The Claude 3 Model Family: Opus, Sonnet, Haiku",2024,Anthropic,Anthropic,"Anthropic. (2024). The Claude 3 model family: Opus, Sonnet, Haiku.","Anthropic. (2024, March 4). Introducing the next generation of Claude.","Anthropic. (2024, March 13). Claude 3 Haiku: our fastest model yet.","Claude 3 models (Opus, Sonnet, and Haiku) excel in multimodal understanding, including reasoning, math, coding, and multilingual tasks. Opus delivers state-of-the-art performance on benchmarks like MMLU, while Sonnet balances speed and intelligence, and Haiku provides the fastest, most cost-effective option for enterprise use.",Opus achieves state-of-the-art on MMLU with 88.2% accuracy.,Multimodal capabilities enable image-text processing for diverse applications.,Significant improvements in non-English fluency and multilingual reasoning.,Performance varies for low-resource languages and smaller datasets.,High computational requirements for Opus and Sonnet models.,Limited support for highly specialised or domain-specific tasks.,208,MMLU,Accuracy(%),"['Claude 3 Opus', 'Claude 3 Sonnet', 'Claude 3 Haiku', 'GPT-4', 'GPT-3.5', 'Gemini 1.0 Ultra', 'Gemini 1.5 Pro', 'Gemini 1.0 Pro']","[86.8, 79.0, 75.2, 86.4, 70.0, 83.7, 81.9, 71.8]",%,58.png,
107,Gemini Vision,v6.4.5,multimodal,세준,Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models,2023,"Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, Jiaya Jia","The Chinese University of Hong Kong, SmartMore","Li, Y., Zhang, Y., Wang, C., Zhong, Z., Chen, Y., Chu, R., ... & Jia, J. (2024). Mini-gemini: Mining the potential of multi-modality vision language models. arXiv preprint arXiv:2403.18814.","Zanella, M., & Ben Ayed, I. (2024). On the test-time zero-shot generalization of vision-language models: Do we really need prompt learning?. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 23783-23793).","Ge, Y., Zhao, S., Zhu, J., Ge, Y., Yi, K., Song, L., ... & Shan, Y. (2024). Seed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint arXiv:2404.14396.","Gemini introduces an advanced multi-modal framework to enhance Vision Language Models (VLMs) by focusing on high-resolution visual tokens, high-quality data, and VLM-guided generation for robust visual understanding and reasoning.",Excels in visual reasoning tasks.,Reduces computational cost with high-resolution methods.,Outperforms state-of-the-art VLMs in complex benchmarks.,Requires extensive pretraining data.,Sensitive to visual token refinement process.,Prone to domain-specific challenges.,113,Performance,VQA,"['MobileVLM', 'InstructBLIP', 'Qwen-VL-Chat', 'LLaVA-1.5', 'Mini-Gemini', 'Mini-Gemini-HD']","[47.5, 50.7, 61.5, 61.3, 65.2, 68.4]",%,107.png,
108,Whisper,v6.5.1,multimodal,세준,Robust Speech Recognition via Large-Scale Weak Supervision,2022,"Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever",OpenAI,"Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., & Sutskever, I. (2023, July). Robust speech recognition via large-scale weak supervision. In International conference on machine learning (pp. 28492-28518). PMLR.","Zhang, Z., Geiger, J., Pohjalainen, J., Mousa, A. E. D., Jin, W., & Schuller, B. (2018). Deep learning for environmentally robust speech recognition: An overview of recent developments. ACM Transactions on Intelligent Systems and Technology (TIST), 9(5), 1-28.","Liang, D., Huang, Z., & Lipton, Z. C. (2018, December). Learning noise-invariant representations for robust speech recognition. In 2018 IEEE Spoken Language Technology Workshop (SLT) (pp. 56-63). IEEE.","Whisper is a robust speech recognition model trained on 680,000 hours of multilingual audio. It uses weak supervision to generalize across languages, tasks, and environments, achieving competitive results without fine-tuning, including zero-shot transcription, translation, and multilingual ASR tasks.",Generalizes well across languages and domains.,Excels in zero-shot transcription and translation.,Achieves robustness with large-scale weak supervision.,Lacks scalability for rare languages and tasks.,Sensitive to transcription noise in training data.,Computational cost increases with model scaling.,3218,Speech Recognition,Error Rate,"['Whisper Tiny', 'Whisper Base', 'Whisper Small', 'Whisper Medium', 'Whisper Large', 'Whisper Large-V2']","[12.8, 9.4, 6.0, 5.2, 5.6, 5.2]",%,108.png,
109,Deep Q-Network (DQN),v7.0.1,reinforcement,세준,Playing Atari with Deep Reinforcement Learning,2015,"Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin Riedmiller",DeepMind,"Mnih, V. (2013). Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602.","Liang, Y., Machado, M. C., Talvitie, E., & Bowling, M. (2015). State of the art control of atari games using shallow reinforcement learning. arXiv preprint arXiv:1512.01563.","Kaiser, L., Babaeizadeh, M., Milos, P., Osinski, B., Campbell, R. H., Czechowski, K., ... & Michalewski, H. (2019). Model-based reinforcement learning for atari. arXiv preprint arXiv:1903.00374.","DQN learns control policies directly from high-dimensional sensory input using reinforcement learning. A convolutional neural network, trained via Q-learning and experience replay, it achieved human-level performance on multiple Atari 2600 games, surpassing existing RL methods on six out of seven games tested.",Uses raw pixels for end-to-end learning.,Achieves human-level control in games.,Combines Q-learning with experience replay for stability.,Requires significant computational resources.,Struggles with long-term reward dependencies.,Limited by the simplicity of Atari-like environments.,16264,Game Performance,Score,"['Random', 'Sarsa', 'Contingency', 'DQN', 'Human', 'HNeat Best', 'HNeat Pixel', 'DQN Best']","[354, 996, 1743, 4092, 7456, 3616, 1332, 5184]",Score,109.png,
110,A3C,v7.0.2,reinforcement,세준,Asynchronous Methods for Deep Reinforcement Learning,2016,"Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Tim Harley, Timothy P. Lillicrap, David Silver, Koray Kavukcuoglu","DeepMind, University of Montreal","Mnih, V. (2016). Asynchronous Methods for Deep Reinforcement Learning. arXiv preprint arXiv:1602.01783.","Zhao, C., Sigaud, O., Stulp, F., & Hospedales, T. M. (2019). Investigating generalisation in continuous deep reinforcement learning. arXiv preprint arXiv:1902.07015.","Graesser, L., & Keng, W. L. (2019). Foundations of deep reinforcement learning: theory and practice in Python. Addison-Wesley Professional.","A3C introduces asynchronous methods for reinforcement learning using multiple agents interacting with the environment in parallel. It stabilizes training through decorrelated updates and achieves state-of-the-art performance on Atari and continuous control tasks, reducing training time significantly compared to previous approaches.",Stabilizes training without replay memory.,Excels in both discrete and continuous control tasks.,Trains faster using only CPUs.,Requires careful parameter tuning for parallelization.,Sensitive to thread-specific biases in updates.,Limited scalability with a large number of threads.,11980,Game Performance,Normalized Score,"['DQN', 'Gorila', 'Double DQN', 'Dueling DQN', 'Prioritized DQN', 'A3C FF (1 Day)', 'A3C FF', 'A3C LSTM']","[121.9, 215.2, 332.9, 343.8, 463.6, 344.1, 496.8, 623.0]",%,110.png,
111,PPO,v7.0.3,reinforcement,세준,Proximal Policy Optimization Algorithms,2017,"John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov",OpenAI,"Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017). Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347.","Liu, Y., Ramachandran, P., Liu, Q., & Peng, J. (2017). Stein variational policy gradient. arXiv preprint arXiv:1704.02399.","Hsu, C. C. Y., Mendler-Dünner, C., & Hardt, M. (2020). Revisiting design choices in proximal policy optimization. arXiv preprint arXiv:2009.10897.","PPO simplifies policy optimization with a clipped surrogate objective for stable learning. It achieves superior performance on continuous control and Atari tasks, balancing simplicity, stability, and computational efficiency compared to previous methods like TRPO.",Outperforms A3C on continuous control tasks.,Simplifies implementation compared to TRPO.,Balances sample efficiency and learning stability.,Sensitive to hyperparameter tuning.,Limited performance on highly sparse rewards.,Requires high-quality baseline models for advantage estimation.,21674,Game Performance,Mean Score,"['A2C', 'ACER', 'PPO']","[19735.9, 156225.6, 37389.0]",Score,111.png,
112,AlphaGo,v7.1.1,reinforcement,세준,Mastering the game of Go with deep neural networks and tree search,2016,"David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, Demis Hassabis",Deepmind,"Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., ... & Hassabis, D. (2016). Mastering the game of Go with deep neural networks and tree search. nature, 529(7587), 484-489.","Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., ... & Hassabis, D. (2017). Mastering the game of go without human knowledge. nature, 550(7676), 354-359.","Maddison, C. J., Huang, A., Sutskever, I., & Silver, D. (2014). Move evaluation in Go using deep convolutional neural networks. arXiv preprint arXiv:1412.6564.","AlphaGo combines policy and value networks with Monte Carlo Tree Search, achieving superhuman performance through deep learning and reinforcement learning. It defeated top professional players, such as Lee Sedol, showcasing advanced planning and decision-making capabilities.",Achieves superhuman performance in Go.,Combines policy and value networks with MCTS.,Demonstrates robust planning in complex games.,Requires substantial computational resources.,Limited to perfect-information games.,Sensitive to handcrafted features.,20360,Performance,Elo Rating,"['AlphaGo', 'Distributed AlphaGo', 'Crazy Stone', 'Zen', 'Pachi', 'Fuego', 'GnuGo']","[2890, 3140, 1929, 1888, 1298, 1148, 431]",Elo Rating,112.png,
113,AlphaGo Zero,v7.1.2,reinforcement,세준,Mastering the game of Go without human knowledge,2017,"David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre, George van den Driessche, Thore Graepel, Demis Hassabis",DeepMind,"Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., ... & Hassabis, D. (2017). Mastering the game of go without human knowledge. nature, 550(7676), 354-359.","Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., ... & Hassabis, D. (2016). Mastering the game of Go with deep neural networks and tree search. nature, 529(7587), 484-489.","Singh, S., Okun, A., & Jackson, A. (2017). Learning to play Go from scratch. Nature, 550(7676), 336-337.","AlphaGo Zero uses self-play reinforcement learning starting from scratch, without any human data or supervision. It surpasses all previous AlphaGo versions by using a simpler architecture with a single neural network and learning from first principles, achieving superhuman performance in just 40 days.",Learns tabula rasa without human data.,Combines policy and value functions in one network.,Defeats AlphaGo Master 89-11.,Requires massive computational power for training.,Struggles with input noise in raw board states.,"Limited to structured, perfect-information domains.",11585,Performance,Elo Rating,"['AlphaGo Zero', 'AlphaGo', 'Distributed AlphaGo', 'Crazy Stone', 'Pachi', 'GnuGo']","[5000, 2890, 3140, 1929, 1298, 431]",Elo Rating,113.png,
114,AlphaZero,v7.1.3,reinforcement,세준,Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm,2017,"David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, Demis Hassabis",DeepMind,"Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., ... & Hassabis, D. (2017). Mastering chess and shogi by self-play with a general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815.","Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., ... & Hassabis, D. (2018). A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play. Science, 362(6419), 1140-1144.","Lai, M. (2015). Giraffe: Using deep reinforcement learning to play chess. arXiv preprint arXiv:1509.01549.","AlphaZero generalizes AlphaGo Zero's algorithm to chess, shogi, and Go. Starting with no domain knowledge other than game rules, it achieves superhuman performance in all three games using deep reinforcement learning and Monte Carlo Tree Search.","Generalizes across chess, shogi, and Go.",Learns tabula rasa without human input.,Outperforms world-champion programs in hours.,Requires extensive computational resources for training.,Limited to perfect-information games.,Sensitive to the complexity of new rules.,2407,Game Performance,Elo Rating,"['Stockfish', 'Elmo', 'AlphaGo Zero', 'AlphaZero (Chess)', 'AlphaZero (Shogi)', 'AlphaZero (Go)']","[3435, 3518, 5185, 3512, 3525, 5185]",Elo Rating,114.png,
115,MuZero,v7.1.4,reinforcement,세준,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",2019,"Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy Lillicrap, David Silver","DeepMind, University College London","Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L., Schmitt, S., ... & Silver, D. (2020). Mastering atari, go, chess and shogi by planning with a learned model. Nature, 588(7839), 604-609.","Lipovetzky, N., Ramirez, M., & Geffner, H. (2015, July). Classical planning with simulators: Results on the atari video games. In Proc. IJCAI.","Antonoglou, I., Schrittwieser, J., Ozair, S., Hubert, T. K., & Silver, D. (2021, October). Planning in stochastic environments with a learned model. In International Conference on Learning Representations.","MuZero extends AlphaZero by learning a model of the environment, enabling superhuman performance in chess, shogi, Go, and Atari games without knowledge of the environment's rules. It predicts policy, value, and reward end-to-end, integrating planning with learning to generalize to broader, more complex domains.",Learns environment dynamics without explicit rules.,Excels in visually complex Atari games.,Combines model-based and model-free RL effectively.,Computationally expensive for high-complexity environments.,Sensitive to inaccurate learned dynamics models.,Requires substantial hyperparameter optimization.,2604,Game Performance,Elo Rating,"['AlphaZero (Chess)', 'AlphaZero (Shogi)', 'AlphaZero (Go)', 'MuZero (Chess)', 'MuZero (Shogi)', 'MuZero (Go)']","[3512, 3525, 5185, 3520, 3523, 5186]",Elo Rating,115.png,
116,AlphaStar,v7.2.1,reinforcement,세준,Grandmaster level in StarCraft II using multi-agent reinforcement learning,2019,"Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David H. Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Danihelka, Aja Huang, Laurent Sifre, Trevor Cai, John P. Agapiou, Chris Apps, David Silver","DeepMind, Team Liquid","Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung, J., ... & Silver, D. (2019). Grandmaster level in StarCraft II using multi-agent reinforcement learning. nature, 575(7782), 350-354.","Vinyals, O., Ewalds, T., Bartunov, S., Georgiev, P., Vezhnevets, A. S., Yeo, M., ... & Tsing, R. (2017). Starcraft ii: A new challenge for reinforcement learning. arXiv preprint arXiv:1708.04782.","Wang, X., Song, J., Qi, P., Peng, P., Tang, Z., Zhang, W., ... & Yuan, Q. (2021, July). SCC: An efficient deep reinforcement learning agent mastering the game of StarCraft II. In International conference on machine learning (pp. 10905-10915). PMLR.","AlphaStar is the first AI to achieve Grandmaster level in StarCraft II, mastering a complex, real-time strategy game. It uses multi-agent reinforcement learning, human imitation learning, and league-based self-play, achieving above 99.8% win rate against ranked human players in all three StarCraft races.",Achieves Grandmaster level in StarCraft II.,Handles complex real-time strategy environments.,Combines supervised and reinforcement learning effectively.,Requires significant computational resources for training.,Struggles with long-term planning in novel scenarios.,Limited scalability to real-world domains beyond StarCraft.,4858,Game Performance,MMR,"['AlphaStar Supervised', 'AlphaStar Mid', 'AlphaStar Final (Protoss)', 'AlphaStar Final (Terran)', 'AlphaStar Final (Zerg)']","[3699, 6196, 6297, 6048, 5835]",MMR,116.png,
117,AlphaFold,v7.3.1,reinforcement,세준,Improved protein structure prediction using potentials from deep learning,2020,"Andrew W. Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green, Chongli Qin, Augustin Žídek, Alexander W. R. Nelson, Alex Bridgland, Hugo Penedones, Stig Petersen, Karen Simonyan, Steve Crossan, Pushmeet Kohli, David T. Jones, David Silver, Koray Kavukcuoglu, Demis Hassabis","DeepMind, The Francis Crick Institute, University College London","Senior, A. W., Evans, R., Jumper, J., Kirkpatrick, J., Sifre, L., Green, T., ... & Hassabis, D. (2020). Improved protein structure prediction using potentials from deep learning. Nature, 577(7792), 706-710.","Jumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, O., ... & Hassabis, D. (2021). Highly accurate protein structure prediction with AlphaFold. nature, 596(7873), 583-589.","Kryshtafovych, A., Schwede, T., Topf, M., Fidelis, K., & Moult, J. (2021). Critical assessment of methods of protein structure prediction (CASP)—Round XIV. Proteins: Structure, Function, and Bioinformatics, 89(12), 1607-1617.","AlphaFold CASP13 uses a neural network to predict pairwise residue distances and folding potentials, achieving high accuracy in protein structure prediction. It demonstrated remarkable success in the CASP13 competition, surpassing other methods in free modeling tasks and creating biologically relevant 3D protein models without relying on homologous templates.",Excels in free-modeling tasks in CASP13.,Demonstrates high precision in distance prediction.,Creates biologically relevant protein structures.,Struggles with multi-domain proteins.,Limited by input alignment quality.,Sensitive to poorly represented sequences in MSAs.,3381,Prediction Performance,TM-Score,"['CASP13 AlphaFold', 'CASP13 Next Best', 'CASP14 AlphaFold', 'CASP14 Next Best']","[0.7, 0.55, 0.92, 0.78]",TM-Score,117.png,
118,AlphaFold 2,v7.3.2,reinforcement,세준,Highly accurate protein structure prediction with AlphaFold,2021,"John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, Alex Bridgland, Clemens Meyer, Simon A. A. Kohl, Andrew J. Ballard, Andrew Cowie, Bernardino Romera-Paredes, Stanislav Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen, David Reiman, Ellen Clancy, Michal Zielinski, Martin Steinegger, Michalina Pacholska, Tamas Berghammer, Sebastian Bodenstein, David Silver, Oriol Vinyals, Andrew W. Senior, Koray Kavukcuoglu, Pushmeet Kohli, Demis Hassabis","DeepMind, Seoul National University","Jumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, O., ... & Hassabis, D. (2021). Highly accurate protein structure prediction with AlphaFold. nature, 596(7873), 583-589.","Senior, A. W., Evans, R., Jumper, J., Kirkpatrick, J., Sifre, L., Green, T., ... & Hassabis, D. (2020). Improved protein structure prediction using potentials from deep learning. Nature, 577(7792), 706-710.","Lane, T. J. (2023). Protein structure prediction has reached the single-structure frontier. Nature Methods, 20(2), 170-173.","AlphaFold revolutionizes protein structure prediction by achieving near-experimental accuracy using neural networks that model evolutionary, physical, and geometric constraints. It demonstrates remarkable results in CASP14, predicting protein 3D structures with atomic accuracy across diverse cases, enabling breakthroughs in biology.",Achieves atomic-level accuracy in protein structures.,Outperforms traditional methods in CASP14.,Accelerates proteomics research at unprecedented scale.,Limited by availability of high-quality sequence data.,Struggles with multi-chain protein complexes.,Computational cost for very large proteins.,29012,Accuracy,Root Mean Square Deviation,"['CASP13 AlphaFold', 'CASP14 AlphaFold', 'Next Best CASP14', 'Experimental Accuracy']","[2.1, 0.96, 2.8, 0.85]",Å,118.png,